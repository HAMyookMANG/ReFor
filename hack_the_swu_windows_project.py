# -*- coding: utf-8 -*-
"""Hack the swu_windows project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F23uSpmQxeY-tLoSxt0JryiNr_hix_p2
"""

# ==========================================
# Noise Residual 기반 "2단계" 분류 파이프라인
#   1) 1단계: real vs fake (binary)
#   2) 2단계: fake로 판정된 경우에만 모델 attribution (multi-class)
#
# 기대 폴더 구조 (동일):
#   /content/Drive/MyDrive/prnu_detector/dataset/
#       biggan/{0_real,1_fake}/**.png|jpg|jpeg|bmp|webp
#       cyclegan/{0_real,1_fake}/**.*
#       ...
# ==========================================
!pip -q install --upgrade torch torchvision scikit-learn

import os, math, random, io, itertools
from typing import Tuple, List, Dict
from PIL import Image
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, WeightedRandomSampler
from torchvision import transforms, models

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

# --- 1. Google Drive 마운트 ---
from google.colab import drive
drive.mount('/content/drive')

# --------------------------
# 0) 환경/시드/GPU
# --------------------------
def seed_all(seed=42):
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)
seed_all(42)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Device:", device)

# --------------------------
# 1) Noise Residual 변환
# --------------------------
class ToResidualTensor:
    def __init__(self, ksize=5, sigma=1.2, eps=1e-6):
        self.ksize = ksize
        self.sigma = sigma
        self.eps = eps
        # 가우시안 커널
        ax = torch.arange(ksize) - (ksize-1)/2
        xx, yy = torch.meshgrid(ax, ax, indexing='ij')
        kernel = torch.exp(-(xx**2 + yy**2) / (2*sigma*sigma))
        kernel = kernel / kernel.sum()
        self.registered = False
        self.kernel = kernel

    def _register(self, device):
        k = self.kernel.to(device=device, dtype=torch.float32)
        self.weight = k.view(1,1,self.ksize,self.ksize)
        self.registered = True

    def __call__(self, img: Image.Image):
        # 1) PIL -> Tensor [0,1] (C,H,W)
        x = transforms.functional.to_tensor(img)
        # 2) Grayscale(Y)
        if x.shape[0] == 3:
            y = 0.299*x[0] + 0.587*x[1] + 0.114*x[2]
        else:
            y = x[0]
        y = y.unsqueeze(0).unsqueeze(0)  # (1,1,H,W)
        if not self.registered:
            self._register(y.device)
        # 3) Blur
        y_blur = F.conv2d(y, self.weight, padding=self.ksize//2)
        # 4) Residual
        r = y - y_blur
        # 5) Normalize
        mean = r.mean()
        std  = r.std()
        r = (r - mean) / (std + self.eps)
        # 6) 3채널 복제
        r3 = r.repeat(1,3,1,1).squeeze(0)  # (3,H,W)
        return r3

# --------------------------
# 2) 데이터 구성 (2단계 전용)
# --------------------------
IMG_SIZE = 224
IMG_EXTS = ('.jpg', '.jpeg', '.png', '.bmp', '.webp')

train_tfms_geometric = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(IMG_SIZE),
    transforms.RandomHorizontalFlip(),
])

val_tfms_geometric = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(IMG_SIZE),
])

residualizer = ToResidualTensor(ksize=5, sigma=1.2)

def _list_images(d):
    return [os.path.join(d, f) for f in os.listdir(d)
            if os.path.isfile(os.path.join(d,f)) and f.lower().endswith(IMG_EXTS)]

# ---- (A) 1단계: Binary (real=0, fake=1) ----
class ResidualBinaryDataset(torch.utils.data.Dataset):
    """
    root/
      modelA/{0_real,1_fake}/...
      modelB/{0_real,1_fake}/...
      ...
    - 0_real → label 0 (real)
    - 1_fake → label 1 (fake)
    """
    def __init__(self, root: str, split: str = "train"):
        super().__init__()
        self.root = root
        self.split = split
        self.samples: List[Tuple[str,int]] = []

        model_dirs = [d for d in os.listdir(root) if os.path.isdir(os.path.join(root,d))]
        model_dirs = sorted(model_dirs)

        for m in model_dirs:
            mdir = os.path.join(root, m)
            real_dir = os.path.join(mdir, "0_real")
            fake_dir = os.path.join(mdir, "1_fake")

            if os.path.isdir(real_dir):
                for p in _list_images(real_dir):
                    self.samples.append((p, 0))  # real
            if os.path.isdir(fake_dir):
                for p in _list_images(fake_dir):
                    self.samples.append((p, 1))  # fake

        if len(self.samples) == 0:
            raise RuntimeError(f"No images found under: {root}")

        self.transform = train_tfms_geometric if split == "train" else val_tfms_geometric

    def __len__(self): return len(self.samples)

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        img = Image.open(path).convert('RGB')
        img = self.transform(img)
        return img, label, path

# ---- (B) 2단계: Fake Attribution (multi-class: 각 모델 이름) ----
class ResidualAttributionDataset(torch.utils.data.Dataset):
    """
    root/
      modelA/{0_real,1_fake}/...
      modelB/{0_real,1_fake}/...
      ...
    - 1_fake → 해당 모델 라벨
    - 0_real은 사용하지 않음 (2단계는 fake만)
    """
    def __init__(self, root: str, split: str = "train"):
        super().__init__()
        self.root = root
        self.split = split

        # ✅ 1) fake 데이터가 '실제로 존재'하는 모델만 클래스에 포함
        all_model_dirs = [d for d in os.listdir(root) if os.path.isdir(os.path.join(root,d))]
        model_names = []
        for m in sorted(all_model_dirs):
            fake_dir = os.path.join(root, m, "1_fake")
            if os.path.isdir(fake_dir):
                imgs = _list_images(fake_dir)
                if len(imgs) > 0:
                    model_names.append(m)

        if len(model_names) == 0:
            raise RuntimeError(f"No valid model folders with 1_fake images under: {root}")

        self.model_names = model_names
        self.class_to_idx = {m:i for i,m in enumerate(self.model_names)}

        # ✅ 2) 샘플 구성
        self.samples: List[Tuple[str,int]] = []
        for m in self.model_names:
            fake_dir = os.path.join(root, m, "1_fake")
            for p in _list_images(fake_dir):
                self.samples.append((p, self.class_to_idx[m]))

        if len(self.samples) == 0:
            raise RuntimeError(f"No fake images found under: {root}")

        self.transform = train_tfms_geometric if split == "train" else val_tfms_geometric

    def __len__(self): return len(self.samples)

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        img = Image.open(path).convert('RGB')
        img = self.transform(img)
        return img, label, path

# ---- 공통: 로더/샘플러 ----
def _class_counts_from_indices(dataset, indices: List[int], num_classes: int):
    counts = [0]*num_classes
    for i in indices:
        _, y, _ = dataset[i]
        counts[y] += 1
    return counts

def _split_indices(n: int, val_ratio: float = 0.2, seed: int = 42):
    g = torch.Generator().manual_seed(seed)
    indices = torch.randperm(n, generator=g).tolist()
    n_val = max(1, int(len(indices)*val_ratio))
    return indices[n_val:], indices[:n_val]  # train, val

def _make_loader_with_residual(dataset, batch_size=32, num_workers=2, use_sampler=True):
    # collate: residualizer 적용 + 텐서 스택
    def _collate_with_residual(batch):
        imgs, labels, _paths = zip(*batch)
        tensors = [residualizer(img) for img in imgs]
        x = torch.stack(tensors, dim=0)
        y = torch.tensor(labels, dtype=torch.long)
        return x, y
    if use_sampler:
        # 클래스 불균형 보정
        labels = [dataset[i][1] for i in range(len(dataset))]
        K = max(labels)+1
        counts = [1]*K
        for y in labels: counts[y] += 1
        class_w = torch.tensor([1.0/c for c in counts], dtype=torch.float32)
        sample_w = [class_w[y].item() for y in labels]
        sampler = WeightedRandomSampler(sample_w, num_samples=len(sample_w), replacement=True)
        return DataLoader(dataset, batch_size=batch_size, sampler=sampler,
                          num_workers=num_workers, pin_memory=True,
                          collate_fn=_collate_with_residual)
    else:
        return DataLoader(dataset, batch_size=batch_size, shuffle=False,
                          num_workers=num_workers, pin_memory=True,
                          collate_fn=_collate_with_residual)

def make_binary_loaders(dataset_root: str, batch_size=32, num_workers=2, val_ratio=0.2, seed=42):
    full = ResidualBinaryDataset(dataset_root, split="train")
    train_idx, val_idx = _split_indices(len(full), val_ratio, seed)

    class Subset(torch.utils.data.Dataset):
        def __init__(self, base, idxs, split):
            self.base = base; self.idxs = idxs
            self.tf = train_tfms_geometric if split=="train" else val_tfms_geometric
        def __len__(self): return len(self.idxs)
        def __getitem__(self, i):
            _, y, p = self.base[self.idxs[i]]
            img = Image.open(p).convert('RGB')
            img = self.tf(img)
            return img, y, p

    train_ds = Subset(full, train_idx, "train")
    val_ds   = Subset(full, val_idx,   "val")
    train_loader = _make_loader_with_residual(train_ds, batch_size, num_workers, use_sampler=True)
    val_loader   = _make_loader_with_residual(val_ds,   batch_size, num_workers, use_sampler=False)

    print(f"[Binary Data] train={len(train_idx)} val={len(val_idx)}")
    return train_loader, val_loader

def make_attr_loaders(dataset_root: str, batch_size=32, num_workers=2, val_ratio=0.2, seed=42):
    full = ResidualAttributionDataset(dataset_root, split="train")
    class_names = full.model_names
    train_idx, val_idx = _split_indices(len(full), val_ratio, seed)

    class Subset(torch.utils.data.Dataset):
        def __init__(self, base, idxs, split):
            self.base = base; self.idxs = idxs
            self.tf = train_tfms_geometric if split=="train" else val_tfms_geometric
        def __len__(self): return len(self.idxs)
        def __getitem__(self, i):
            _, y, p = self.base[self.idxs[i]]
            img = Image.open(p).convert('RGB')
            img = self.tf(img)
            return img, y, p

    train_ds = Subset(full, train_idx, "train")
    val_ds   = Subset(full, val_idx,   "val")
    train_loader = _make_loader_with_residual(train_ds, batch_size, num_workers, use_sampler=True)
    val_loader   = _make_loader_with_residual(val_ds,   batch_size, num_workers, use_sampler=False)

    print(f"[Attribution Data] classes={class_names}")
    print(f"[Attribution Data] train={len(train_idx)} val={len(val_idx)}")
    return train_loader, val_loader, class_names

# --------------------------
# 3) 모델 정의
# --------------------------
class ResNetHead(nn.Module):
    def __init__(self, num_classes: int):
        super().__init__()
        self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        in_f = self.backbone.fc.in_features
        self.backbone.fc = nn.Linear(in_f, num_classes)

    def forward(self, x):
        return self.backbone(x)

# --------------------------
# 4) 학습/평가 루프 (체크포인트 로드/저장 + 혼동행렬/리포트 포함)
# --------------------------
import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

def accuracy_from_logits(logits, y):
    pred = logits.argmax(dim=1)
    return (pred == y).float().mean().item()

def _plot_cm(cm, class_names, title="Confusion Matrix", normalize=True):
    if normalize:
        cm = cm.astype('float') / (cm.sum(axis=1, keepdims=True) + 1e-12)
    fig, ax = plt.subplots(figsize=(6, 5))
    im = ax.imshow(cm, interpolation='nearest')
    ax.figure.colorbar(im, ax=ax)
    ax.set(xticks=np.arange(len(class_names)), yticks=np.arange(len(class_names)),
           xticklabels=class_names, yticklabels=class_names,
           ylabel='True', xlabel='Predicted', title=title)
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
    thresh = cm.max() / 2.0
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, f"{cm[i, j]:.2f}" if normalize else int(cm[i, j]),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    plt.tight_layout()
    plt.show()

def _print_report_and_cm(y_true, y_pred, class_names=None, title_prefix=""):
    if class_names is None:
        # 기본: 이진 분류 가정 (0=real, 1=fake)
        class_names = ["real", "fake"] if len(set(y_true)) <= 2 else [str(i) for i in sorted(set(y_true))]
    print(f"\n{title_prefix}Classification Report")
    print(classification_report(y_true, y_pred, target_names=class_names, digits=3))
    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))
    print(f"{title_prefix}Confusion Matrix (counts)")
    print(cm)
    _plot_cm(cm, class_names, title=f"{title_prefix}Confusion Matrix (normalized)")

def train_one_epoch(model, loader, optimizer, scaler, criterion):
    model.train()
    loss_m = acc_m = 0.0
    for x, y in loader:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad(set_to_none=True)
        with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
            logits = model(x)
            loss = criterion(logits, y)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        acc = accuracy_from_logits(logits, y)
        loss_m += loss.item()
        acc_m += acc
    n = len(loader)
    return loss_m/n, acc_m/n

@torch.no_grad()
def evaluate(model, loader, criterion):
    model.eval()
    loss_m = acc_m = 0.0
    all_true, all_pred = [], []
    for x, y in loader:
        x, y = x.to(device), y.to(device)
        with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
            logits = model(x)
            loss = criterion(logits, y)
        acc = accuracy_from_logits(logits, y)
        loss_m += loss.item()
        acc_m += acc
        all_true.extend(y.detach().cpu().tolist())
        all_pred.extend(logits.argmax(dim=1).detach().cpu().tolist())
    n = len(loader)
    return (loss_m/n, acc_m/n, np.array(all_true), np.array(all_pred))

def fit_model(model, train_loader, val_loader, epochs=8, lr=1e-3, wd=1e-4,
              head_freeze=True, save_path="/content/best.pth", label_smoothing=0.1,
              load_if_exists=True, class_names=None, print_metrics=True):
    """
    - load_if_exists=True 이고 save_path가 존재하면: 체크포인트 로드 후 바로 평가/지표 출력하고 종료
    - 아니면 학습을 수행하고, 검증 최고 성능 모델을 save_path에 저장
    - 종료 시점에 best ckpt를 로드하여 검증셋 혼동행렬/리포트를 출력
    """
    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)

    # 0) 체크포인트가 있으면 곧바로 로드 & 평가
    if load_if_exists and os.path.exists(save_path):
        state = torch.load(save_path, map_location=device)
        model.load_state_dict(state)
        model.to(device).eval()
        print(f"[Load] Loaded checkpoint: {save_path}")

        if val_loader is not None and print_metrics:
            vl, va, y_true, y_pred = evaluate(model, val_loader, criterion)
            print(f"[Loaded Model] val loss/acc: {vl:.4f}/{va:.3f}")
            _print_report_and_cm(y_true, y_pred, class_names=class_names, title_prefix="[Loaded Model] ")
            return va  # 로드된 모델의 검증 정확도 반환

        return None  # 평가 로더가 없다면 그냥 종료

    # 1) 헤드-프리즈 설정
    params_all = list(model.parameters())
    if head_freeze:
        for p in model.backbone.layer1.parameters(): p.requires_grad = False
        for p in model.backbone.layer2.parameters(): p.requires_grad = False
        for p in model.backbone.layer3.parameters(): p.requires_grad = False
        for p in model.backbone.layer4.parameters(): p.requires_grad = False
        for p in model.backbone.fc.parameters():     p.requires_grad = True

    opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),
                            lr=lr, weight_decay=wd)
    scaler = torch.amp.GradScaler('cuda', enabled=(device.type=='cuda'))
    scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=3, gamma=0.5)

    tr_losses=[]; va_losses=[]; tr_accs=[]; va_accs=[]
    best_acc=-1.0

    head_epochs = max(3, min(5, epochs//2))
    # 2) 헤드 학습
    for ep in range(1, head_epochs+1):
        tl, ta = train_one_epoch(model, train_loader, opt, scaler, criterion)
        vl, va, y_true, y_pred = evaluate(model, val_loader, criterion)
        scheduler.step()
        tr_losses.append(tl); tr_accs.append(ta); va_losses.append(vl); va_accs.append(va)
        print(f"[Head {ep}/{head_epochs}] train {tl:.4f}/{ta:.3f} | val {vl:.4f}/{va:.3f}")
        if va > best_acc:
            best_acc = va
            torch.save(model.state_dict(), save_path)
            print("  -> saved best (head stage)")

    # 3) 전체 미세조정
    for p in model.parameters(): p.requires_grad = True
    opt = torch.optim.AdamW(model.parameters(), lr=max(lr*0.03, 3e-5), weight_decay=wd)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max(1, epochs-head_epochs))

    for ep in range(head_epochs+1, epochs+1):
        tl, ta = train_one_epoch(model, train_loader, opt, scaler, criterion)
        vl, va, y_true, y_pred = evaluate(model, val_loader, criterion)
        scheduler.step()
        tr_losses.append(tl); tr_accs.append(ta); va_losses.append(vl); va_accs.append(va)
        print(f"[Finetune {ep}/{epochs}] train {tl:.4f}/{ta:.3f} | val {vl:.4f}/{va:.3f}")
        if va > best_acc:
            best_acc = va
            torch.save(model.state_dict(), save_path)
            print("  -> saved best")

    # 4) 학습 곡선
    epochs_range = range(1, len(tr_losses)+1)
    plt.figure(figsize=(7,5))
    plt.plot(epochs_range, tr_losses, marker='o', label='Train Loss')
    plt.plot(epochs_range, va_losses, marker='s', label='Val Loss')
    plt.title("Loss"); plt.xlabel("Epoch"); plt.ylabel("Loss")
    plt.grid(True); plt.legend(); plt.show()

    plt.figure(figsize=(7,5))
    plt.plot(epochs_range, tr_accs, marker='o', label='Train Acc')
    plt.plot(epochs_range, va_accs, marker='s', label='Val Acc')
    plt.title("Accuracy"); plt.xlabel("Epoch"); plt.ylabel("Accuracy")
    plt.grid(True); plt.legend(); plt.show()

    # 5) 베스트 체크포인트 로드 후 최종 검증/지표
    if os.path.exists(save_path):
        state = torch.load(save_path, map_location=device)
        model.load_state_dict(state)
        model.to(device).eval()

    print(f"[Best Val Acc] {best_acc:.3f} (saved to {save_path})")

    if val_loader is not None and print_metrics:
        vl, va, y_true, y_pred = evaluate(model, val_loader, criterion)
        print(f"[Final Eval @ Best CKPT] val loss/acc: {vl:.4f}/{va:.3f}")
        _print_report_and_cm(y_true, y_pred, class_names=class_names, title_prefix="[Best CKPT] ")

    return best_acc

# --------------------------
# 5) 추론 유틸 (2단계 연결)
# --------------------------
@torch.no_grad()
def _softmax_np(logits: torch.Tensor):
    p = torch.softmax(logits, dim=1)[0].detach().cpu().numpy()
    return p

@torch.no_grad()
def predict_two_stage(
    img_path: str,
    binary_model: nn.Module,
    attr_model: nn.Module,
    attr_class_names: List[str],
    tta: bool = True,
    fake_threshold: float = 0.5
):
    """
    1단계에서 fake 확률 >= fake_threshold 이면 2단계 attribution.
    반환:
      {
        'stage1': {'pred': 'real'|'fake', 'prob_fake': float},
        'stage2': {'pred_model': str|None, 'prob': float|None, 'probs': list|None}
      }
    """
    # 공통 변환
    img = Image.open(img_path).convert('RGB')
    xg = val_tfms_geometric(img)
    xr = residualizer(xg).unsqueeze(0).to(device)

    # ---- Stage 1: real/fake
    binary_model.eval()
    with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
        logit_b = binary_model(xr)
        if tta: logit_b = logit_b + binary_model(torch.flip(xr, dims=[3]))
    prob_fake = float(_softmax_np(logit_b)[1])  # [real, fake] 순으로 가정
    stage1_pred = 'fake' if prob_fake >= fake_threshold else 'real'

    # ---- Stage 2: only if fake
    stage2 = {'pred_model': None, 'prob': None, 'probs': None}
    if stage1_pred == 'fake':
        attr_model.eval()
        with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
            logit_a = attr_model(xr)
            if tta: logit_a = logit_a + attr_model(torch.flip(xr, dims=[3]))
        probs = _softmax_np(logit_a)
        idx = int(np.argmax(probs))
        stage2 = {'pred_model': attr_class_names[idx], 'prob': float(probs[idx]), 'probs': probs.tolist()}

    return {
        'stage1': {'pred': stage1_pred, 'prob_fake': prob_fake},
        'stage2': stage2
    }

# --------------------------
# 6) 평가(예시): 균형 샘플 평가 (혼동행렬/리포트 강화)
# --------------------------
import itertools
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt

def _plot_cm(cm, class_names, title="Confusion Matrix", normalize=True):
    if normalize:
        cm = cm.astype('float') / (cm.sum(axis=1, keepdims=True) + 1e-12)
    fig, ax = plt.subplots(figsize=(6, 5))
    im = ax.imshow(cm, interpolation='nearest')
    ax.figure.colorbar(im, ax=ax)
    ax.set(xticks=np.arange(len(class_names)), yticks=np.arange(len(class_names)),
           xticklabels=class_names, yticklabels=class_names,
           ylabel='True', xlabel='Predicted', title=title)
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
    thresh = cm.max() / 2.0
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, f"{cm[i, j]:.2f}" if normalize else int(cm[i, j]),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    plt.tight_layout()
    plt.show()

def evaluate_two_stage_random(
    dataset_root: str,
    binary_model: nn.Module,
    attr_model: nn.Module,
    attr_class_names: List[str],
    total_samples: int = 30,
    tta: bool = True,
    fake_threshold: float = 0.5
):
    rng = random.Random(42)
    model_names = attr_class_names
    per_bucket = max(1, total_samples // (len(model_names) + 1))  # +1 for real

    rows = []
    # 1단계 이진 지표
    y_true_bin, y_pred_bin = [], []

    # 2단계 출처 지표 (fake GT만 의미)
    #   - attr 전용 성능: (stage1이 fake로 맞춘 샘플만) → "given_fake"
    #   - 엔드투엔드: stage1 실패 포함, 라벨 공간에 "real"을 추가 → "e2e"
    y_true_attr_all, y_pred_attr_all = [], []   # fake GT 전체에 대한 예측(실패 시 -1)
    y_true_attr_given, y_pred_attr_given = [], []  # stage1이 fake로 잡은 케이스만

    print(f"{'fake_p':>7} | {'stage1':>6} | {'stage2':>10} | File")
    print("-"*90)

    # Fake (각 모델에서)
    for m in model_names:
        fake_dir = os.path.join(dataset_root, m, "1_fake")
        if not os.path.isdir(fake_dir):
            continue
        files = _list_images(fake_dir)
        rng.shuffle(files)
        for img_path in files[:per_bucket]:
            out = predict_two_stage(
                img_path, binary_model, attr_model, attr_class_names,
                tta=tta, fake_threshold=fake_threshold
            )
            print(f"{out['stage1']['prob_fake']:7.4f} | {out['stage1']['pred']:>6} | {str(out['stage2']['pred_model']):>10} | {os.path.basename(img_path)}")
            rows.append((out['stage1']['prob_fake'], out['stage1']['pred'], out['stage2']['pred_model'], img_path))

            # Stage-1 (binary)
            y_true_bin.append(1)  # fake
            y_pred_bin.append(1 if out['stage1']['pred']=='fake' else 0)

            # Stage-2 (attribution)
            true_idx = attr_class_names.index(m)
            y_true_attr_all.append(true_idx)
            if out['stage1']['pred']=='fake' and out['stage2']['pred_model'] is not None:
                pred_idx = attr_class_names.index(out['stage2']['pred_model'])
                y_pred_attr_all.append(pred_idx)
                # given_fake (stage1 통과한 것들만)
                y_true_attr_given.append(true_idx)
                y_pred_attr_given.append(pred_idx)
            else:
                # stage1이 real로 분류한 실패 샘플 → -1로 표기 (엔드투엔드 집계용)
                y_pred_attr_all.append(-1)

    # Real (각 모델의 0_real에서 고르게)
    real_paths = list(itertools.chain.from_iterable(
        [_list_images(os.path.join(dataset_root, m, "0_real")) for m in model_names
         if os.path.isdir(os.path.join(dataset_root, m, "0_real"))]
    ))
    rng.shuffle(real_paths)
    for img_path in real_paths[:per_bucket]:
        out = predict_two_stage(
            img_path, binary_model, attr_model, attr_class_names,
            tta=tta, fake_threshold=fake_threshold
        )
        print(f"{out['stage1']['prob_fake']:7.4f} | {out['stage1']['pred']:>6} | {str(out['stage2']['pred_model']):>10} | {os.path.basename(img_path)}")
        rows.append((out['stage1']['prob_fake'], out['stage1']['pred'], out['stage2']['pred_model'], img_path))
        y_true_bin.append(0)  # real
        y_pred_bin.append(1 if out['stage1']['pred']=='fake' else 0)

    # -------------------------
    # Stage-1 Binary 리포트/혼동행렬
    # -------------------------
    print("\n[Stage-1 Binary] (rows=true, cols=pred)")
    print(classification_report(y_true_bin, y_pred_bin, target_names=['real','fake'], digits=3))
    cm_bin = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1])
    print(cm_bin)
    _plot_cm(cm_bin, ['real','fake'], title='[Stage-1] Confusion Matrix (normalized)')

    # -------------------------
    # Stage-2 Attribution 지표/혼동행렬
    # -------------------------
    y_true_attr_all = np.array(y_true_attr_all)
    y_pred_attr_all = np.array(y_pred_attr_all)

    # (a) given fake: stage1이 fake로 잡은 샘플만
    if len(y_true_attr_given) > 0:
        print("\n[Stage-2 Attribution] Classification Report (given stage-1 predicted fake)")
        print(classification_report(
          y_true_attr_given, y_pred_attr_given,
          labels=list(range(len(attr_class_names))),
          target_names=attr_class_names, digits=3, zero_division=0
        ))

        cm_attr_given = confusion_matrix(y_true_attr_given, y_pred_attr_given,
                                         labels=list(range(len(attr_class_names))))
        print("[Stage-2] Confusion Matrix (given fake, counts)")
        print(cm_attr_given)
        _plot_cm(cm_attr_given, attr_class_names,
                 title='[Stage-2] Confusion Matrix (given fake, normalized)')
    else:
        print("\n[Stage-2 Attribution] No samples where stage-1 predicted fake. (given_fake set is empty)")

    # (b) End-to-End: 1단계 실패 포함 (라벨에 'real'을 추가)
    #     - GT: 모두 fake이므로 true label 공간은 1..K (0은 'real')
    #     - Pred: stage1이 real이면 0, 아니면 1..K
    K = len(attr_class_names)
    classes_e2e = ["real"] + list(attr_class_names)
    y_true_e2e = 1 + y_true_attr_all  # 모두 fake GT → +1 오프셋
    y_pred_e2e = []
    for p in y_pred_attr_all:
        if p < 0:  # stage1에서 real로 빠진 경우
            y_pred_e2e.append(0)
        else:
            y_pred_e2e.append(1 + p)
    y_pred_e2e = np.array(y_pred_e2e, dtype=int)

    acc_including_stage1 = float((y_pred_attr_all == y_true_attr_all).mean()) if len(y_true_attr_all)>0 else np.nan
    given_mask = (y_pred_attr_all >= 0)
    acc_given_fake = float((y_pred_attr_all[given_mask] == y_true_attr_all[given_mask]).mean()) if given_mask.sum()>0 else np.nan

    print("\n[Stage-2 Attribution] Accuracy (including stage-1 failures):", np.round(acc_including_stage1, 3))
    print("[Stage-2 Attribution] Accuracy (given stage-1 predicted fake):", np.round(acc_given_fake, 3))

    # E2E 혼동행렬/리포트
    cm_e2e = confusion_matrix(y_true_e2e, y_pred_e2e, labels=list(range(K+1)))
    print("\n[Two-Stage E2E] Confusion Matrix (counts) with 'real' class included")
    print(cm_e2e)
    _plot_cm(cm_e2e, classes_e2e, title='[Two-Stage] End-to-End Confusion Matrix (normalized)')

    # 리포트(각 클래스 지원수가 적을 수 있으니 참고용)
    print("\n[Two-Stage E2E] Classification Report (with 'real' class)")
    print(classification_report(
        y_true_e2e, y_pred_e2e,
        labels=list(range(K+1)),            # 0='real', 1..K=각 출처
        target_names=classes_e2e, digits=3, zero_division=0
    ))

    metrics = {
        "stage1": {
            "cm_counts": cm_bin,
            "report_acc": float((np.array(y_true_bin) == np.array(y_pred_bin)).mean())
        },
        "stage2": {
            "acc_including_stage1": acc_including_stage1,
            "acc_given_fake": acc_given_fake,
            "cm_given_fake_counts": cm_attr_given if len(y_true_attr_given)>0 else None,
            "cm_e2e_counts": cm_e2e
        }
    }

    return rows, metrics

# --------------------------
# 6.5) 전체 검증셋 End-to-End 평가 (1단계→2단계 체인)
# --------------------------
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report

# 필요시 시각화 유틸이 없으면 간단 버전 정의
if "_plot_cm" not in globals():
    import matplotlib.pyplot as plt
    def _plot_cm(cm, class_names, title="Confusion Matrix", normalize=True):
        if normalize:
            cm = cm.astype('float') / (cm.sum(axis=1, keepdims=True) + 1e-12)
        fig, ax = plt.subplots(figsize=(6, 5))
        im = ax.imshow(cm, interpolation='nearest')
        ax.figure.colorbar(im, ax=ax)
        ax.set(xticks=np.arange(len(class_names)), yticks=np.arange(len(class_names)),
               xticklabels=class_names, yticklabels=class_names,
               ylabel='True', xlabel='Predicted', title=title)
        plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
        thresh = cm.max() / 2.0
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                ax.text(j, i, f"{cm[i, j]:.2f}" if normalize else int(cm[i, j]),
                        ha="center", va="center",
                        color="white" if cm[i, j] > thresh else "black")
        plt.tight_layout()
        plt.show()

class _BinaryValE2EDataset(torch.utils.data.Dataset):
    """Binary 검증 분할(=real+fake)을 그대로 쓰되,
       e2e 정답 라벨(0='real', 1..K=각 출처)까지 함께 계산해서 반환"""
    def __init__(self, root, indices, attr_class_names):
        self.base = ResidualBinaryDataset(root, split="train")  # 전체 스캔
        self.idxs = indices
        self.tf = val_tfms_geometric
        self.attr_names = list(attr_class_names)
        self.attr_to_idx = {n:i for i,n in enumerate(self.attr_names)}

    def __len__(self): return len(self.idxs)

    def __getitem__(self, i):
        _, y_bin, p = self.base[self.idxs[i]]  # (PIL 변환 전 경로/라벨)
        img = Image.open(p).convert('RGB')
        x = residualizer(self.tf(img))  # (3,H,W) residual tensor
        # e2e 정답 라벨
        if y_bin == 0:   # real
            y_e2e = 0
        else:            # fake -> 모델명 추출
            rel = os.path.relpath(p, self.base.root)
            model_name = rel.split(os.sep)[0]
            y_e2e = 1 + self.attr_to_idx[model_name]
        return x, y_bin, y_e2e

def evaluate_two_stage_full_val(
    dataset_root: str,
    binary_model: nn.Module,
    attr_model: nn.Module,
    attr_class_names: List[str],
    val_ratio: float = 0.2,
    seed: int = 42,
    batch_size: int = 64,
    tta: bool = True,
    fake_threshold: float = 0.5,
):
    # ---- Binary 검증 분할을 재현(학습 때와 동일 seed/ratio)
    full = ResidualBinaryDataset(dataset_root, split="train")
    train_idx, val_idx = _split_indices(len(full), val_ratio, seed)

    ds = _BinaryValE2EDataset(dataset_root, val_idx, attr_class_names)
    def _collate(batch):
        xs, yb, ye = zip(*batch)
        return torch.stack(xs, 0), torch.tensor(yb, dtype=torch.long), torch.tensor(ye, dtype=torch.long)
    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=2,
                        pin_memory=True, collate_fn=_collate)

    K = len(attr_class_names)
    classes_e2e = ["real"] + list(attr_class_names)

    # 저장 배열
    y_true_bin_all, y_pred_bin_all = [], []
    y_true_e2e_all, y_pred_e2e_all = [], []
    y_true_attr_given, y_pred_attr_given = [], []  # stage-1이 fake로 잡은 케이스만

    binary_model.eval(); attr_model.eval()

    @torch.no_grad()
    def _forward_all(x):
        with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
            lb = binary_model(x)
            if tta: lb = lb + binary_model(torch.flip(x, dims=[3]))
            la = attr_model(x)
            if tta: la = la + attr_model(torch.flip(x, dims=[3]))
        pb = torch.softmax(lb, dim=1)[:,1]              # fake 확률
        pred_bin = (pb >= fake_threshold).long()        # 0=real, 1=fake
        pred_attr = la.argmax(dim=1)                    # 0..K-1
        return pred_bin.cpu().numpy(), pred_attr.cpu().numpy()

    for x, y_bin, y_e2e_true in loader:
        x = x.to(device, non_blocking=True)
        pred_bin, pred_attr = _forward_all(x)

        # Stage-1 저장
        y_true_bin_all.append(y_bin.numpy())
        y_pred_bin_all.append(pred_bin)

        # E2E 예측 라벨 구성
        e2e_pred = np.where(pred_bin==1, 1 + pred_attr, 0)  # 1..K or 0
        y_true_e2e_all.append(y_e2e_true.numpy())
        y_pred_e2e_all.append(e2e_pred)

        # given_fake(1단계 통과)만의 출처 평가
        mask = pred_bin == 1
        if mask.any():
            y_true_attr_given.append((y_e2e_true.numpy()[mask] - 1))  # 0..K-1
            y_pred_attr_given.append(pred_attr[mask])

    # 합치기
    y_true_bin = np.concatenate(y_true_bin_all)
    y_pred_bin = np.concatenate(y_pred_bin_all)
    y_true_e2e = np.concatenate(y_true_e2e_all)
    y_pred_e2e = np.concatenate(y_pred_e2e_all)
    if len(y_true_attr_given) > 0:
        y_true_attr_given = np.concatenate(y_true_attr_given)
        y_pred_attr_given = np.concatenate(y_pred_attr_given)

    # ---- 리포트/혼동행렬 출력
    # 1) Stage-1 (검증 전체)
    print("\n[Full-Val Stage-1] Classification Report")
    print(classification_report(
        y_true_bin, y_pred_bin,
        labels=[0,1], target_names=["real","fake"], digits=3, zero_division=0
    ))
    cm1 = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1])
    print("[Full-Val Stage-1] Confusion Matrix (counts)\n", cm1)
    _plot_cm(cm1, ["real","fake"], title="[Full-Val Stage-1] CM (normalized)")

    # 2) Stage-2 (given Stage-1 predicted fake)
    if len(y_true_attr_given) > 0:
        print("\n[Full-Val Stage-2 | given fake] Classification Report")
        print(classification_report(
            y_true_attr_given, y_pred_attr_given,
            labels=list(range(K)), target_names=attr_class_names, digits=3, zero_division=0
        ))
        cm2 = confusion_matrix(y_true_attr_given, y_pred_attr_given, labels=list(range(K)))
        print("[Full-Val Stage-2 | given fake] Confusion Matrix (counts)\n", cm2)
        _plot_cm(cm2, attr_class_names, title="[Full-Val Stage-2 | given fake] CM (normalized)")
    else:
        print("\n[Full-Val Stage-2 | given fake] No samples (Stage-1 predicted none as fake).")

    # 3) End-to-End (1단계 실패 포함, 라벨= ['real']+K 클래스)
    print("\n[Full-Val Two-Stage E2E] Classification Report")
    print(classification_report(
        y_true_e2e, y_pred_e2e,
        labels=list(range(K+1)), target_names=classes_e2e, digits=3, zero_division=0
    ))
    cm_e2e = confusion_matrix(y_true_e2e, y_pred_e2e, labels=list(range(K+1)))
    print("[Full-Val Two-Stage E2E] Confusion Matrix (counts)\n", cm_e2e)
    _plot_cm(cm_e2e, classes_e2e, title="[Full-Val Two-Stage E2E] CM (normalized)")

    return {
        "cm_stage1": cm1,
        "cm_stage2_given_fake": cm2 if len(y_true_attr_given)>0 else None,
        "cm_e2e": cm_e2e
    }

# Colab 기본 gdown이 있긴 하지만, 여기서는 이미 Drive에 zip 파일이 있으므로 gdown 불필요
!pip -q install gdown==4.6.0

# biggan 폴더 생성 (없으면 생성)
!mkdir -p /content/drive/MyDrive/prnu_detector/dataset/stylegan2

# 압축 해제 (Drive에 있는 biggan.zip을 biggan 폴더 안에 풀기)
!unzip -q -o \
  /content/drive/MyDrive/prnu_detector/dataset/stylegan2.zip \
  -d /content/drive/MyDrive/prnu_detector/dataset/stylegan2/

# (선택) 구조 확인
!find /content/drive/MyDrive/prnu_detector/dataset/stylegan2 -maxdepth 2 -type d | sort | head -n 50

# ================================================
# Residual two‑stage EVAL (NO training) — CM text color fixed
# + NEW: Merge multiple attribution (.pth) files into ONE .pth
#        with a FIXED class order
# + NEW: "unknown" 전용 단일 클래스 학습 유틸 추가
#        - 학습 데이터: /content/drive/MyDrive/prnu_detector/dataset/unknown/0_real
#        - 산출물(pth): /content/drive/MyDrive/prnu_detector/dataset/unknown/_ckpts/unknown_binary_best.pth
#        - 평가/리포트 파일명/출력에서도 unknown~ 접두어 사용
#  - Per‑model .pth 로드 → 평가만 수행
#  - Confusion Matrix 주석 색상 규칙:
#      * 노란색(값이 큰 셀) → **검은색** 텍스트
#      * 보라색(값이 작은 셀) → **흰색** 텍스트
#  - NEW: merge_attr_checkpoints(base_root, target_order)
#          → attr ckpt들을 찾아 지정한 순서(target_order)로 fc 가중치를 재배열해
#            하나의 글로벌 attr .pth를 생성 (classes.json 포함)
#  - UPDATED: Confusion Matrix 그리기 → 첨부 이미지와 동일한 "Blues" 팔레트,
#             각 셀에 [TN/FP/FN/TP + count + (전체 대비 %)] 표기
# ================================================
import os, json, warnings
from typing import List, Tuple, Optional, Dict
from PIL import Image
import numpy as np

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, WeightedRandomSampler
from torchvision import transforms, models

import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix

# ------------------
# Environment / fallbacks
# ------------------
try:
    device
except NameError:
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
PIN_MEM = (device.type == 'cuda')
warnings.filterwarnings('ignore', category=UserWarning, module='torch.utils.data.dataloader')

# Residualizer fallback — accepts either PIL.Image or Tensor
try:
    residualizer
except NameError:
    class _IdentityResidual:
        def __call__(self, img):
            if isinstance(img, torch.Tensor):
                return img
            from torchvision.transforms.functional import to_tensor
            return to_tensor(img)
    residualizer = _IdentityResidual()

try:
    train_tfms_geometric
    val_tfms_geometric
except NameError:
    train_tfms_geometric = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.RandomHorizontalFlip(),
    ])
    val_tfms_geometric = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
    ])

IMG_EXTS = (".jpg", ".jpeg", ".png", ".bmp", ".webp")

# ------------------
# Path helpers
# ------------------

def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)
    return p


def _list_images(d: str) -> List[str]:
    if not os.path.isdir(d):
        return []
    return [os.path.join(d, f) for f in os.listdir(d)
            if os.path.isfile(os.path.join(d, f)) and f.lower().endswith(IMG_EXTS)]


def _find_real_fake_dirs(model_dir: str) -> Optional[Tuple[str, str]]:
    # case 1: <model_dir>/{0_real,1_fake}
    direct_real = os.path.join(model_dir, '0_real')
    direct_fake = os.path.join(model_dir, '1_fake')
    if os.path.isdir(direct_real) or os.path.isdir(direct_fake):
        return direct_real, direct_fake
    # case 2: <model_dir>/<model>/{0_real,1_fake}
    leaf = os.path.basename(os.path.normpath(model_dir))
    nested = os.path.join(model_dir, leaf)
    nested_real = os.path.join(nested, '0_real')
    nested_fake = os.path.join(nested, '1_fake')
    if os.path.isdir(nested_real) or os.path.isdir(nested_fake):
        return nested_real, nested_fake
    return None


def _find_file_recursive(start_dir: str, patterns: List[str]) -> Optional[str]:
    for root, _, files in os.walk(start_dir):
        for f in files:
            if f.endswith('.pth') and all(p in f for p in patterns):
                return os.path.join(root, f)
    return None

# ------------------
# Datasets
# ------------------
class ResidualBinaryDataset(torch.utils.data.Dataset):
    def __init__(self, model_root: str, split: str = "train"):
        super().__init__()
        rf = _find_real_fake_dirs(model_root)
        if rf is None:
            raise RuntimeError(f"No 0_real/1_fake under {model_root}")
        real_dir, fake_dir = rf
        self.samples = [(p,0) for p in _list_images(real_dir)] + [(p,1) for p in _list_images(fake_dir)]
        if not self.samples:
            raise RuntimeError(f"No images under {model_root}")
        self.tf = train_tfms_geometric if split=='train' else val_tfms_geometric
    def __len__(self): return len(self.samples)
    def __getitem__(self, i):
        p,y = self.samples[i]
        img = Image.open(p).convert('RGB'); img = self.tf(img)
        return img,y,p

class SingleClassUnknownDataset(torch.utils.data.Dataset):
    """
    unknown 전용(단일 클래스) 학습을 위해 0_real 폴더만 사용합니다.
    반환 라벨은 모두 0 하나로 고정하여, 단일 클래스 분류 헤드를 학습합니다.
    """
    def __init__(self, root_dir: str, split: str='train'):
        super().__init__()
        self.real_dir = os.path.join(root_dir, '0_real')
        if not os.path.isdir(self.real_dir):
            raise RuntimeError(f"No 0_real under {root_dir}")
        self.samples = _list_images(self.real_dir)
        if not self.samples:
            raise RuntimeError(f"No images in {self.real_dir}")
        self.tf = train_tfms_geometric if split=='train' else val_tfms_geometric
    def __len__(self): return len(self.samples)
    def __getitem__(self, i):
        p = self.samples[i]
        img = Image.open(p).convert('RGB'); img = self.tf(img)
        return img, 0, p  # label=0

class ResidualAttributionDataset(torch.utils.data.Dataset):
    def __init__(self, base_root: str, model_names: List[str], split: str='train'):
        super().__init__()
        self.model_names = [m for m in model_names if os.path.isdir(os.path.join(base_root, m))]
        self.class_to_idx = {m:i for i,m in enumerate(self.model_names)}
        self.samples = []
        for m in self.model_names:
            mdir = os.path.join(base_root, m)
            rf = _find_real_fake_dirs(mdir)
            if rf is None: continue
            _r, fake_dir = rf
            self.samples += [(p, self.class_to_idx[m]) for p in _list_images(fake_dir)]
        if not self.samples:
            raise RuntimeError("No fake images for attribution")
        self.tf = train_tfms_geometric if split=='train' else val_tfms_geometric
    def __len__(self): return len(self.samples)
    def __getitem__(self, i):
        p,y = self.samples[i]
        img = Image.open(p).convert('RGB'); img = self.tf(img)
        return img,y,p

# ------------------
# Loaders
# ------------------

def _split_indices(n:int, val_ratio:float=0.2, seed:int=42):
    g = torch.Generator().manual_seed(seed)
    idx = torch.randperm(n, generator=g).tolist()
    n_val = max(1, int(n*val_ratio))
    return idx[n_val:], idx[:n_val]


def make_binary_loaders_for_model(model_root:str, batch_size=32, num_workers=2, val_ratio=0.2, seed=42):
    full = ResidualBinaryDataset(model_root, split='train')
    tr_idx, va_idx = _split_indices(len(full), val_ratio, seed)

    # subset views so we don't reload full images in __getitem__ just to get labels
    class Subset(torch.utils.data.Dataset):
        def __init__(self, base, idxs, split):
            self.base=base; self.idxs=idxs
            self.tf = train_tfms_geometric if split=='train' else val_tfms_geometric
        def __len__(self): return len(self.idxs)
        def __getitem__(self, i):
            _,y,p = self.base[self.idxs[i]]
            img = Image.open(p).convert('RGB'); img=self.tf(img); return img,y,p

    tr_ds = Subset(full, tr_idx, 'train')
    va_ds = Subset(full, va_idx, 'val')

    # --- FIX: Sampler weights must match the TRAIN subset length ---
    labels_full = [y for (_p,y) in [(p,y) for (p,y) in [(s[0], s[1]) for s in full.samples]]]
    # Simpler: harvest directly
    labels_full = [y for (_p,y) in full.samples]
    labels_tr = [labels_full[i] for i in tr_idx]
    K = max(labels_tr)+1 if labels_tr else 2
    counts = [1]*K
    for y in labels_tr: counts[y]+=1
    class_w = torch.tensor([1.0/c for c in counts], dtype=torch.float)
    sample_w = [class_w[y].item() for y in labels_tr]
    sampler = WeightedRandomSampler(sample_w, num_samples=len(sample_w), replacement=True)

    def _collate(b):
        imgs,labels,_ = zip(*b)
        x = torch.stack([residualizer(im) for im in imgs],0)
        y = torch.tensor(labels, dtype=torch.long)
        return x,y

    tr = DataLoader(tr_ds, batch_size=batch_size, sampler=sampler, num_workers=num_workers, pin_memory=PIN_MEM, collate_fn=_collate)
    va = DataLoader(va_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=PIN_MEM, collate_fn=_collate)
    return tr, va, va_idx


def make_unknown_loaders(unknown_root: str, batch_size=32, num_workers=2, val_ratio=0.2, seed=42):
    full = SingleClassUnknownDataset(unknown_root, split='train')
    tr_idx, va_idx = _split_indices(len(full), val_ratio, seed)

    class Subset(torch.utils.data.Dataset):
        def __init__(self, base, idxs, split):
            self.base=base; self.idxs=idxs
            self.tf = train_tfms_geometric if split=='train' else val_tfms_geometric
        def __len__(self): return len(self.idxs)
        def __getitem__(self, i):
            _,_,p = self.base[self.idxs[i]]
            img = Image.open(p).convert('RGB'); img=self.tf(img); return img,0,p

    tr_ds = Subset(full, tr_idx, 'train')
    va_ds = Subset(full, va_idx, 'val')

    def _collate(b):
        imgs,labels,_ = zip(*b)
        x = torch.stack([residualizer(im) for im in imgs],0)
        y = torch.zeros(len(labels), dtype=torch.long)  # 모두 0
        return x,y

    tr = DataLoader(tr_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=PIN_MEM, collate_fn=_collate)
    va = DataLoader(va_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=PIN_MEM, collate_fn=_collate)
    return tr, va, va_idx

# ------------------
# Model
# ------------------
class ResNetHead(nn.Module):
    def __init__(self, num_classes:int):
        super().__init__()
        self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        in_f = self.backbone.fc.in_features
        self.backbone.fc = nn.Linear(in_f, num_classes)
    def forward(self, x):
        return self.backbone(x)

# ------------------
# Helper: draw CM like example image (Blues + TN/FP/FN/TP + counts + global %)
# ------------------

def _draw_confusion_matrix_like_example(cm: np.ndarray, title: str, out_path: str):
    """Draws a 2x2 confusion matrix using a Blues colormap and annotates each cell with
    TN/FP/FN/TP, the raw count, and the global percent.

    NOTE (bugfix): 이전 코드에서 f-string을 줄바꿈으로 나눠 적어 파서가 블록을 잘못 인식해
    IndentationError가 났습니다. '\n'으로 명시적인 줄바꿈을 넣어 해결했습니다. 또한
    "값이 큰 셀 = 검은색 텍스트" 규칙에 맞게 색상 조건을 수정했습니다.
    """
    assert cm.shape == (2,2), "This drawer expects a 2x2 confusion matrix."
    total = cm.sum()
    perc = cm.astype(float) / total if total>0 else np.zeros_like(cm, dtype=float)

    fig, ax = plt.subplots(figsize=(6,5))
    im = ax.imshow(cm, cmap='Blues')
    cbar = ax.figure.colorbar(im, ax=ax)
    cbar.ax.set_ylabel('Count', rotation=-90, va='bottom')

    ax.set_xticks([0,1]); ax.set_yticks([0,1])
    ax.set_xticklabels(['0','1']); ax.set_yticklabels(['0','1'])
    ax.set_xlabel('Predicted Label'); ax.set_ylabel('True Label')
    ax.set_title(title)

    cell_titles = [["TN", "FP (type 1 error)"],["FN (type 2 error)", "TP"]]
    vmax = cm.max() if cm.max()>0 else 1
    thresh = vmax / 2.0
    for i in range(2):
        for j in range(2):
            val = int(cm[i, j])
            pct = float(perc[i, j] * 100.0)
            # 큰 값 = 검은색 텍스트, 작은 값 = 흰색 텍스트
            color = 'black' if val > thresh else 'white'
            text = f"{cell_titles[i][j]}\n{val}\n({pct:.1f}%)"
            ax.text(j, i, text, ha='center', va='center', color=color)

    fig.tight_layout()
    fig.savefig(out_path, dpi=160, bbox_inches='tight')
    plt.close(fig)

# ------------------
# Unknown 전용 학습 & 저장 유틸
# ------------------

def train_unknown_and_save(unknown_root: str, out_dir: str, epochs: int=3, lr: float=1e-4, wd: float=1e-4, seed: int=42) -> str:
    """
    /unknown/0_real 이미지를 단일 클래스(unknown)로 학습하여 unknown_binary_best.pth 생성.
    - 분류 헤드 차원: num_classes=1(로짓) 대신 2-class로 유지하여 추후 파이프라인 호환성 보장
      (label=0: unknown-real, label=1: dummy)
    - 실제 학습은 클래스 0 샘플만 사용하므로, 클래스 1의 fc 가중치는 0으로 유지
    """
    torch.manual_seed(seed); np.random.seed(seed)
    ensure_dir(out_dir)

    # 로더 구성: unknown의 0_real만 사용 → 배치마다 y=0
    tr, va, _ = make_unknown_loaders(unknown_root, batch_size=32, num_workers=2, val_ratio=0.2, seed=seed)

    net = ResNetHead(2).to(device)
    opt = torch.optim.AdamW(net.parameters(), lr=lr, weight_decay=wd)
    loss_fn = nn.CrossEntropyLoss()

    best_state = None; best_loss = float('inf')
    for ep in range(1, epochs+1):
        net.train(); tr_loss=0.0
        for xb, yb in tr:
            xb = xb.to(device); yb = yb.to(device)
            with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
                logits = net(xb)
                loss = loss_fn(logits, yb)
            opt.zero_grad(); loss.backward(); opt.step()
            tr_loss += loss.item()*xb.size(0)
        tr_loss /= max(1, len(tr.dataset))

        # 간단한 val: 동일 분포, y=0
        net.eval(); va_loss=0.0
        with torch.no_grad():
            for xb, yb in va:
                xb = xb.to(device); yb = yb.to(device)
                with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
                    logits = net(xb)
                    loss = loss_fn(logits, yb)
                va_loss += loss.item()*xb.size(0)
        va_loss /= max(1, len(va.dataset))
        print(f"[unknown][ep {ep}] train_loss={tr_loss:.4f}  val_loss={va_loss:.4f}")
        if va_loss < best_loss:
            best_loss = va_loss
            best_state = {k: v.detach().cpu().clone() for k,v in net.state_dict().items()}

    # 저장 파일명: unknown_binary_best.pth
    pth_path = os.path.join(out_dir, 'unknown_binary_best.pth')
    torch.save(best_state if best_state is not None else net.state_dict(), pth_path)
    print(f"[unknown] Saved: {pth_path}")
    return pth_path

# ------------------
# (이하 기존 병합/평가 유틸 — 필요시 그대로 사용)
# ------------------
# TODO: merge_attr_checkpoints, merge_binary_checkpoints, eval_* 함수들을 이어서 붙여 사용하세요.

# ================================
# MAIN (예시)
# ================================
if __name__ == '__main__':
    BASE_DATASET_ROOT = "/content/drive/MyDrive/prnu_detector/dataset"
    UNKNOWN_ROOT = os.path.join(BASE_DATASET_ROOT, 'unknown')  # 학습 데이터: unknown/0_real
    UNKNOWN_CKPT_DIR = os.path.join(UNKNOWN_ROOT, '_ckpts')

    ensure_dir(UNKNOWN_CKPT_DIR)
    # 1) unknown 전용 학습 및 pth 저장 (파일명: unknown_binary_best.pth)
    train_unknown_and_save(
        unknown_root=UNKNOWN_ROOT,
        out_dir=UNKNOWN_CKPT_DIR,
        epochs=3,
        lr=1e-4,
        wd=1e-4,
        seed=42,
    )

    # 2) 이후 사용자는 기존 파이프라인의 병합/평가 루틴을 호출 가능
    #    - 예) merge_binary_checkpoints(...) 호출 시 unknown을 TARGET_ORDER에 포함시키면 멀티클래스에 합류
    #    - 또는 eval_stage1_for_model('unknown', UNKNOWN_ROOT, UNKNOWN_CKPT_DIR, ...) 로 리포트 생성

# ================================================
# Residual two‑stage EVAL (NO training) — CM text color fixed
# + NEW: Merge multiple attribution (.pth) files into ONE .pth
#        with a FIXED class order
#  - Per‑model .pth 로드 → 평가만 수행
#  - Confusion Matrix 주석 색상 규칙:
#      * 노란색(값이 큰 셀) → **검은색** 텍스트
#      * 보라색(값이 작은 셀) → **흰색** 텍스트
#  - NEW: merge_attr_checkpoints(base_root, target_order)
#          → attr ckpt들을 찾아 지정한 순서(target_order)로 fc 가중치를 재배열해
#            하나의 글로벌 attr .pth를 생성 (classes.json 포함)
#  - UPDATED: Confusion Matrix 그리기 → 첨부 이미지와 동일한 "Blues" 팔레트,
#             각 셀에 [TN/FP/FN/TP + count + (전체 대비 %)] 표기
# ================================================
import os, json, warnings
from typing import List, Tuple, Optional, Dict
from PIL import Image
import numpy as np

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, WeightedRandomSampler
from torchvision import transforms, models

import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix

# ------------------
# Environment / fallbacks
# ------------------
try:
    device
except NameError:
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
PIN_MEM = (device.type == 'cuda')
warnings.filterwarnings('ignore', category=UserWarning, module='torch.utils.data.dataloader')

# Residualizer fallback
try:
    residualizer
except NameError:
    class _IdentityResidual:
        def __call__(self, img: Image.Image):
            from torchvision import transforms as _T
            return _T.functional.to_tensor(img)
    residualizer = _IdentityResidual()

try:
    train_tfms_geometric
    val_tfms_geometric
except NameError:
    train_tfms_geometric = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.RandomHorizontalFlip(),
    ])
    val_tfms_geometric = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
    ])

IMG_EXTS = (".jpg", ".jpeg", ".png", ".bmp", ".webp")

# ------------------
# Path helpers
# ------------------

def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)
    return p


def _list_images(d: str) -> List[str]:
    if not os.path.isdir(d):
        return []
    return [os.path.join(d, f) for f in os.listdir(d)
            if os.path.isfile(os.path.join(d, f)) and f.lower().endswith(IMG_EXTS)]


def _find_real_fake_dirs(model_dir: str) -> Optional[Tuple[str, str]]:
    # case 1: <model_dir>/{0_real,1_fake}
    direct_real = os.path.join(model_dir, '0_real')
    direct_fake = os.path.join(model_dir, '1_fake')
    if os.path.isdir(direct_real) or os.path.isdir(direct_fake):
        return direct_real, direct_fake
    # case 2: <model_dir>/<model>/{0_real,1_fake}
    leaf = os.path.basename(os.path.normpath(model_dir))
    nested = os.path.join(model_dir, leaf)
    nested_real = os.path.join(nested, '0_real')
    nested_fake = os.path.join(nested, '1_fake')
    if os.path.isdir(nested_real) or os.path.isdir(nested_fake):
        return nested_real, nested_fake
    return None


def _find_file_recursive(start_dir: str, patterns: List[str]) -> Optional[str]:
    for root, _, files in os.walk(start_dir):
        for f in files:
            if f.endswith('.pth') and all(p in f for p in patterns):
                return os.path.join(root, f)
    return None

# ------------------
# Datasets
# ------------------
class ResidualBinaryDataset(torch.utils.data.Dataset):
    def __init__(self, model_root: str, split: str = "train"):
        super().__init__()
        rf = _find_real_fake_dirs(model_root)
        if rf is None:
            raise RuntimeError(f"No 0_real/1_fake under {model_root}")
        real_dir, fake_dir = rf
        self.samples = [(p,0) for p in _list_images(real_dir)] + [(p,1) for p in _list_images(fake_dir)]
        if not self.samples:
            raise RuntimeError(f"No images under {model_root}")
        self.tf = train_tfms_geometric if split=='train' else val_tfms_geometric
    def __len__(self): return len(self.samples)
    def __getitem__(self, i):
        p,y = self.samples[i]
        img = Image.open(p).convert('RGB'); img = self.tf(img)
        return img,y,p

class ResidualAttributionDataset(torch.utils.data.Dataset):
    def __init__(self, base_root: str, model_names: List[str], split: str='train'):
        super().__init__()
        self.model_names = [m for m in model_names if os.path.isdir(os.path.join(base_root, m))]
        self.class_to_idx = {m:i for i,m in enumerate(self.model_names)}
        self.samples = []
        for m in self.model_names:
            mdir = os.path.join(base_root, m)
            rf = _find_real_fake_dirs(mdir)
            if rf is None: continue
            _r, fake_dir = rf
            self.samples += [(p, self.class_to_idx[m]) for p in _list_images(fake_dir)]
        if not self.samples:
            raise RuntimeError("No fake images for attribution")
        self.tf = train_tfms_geometric if split=='train' else val_tfms_geometric
    def __len__(self): return len(self.samples)
    def __getitem__(self, i):
        p,y = self.samples[i]
        img = Image.open(p).convert('RGB'); img = self.tf(img)
        return img,y,p

# ------------------
# Loaders
# ------------------

def _split_indices(n:int, val_ratio:float=0.2, seed:int=42):
    g = torch.Generator().manual_seed(seed)
    idx = torch.randperm(n, generator=g).tolist()
    n_val = max(1, int(n*val_ratio))
    return idx[n_val:], idx[:n_val]


def make_binary_loaders_for_model(model_root:str, batch_size=32, num_workers=2, val_ratio=0.2, seed=42):
    full = ResidualBinaryDataset(model_root, split='train')
    tr_idx, va_idx = _split_indices(len(full), val_ratio, seed)
    class Subset(torch.utils.data.Dataset):
        def __init__(self, base, idxs, split):
            self.base=base; self.idxs=idxs
            self.tf = train_tfms_geometric if split=='train' else val_tfms_geometric
        def __len__(self): return len(self.idxs)
        def __getitem__(self, i):
            _,y,p = self.base[self.idxs[i]]
            img = Image.open(p).convert('RGB'); img=self.tf(img); return img,y,p
    tr_ds = Subset(full, tr_idx, 'train'); va_ds = Subset(full, va_idx, 'val')
    def _collate(b):
        imgs,labels,_ = zip(*b)
        x = torch.stack([residualizer(im) for im in imgs],0)
        y = torch.tensor(labels, dtype=torch.long)
        return x,y
    labels = [full[i][1] for i in range(len(full))]
    K = max(labels)+1; counts=[1]*K
    for y in labels: counts[y]+=1
    class_w = torch.tensor([1.0/c for c in counts])
    sample_w = [class_w[y].item() for y in labels]
    sampler = WeightedRandomSampler(sample_w, num_samples=len(sample_w), replacement=True)
    tr = DataLoader(tr_ds, batch_size=batch_size, sampler=sampler, num_workers=num_workers, pin_memory=PIN_MEM, collate_fn=_collate)
    va = DataLoader(va_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=PIN_MEM, collate_fn=_collate)
    return tr, va, va_idx


def make_attr_val_loader(base_root:str, model_names:List[str], batch_size=32, num_workers=2, val_ratio=0.2, seed=42):
    full = ResidualAttributionDataset(base_root, model_names, split='train')
    _, va_idx = _split_indices(len(full), val_ratio, seed)
    class Subset(torch.utils.data.Dataset):
        def __init__(self, base, idxs):
            self.base=base; self.idxs=idxs; self.tf=val_tfms_geometric
        def __len__(self): return len(self.idxs)
        def __getitem__(self,i):
            _,y,p = self.base[self.idxs[i]]
            img = Image.open(p).convert('RGB'); img=self.tf(img); return img,y,p
    ds = Subset(full, va_idx)
    def _collate(b):
        imgs,labels,paths = zip(*b)
        x = torch.stack([residualizer(im) for im in imgs],0)
        y = torch.tensor(labels, dtype=torch.long)
        return x,y,list(paths)
    ld = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=PIN_MEM, collate_fn=_collate)
    return ld, full.model_names

# ------------------
# Model
# ------------------
class ResNetHead(nn.Module):
    def __init__(self, num_classes:int):
        super().__init__()
        self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        in_f = self.backbone.fc.in_features
        self.backbone.fc = nn.Linear(in_f, num_classes)
    def forward(self, x):
        return self.backbone(x)

# ------------------
# Helper: draw CM like example image (Blues + TN/FP/FN/TP + counts + global %)
# ------------------

def _draw_confusion_matrix_like_example(cm: np.ndarray, title: str, out_path: str):
    """
    cm: 2x2 confusion matrix with order [[TN, FP], [FN, TP]] using labels=[0,1]
    Renders with 'Blues' colormap, colorbar, and annotations:
      TN, FP (type 1 error), FN (type 2 error), TP
      <count> and (<global %>) where % is out of total samples
    """
    assert cm.shape == (2,2), "This drawer expects a 2x2 confusion matrix."
    total = cm.sum()
    perc = cm.astype(float) / total if total>0 else np.zeros_like(cm, dtype=float)

    fig, ax = plt.subplots(figsize=(6,5))
    im = ax.imshow(cm, cmap='Blues')  # <— match example color style
    cbar = ax.figure.colorbar(im, ax=ax)
    cbar.ax.set_ylabel('Count', rotation=-90, va='bottom')

    ax.set_xticks([0,1]); ax.set_yticks([0,1])
    ax.set_xticklabels(['0','1']); ax.set_yticklabels(['0','1'])
    ax.set_xlabel('Predicted Label'); ax.set_ylabel('True Label')
    ax.set_title(title)

    # Annotation labels per cell
    cell_titles = [["TN", "FP (type 1 error)"],
                   ["FN (type 2 error)", "TP"]]

    # Choose white text on darker cells for readability
    vmax = cm.max() if cm.max()>0 else 1
    thresh = vmax / 2.0

    for i in range(2):
        for j in range(2):
            val = cm[i, j]
            pct = perc[i, j] * 100.0
            color = 'white' if val > thresh else 'black'
            text = f"{cell_titles[i][j]}\n{val}\n({pct:.1f}%)"
            ax.text(j, i, text, ha='center', va='center', color=color)

    fig.tight_layout()
    fig.savefig(out_path, dpi=160, bbox_inches='tight')
    plt.close(fig)

# ------------------
# Eval helpers
# ------------------

def eval_stage1_for_model(model_name:str, model_root:str, ckpt_dir:str, val_ratio=0.2, seed=42):
    print(f"\n[Stage‑1] Evaluating model: {model_name}")
    _, val_loader, _ = make_binary_loaders_for_model(model_root, 32, 2, val_ratio, seed)

    # find binary checkpoint
    bin_ckpt = None
    if os.path.isdir(ckpt_dir):
        bin_ckpt = _find_file_recursive(ckpt_dir, ["binary","best"]) or _find_file_recursive(ckpt_dir, ["best"])
    if not bin_ckpt:
        raise FileNotFoundError(f"No binary BEST checkpoint under {ckpt_dir}")

    net = ResNetHead(2).to(device)
    state = torch.load(bin_ckpt, map_location=device)
    try: net.load_state_dict(state if isinstance(state, dict) else state['model'])
    except Exception: net.load_state_dict(state.get('model', state), strict=False)
    net.eval()

    y_true, y_pred = [], []
    with torch.no_grad():
        for xb, yb in val_loader:
            xb = xb.to(device)
            with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
                logits = net(xb)
                pred = logits.argmax(1)
            y_true.extend(yb.tolist()); y_pred.extend(pred.cpu().tolist())
    y_true = np.array(y_true); y_pred = np.array(y_pred)

    rep = classification_report(y_true, y_pred, labels=[0,1], target_names=["real","fake"], digits=3, zero_division=0)
    print(rep)

    ensure_dir(ckpt_dir)
    txt = os.path.join(ckpt_dir, f"eval_stage1_{model_name}_classification_report.txt")
    with open(txt,'w') as f:
        f.write(f"[Stage‑1] {model_name} Classification Report\n"); f.write(rep)

    fig, ax = plt.subplots(figsize=(8,4.5)); ax.axis('off')
    ax.text(0,1,"[Stage‑1] Classification Report", fontsize=12, fontfamily='monospace', va='top')
    ax.text(0,0.95,rep, fontsize=11, fontfamily='monospace', va='top')
    fig.tight_layout(); png = os.path.join(ckpt_dir, f"eval_stage1_{model_name}_classification_report.png")
    fig.savefig(png, dpi=150, bbox_inches='tight'); plt.close(fig)

    # Confusion Matrix — match example image style (Blues + TN/FP/FN/TP + global %)
    cm = confusion_matrix(y_true, y_pred, labels=[0,1])  # [[TN, FP],[FN, TP]] with labels=[0,1]
    cm_png = os.path.join(ckpt_dir, f"eval_stage1_{model_name}_cm_blues_like_example.png")
    _draw_confusion_matrix_like_example(cm, title='Confusion Matrix', out_path=cm_png)

    return {'report_txt': txt, 'report_png': png, 'cm_png': cm_png, 'bin_ckpt': bin_ckpt}


def try_find_global_attr_checkpoint(base_root: str) -> Tuple[Optional[str], Optional[List[str]]]:
    candidates = []
    for hint in [os.path.join(base_root, '_ckpts_multi'), base_root]:
        if os.path.isdir(hint):
            p = _find_file_recursive(hint, ["attr","best"])
            if p: candidates.append(p)
    for m in os.listdir(base_root):
        ck = os.path.join(base_root, m, '_ckpts')
        if os.path.isdir(ck):
            p = _find_file_recursive(ck, ["attr","best"])
            if p: candidates.append(p)
    attr_ckpt = candidates[0] if candidates else None

    attr_classes = None
    if attr_ckpt:
        base_dir = os.path.dirname(attr_ckpt)
        for name in ["attr_classes.json","classes.json"]:
            j = os.path.join(base_dir, name)
            if os.path.exists(j):
                try:
                    with open(j,'r') as f: attr_classes = json.load(f)
                    break
                except Exception: pass
    return attr_ckpt, attr_classes


def eval_global_stage2(base_root: str, model_names: List[str], attr_ckpt: str, attr_classes: Optional[List[str]], val_ratio=0.2, seed=42):
    print("\n[Stage‑2] Global attribution evaluation across all models")
    val_loader, discovered = make_attr_val_loader(base_root, model_names, 32, 2, val_ratio, seed)
    class_names = attr_classes or discovered

    net = ResNetHead(len(class_names)).to(device)
    st = torch.load(attr_ckpt, map_location=device)
    try: net.load_state_dict(st if isinstance(st, dict) else st['model'])
    except Exception: net.load_state_dict(st.get('model', st), strict=False)
    net.eval()

    y_true, y_pred = [], []
    with torch.no_grad():
        for xb, yb, _ in val_loader:
            xb = xb.to(device)
            with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
                logits = net(xb); pred = logits.argmax(1)
            y_true.extend(yb.tolist()); y_pred.extend(pred.cpu().tolist())
    y_true = np.array(y_true); y_pred = np.array(y_pred)

    rep = classification_report(y_true, y_pred, labels=list(range(len(class_names))), target_names=class_names, digits=3, zero_division=0)
    print(rep)

    out_dir = ensure_dir(os.path.join(base_root, '_ckpts_multi'))
    txt = os.path.join(out_dir, 'stage2_classification_report.txt')
    with open(txt,'w') as f:
        f.write('[Loaded Model] Classification Report (Stage‑2)\n'); f.write(rep)
    fig, ax = plt.subplots(figsize=(8,6)); ax.axis('off')
    ax.text(0,1,'[Loaded Model] Classification Report', fontsize=12, fontfamily='monospace', va='top')
    ax.text(0,0.95,rep, fontsize=10.5, fontfamily='monospace', va='top')
    fig.tight_layout(); png = os.path.join(out_dir, 'stage2_classification_report_like_screenshot.png')
    fig.savefig(png, dpi=150, bbox_inches='tight'); plt.close(fig)
    return {'report_txt': txt, 'report_png': png}


def per_image_csv_with_optional_attr(model_name: str, model_root: str, ckpt_dir: str, bin_ckpt: str, attr_ckpt: Optional[str], attr_classes: Optional[List[str]], val_ratio=0.2, seed=42):
    full = ResidualBinaryDataset(model_root, split='train')
    _, val_idx = _split_indices(len(full), val_ratio, seed)
    class ValPaths(torch.utils.data.Dataset):
        def __init__(self, base, idxs):
            self.base=base; self.idxs=idxs; self.tf=val_tfms_geometric
        def __len__(self): return len(self.idxs)
        def __getitem__(self,i):
            _,y,p = self.base[self.idxs[i]]
            img = Image.open(p).convert('RGB'); img=self.tf(img); return img,y,p
    ds = ValPaths(full, val_idx)
    def _collate(b):
        imgs,labels,paths = zip(*b)
        x = torch.stack([residualizer(im) for im in imgs],0)
        y = torch.tensor(labels, dtype=torch.long)
        return x,y,list(paths)
    ld = DataLoader(ds, batch_size=64, shuffle=False, num_workers=2, pin_memory=PIN_MEM, collate_fn=_collate)

    bin_net = ResNetHead(2).to(device)
    st = torch.load(ckpt_dir if isinstance(ckpt_dir, dict) else bin_ckpt, map_location=device)
    try: bin_net.load_state_dict(st if isinstance(st, dict) else st['model'])
    except Exception: bin_net.load_state_dict(st.get('model', st), strict=False)
    bin_net.eval()

    # optional attr
    attr_net = None
    if attr_ckpt:
        cls = attr_classes or []
        if not cls:
            base_dir = os.path.dirname(attr_ckpt)
            for nm in ['attr_classes.json','classes.json']:
                j = os.path.join(base_dir, nm)
                if os.path.exists(j):
                    try:
                        cls = json.load(open(j,'r'))
                        break
                    except Exception:
                        pass
        if cls:
            attr_net = ResNetHead(len(cls)).to(device)
            st2 = torch.load(attr_ckpt, map_location=device)
            try: attr_net.load_state_dict(st2 if isinstance(st2, dict) else st2['model'])
            except Exception: attr_net.load_state_dict(st2.get('model', st2), strict=False)
            attr_net.eval(); attr_classes = cls

    rows = []
    print(f"{'fake_p':>7} | {'stage1':>6} | {'stage2':>14} | File  ({model_name})")
    print('-'*100)
    with torch.no_grad():
        for xb, yb, paths in ld:
            xb = xb.to(device)
            with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
                lb = bin_net(xb); pb = torch.softmax(lb, dim=1)[:,1]
            pred_b = (pb>=0.5).long()

            if attr_net is not None and (pred_b==1).any():
                with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
                    la = attr_net(xb[pred_b==1]); pa = torch.softmax(la, dim=1)
                stage2_name=[None]*xb.size(0); stage2_prob=[None]*xb.size(0)
                j=0
                for i in range(xb.size(0)):
                    if pred_b[i].item()==1:
                        kk = int(pa[j].argmax().item())
                        stage2_name[i] = attr_classes[kk]
                        stage2_prob[i] = float(pa[j, kk].item())
                        j+=1
            else:
                stage2_name=[None]*xb.size(0); stage2_prob=[None]*xb.size(0)

            for i,pth in enumerate(paths):
                pf=float(pb[i].item()); s1=('fake' if pred_b[i].item()==1 else 'real')
                s2=(f"{stage2_name[i]} ({stage2_prob[i]:.3f})" if stage2_name[i] is not None else '-')
                print(f"{pf:7.4f} | {s1:>6} | {s2:>14} | {os.path.basename(pth)}")
                rows.append({'file': pth, 'prob_fake': pf, 'pred_stage1': s1, 'pred_stage2': stage2_name[i], 'prob_stage2': stage2_prob[i]})

    import pandas as pd
    csv = os.path.join(ckpt_dir, f"eval_{model_name}_per_image_val_predictions.csv")
    pd.DataFrame(rows).to_csv(csv, index=False)
    return csv

# ------------------
# NEW: Merge multiple attr checkpoints into ONE with fixed class order
# ------------------

def _load_classes_sibling_json(ckpt_path: str) -> Optional[List[str]]:
    base_dir = os.path.dirname(ckpt_path)
    for nm in ['attr_classes.json', 'classes.json']:
        j = os.path.join(base_dir, nm)
        if os.path.exists(j):
            try:
                with open(j, 'r') as f:
                    return json.load(f)
            except Exception:
                pass
    return None


def _extract_fc_and_in_features(state: Dict[str, torch.Tensor]) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[int]]:
    # Support keys like 'backbone.fc.weight' / 'backbone.fc.bias'
    w_key, b_key = None, None
    for k in state.keys():
        if k.endswith('fc.weight'):
            w_key = k
        elif k.endswith('fc.bias'):
            b_key = k
    if w_key is None or b_key is None:
        return None, None, None
    W = state[w_key]  # [C, D]
    b = state[b_key]  # [C]
    in_f = W.shape[1]
    return W, b, in_f


def merge_attr_checkpoints(base_root: str, target_order: List[str], out_dir: Optional[str] = None, init_missing: str = 'zeros') -> Tuple[str, str]:
    """
    base_root 이하에서 'attr' && 'best' 포함하는 .pth를 모두 수집하고,
    각 ckpt 의 classes.json(또는 attr_classes.json) 을 이용해 행(class) → 모델명 매핑을 알아낸 뒤,
    지정한 target_order 순서로 fc.weight/bias 행을 재배치하여 하나의 글로벌 ckpt 를 생성합니다.

    - target_order: 최종 클래스 순서 (고정)
    - init_missing: 'zeros' | 'xavier' — 타 ckpt들에서 가중치를 못 찾은 클래스는 어떻게 초기화할지

    반환: (merged_ckpt_path, classes_json_path)
    """
    # 1) 후보 ckpt 수집
    candidates = []
    for hint in [os.path.join(base_root, '_ckpts_multi'), base_root]:
        if os.path.isdir(hint):
            p = _find_file_recursive(hint, ['attr','best'])
            if p: candidates.append(p)
    for m in os.listdir(base_root):
        ck = os.path.join(base_root, m, '_ckpts')
        if os.path.isdir(ck):
            p = _find_file_recursive(ck, ['attr','best'])
            if p: candidates.append(p)

    if not candidates:
        raise FileNotFoundError('No attr/best checkpoints found to merge.')

    # 2) 템플릿 ckpt 선택 및 in_features 확인
    template_state = None
    template_in_f = None
    template_keys = None

    # class_name → (W_row, b_val)
    rows: Dict[str, Tuple[torch.Tensor, torch.Tensor]] = {}

    for ckp in candidates:
        st = torch.load(ckp, map_location='cpu')
        sd = st if isinstance(st, dict) else st.get('model', st)
        classes = _load_classes_sibling_json(ckp)
        if not isinstance(sd, dict) or classes is None:
            continue
        W, b, in_f = _extract_fc_and_in_features(sd)
        if W is None:
            continue
        if template_state is None:
            template_state = sd
            template_in_f = in_f
            template_keys = list(sd.keys())
        # in_features 불일치 ckpt 는 skip
        if in_f != template_in_f:
            continue
        # 행 매핑 수집
        for idx, name in enumerate(classes):
            if name not in rows:
                rows[name] = (W[idx].clone(), b[idx].clone())

    if template_state is None or template_in_f is None:
        raise RuntimeError('Failed to establish a template attribution checkpoint.')

    # 3) target_order 순서로 최종 W*, b* 구성
    C = len(target_order)
    W_new = torch.zeros((C, template_in_f))
    b_new = torch.zeros((C,))

    if init_missing == 'xavier':
        nn.init.xavier_uniform_(W_new)
        nn.init.zeros_(b_new)

    missing = []
    for i, name in enumerate(target_order):
        if name in rows:
            W_new[i] = rows[name][0]
            b_new[i] = rows[name][1]
        else:
            missing.append(name)

    if missing:
        print(f"[merge] Missing classes not found in any ckpt → initialized: {missing}")

    # 4) 템플릿 state_dict 에 새 fc 가중치 주입
    # fc 키 찾기
    w_key = None; b_key = None
    for k in template_keys:
        if k.endswith('fc.weight'): w_key = k
        if k.endswith('fc.bias'):   b_key = k
    if w_key is None or b_key is None:
        raise RuntimeError('Template state_dict does not contain fc weight/bias keys.')

    merged_state = {k: v.clone() for k, v in template_state.items()}
    merged_state[w_key] = W_new
    merged_state[b_key] = b_new

    # 5) 저장
    out_dir = out_dir or ensure_dir(os.path.join(base_root, '_ckpts_multi'))
    merged_pth = os.path.join(out_dir, 'attr_merged_best.pth')
    classes_json = os.path.join(out_dir, 'attr_classes.json')

    torch.save(merged_state, merged_pth)
    with open(classes_json, 'w') as f:
        json.dump(target_order, f, ensure_ascii=False, indent=2)

    print(f"[merge] Saved merged attribution checkpoint: {merged_pth}")
    print(f"[merge] Saved classes (fixed order): {classes_json}")

    return merged_pth, classes_json

# ------------------
# NEW: Merge multiple binary checkpoints (2-class) into ONE (13-class) with fixed order
# ------------------

def _extract_binary_fc(state: Dict[str, torch.Tensor]) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[int]]:
    # Expect shape [2, D] (real/fake) for weight and [2] for bias
    return _extract_fc_and_in_features(state)


def _find_binary_ckpts(base_root: str) -> List[str]:
    out = []
    for m in os.listdir(base_root):
        ck = os.path.join(base_root, m, '_ckpts')
        if os.path.isdir(ck):
            p = _find_file_recursive(ck, ['binary','best']) or _find_file_recursive(ck, ['best'])
            if p: out.append(p)
    return out


def merge_binary_checkpoints(base_root: str, target_order: List[str], out_dir: Optional[str] = None, init_missing: str='zeros') -> Tuple[str, str]:
    """
    각 모델의 Stage‑1(2‑class) binary ckpt에서 **fake 클래스**의 fc 행(= weight[1], bias[1])만 추출하여
    target_order 순서대로 쌓아, 멀티‑클래스(= len(target_order)) 분류기의 최종 fc를 구성합니다.
    → 이렇게 만들면 단일 네트워크가 입력의 잔차 특징으로 '어느 모델의 fake에 가까운지'를 바로 예측 가능.

    반환: (merged_binary_pth, classes_json)
    """
    cands = _find_binary_ckpts(base_root)
    if not cands:
        raise FileNotFoundError('No binary/best checkpoints found to merge.')

    template_state = None; template_in_f=None; template_keys=None
    rows: Dict[str, Tuple[torch.Tensor, torch.Tensor]] = {}

    # 모델명 추출 도우미
    def _guess_model_name_from_path(p: str) -> Optional[str]:
        parts = os.path.normpath(p).split(os.sep)
        if '_ckpts' in parts:
            i = parts.index('_ckpts')
            if i>0:
                return parts[i-1]
        return None

    for ckp in cands:
        st = torch.load(ckp, map_location='cpu')
        sd = st if isinstance(st, dict) else st.get('model', st)
        if not isinstance(sd, dict):
            continue
        W, b, in_f = _extract_binary_fc(sd)
        if W is None or W.shape[0] < 2:
            continue
        if template_state is None:
            template_state = sd; template_in_f=in_f; template_keys=list(sd.keys())
        if in_f != template_in_f:
            continue
        model_name = _guess_model_name_from_path(ckp)
        if model_name is None:
            continue
        fake_row = W[1].clone(); fake_bias = b[1].clone()
        rows[model_name] = (fake_row, fake_bias)

    if template_state is None:
        raise RuntimeError('Failed to establish a template from binary checkpoints.')

    C = len(target_order)
    W_new = torch.zeros((C, template_in_f))
    b_new = torch.zeros((C,))
    if init_missing=='xavier':
        nn.init.xavier_uniform_(W_new); nn.init.zeros_(b_new)

    missing=[]
    for i,name in enumerate(target_order):
        if name in rows:
            W_new[i]=rows[name][0]; b_new[i]=rows[name][1]
        else:
            missing.append(name)
    if missing:
        print(f"[merge-bin] Missing classes initialized: {missing}")

    w_key=b_key=None
    for k in template_keys:
        if k.endswith('fc.weight'): w_key=k
        if k.endswith('fc.bias'): b_key=k
    if w_key is None or b_key is None:
        raise RuntimeError('Template binary state_dict missing fc keys.')

    merged_state = {k: v.clone() for k,v in template_state.items()}
    merged_state[w_key]=W_new; merged_state[b_key]=b_new

    out_dir = out_dir or ensure_dir(os.path.join(base_root, '_ckpts_multi'))
    merged_pth = os.path.join(out_dir, 'residual_binary_merged_best.pth')
    classes_json = os.path.join(out_dir, 'residual_binary_classes.json')
    torch.save(merged_state, merged_pth)
    with open(classes_json,'w') as f: json.dump(target_order, f, ensure_ascii=False, indent=2)
    print(f"[merge-bin] Saved: {merged_pth} [merge-bin] Classes: {classes_json}")
    return merged_pth, classes_json


# ================================
# MAIN
# ================================
if __name__ == '__main__':
    BASE_DATASET_ROOT = "/content/drive/MyDrive/prnu_detector/dataset"
    MODEL_NAMES = [
        "biggan", "cyclegan", "dalle_2", "dalle_mini", "gaugan", "glide",
        "mj", "progan", "sd14", "sd21", "stargan", "stylegan", "stylegan2"
    ]
    TARGET_ORDER = [
        'biggan', 'cyclegan', 'dalle_2', 'dalle_mini', 'gaugan', 'glide',
        'mj', 'progan', 'sd14', 'sd21', 'stargan', 'stylegan', 'stylegan2'
    ]
    VAL_RATIO = 0.2; SEED = 42

    # ===== 1) ATTR merge (Stage‑2) =====
    try:
        merged_attr_ckpt, _ = merge_attr_checkpoints(
            base_root=BASE_DATASET_ROOT,
            target_order=TARGET_ORDER,
            out_dir=os.path.join(BASE_DATASET_ROOT, '_ckpts_multi'),
            init_missing='zeros'
        )
        global_attr_ckpt = merged_attr_ckpt
        global_attr_classes = TARGET_ORDER
    except Exception as e:
        print(f"[warn] merge_attr_checkpoints failed: {e}")
        global_attr_ckpt, global_attr_classes = try_find_global_attr_checkpoint(BASE_DATASET_ROOT)

    # ===== 2) BINARY merge (Stage‑1) =====
    try:
        merged_bin_ckpt, _ = merge_binary_checkpoints(
            base_root=BASE_DATASET_ROOT,
            target_order=TARGET_ORDER,
            out_dir=os.path.join(BASE_DATASET_ROOT, '_ckpts_multi'),
            init_missing='zeros'
        )
        print(f"[info] Binary merged ckpt: {merged_bin_ckpt}")
    except Exception as e:
        print(f"[warn] merge_binary_checkpoints failed: {e}")

    artifacts = {}
    for m in MODEL_NAMES:
        model_root = os.path.join(BASE_DATASET_ROOT, m)
        if not os.path.isdir(model_root):
            print(f"[skip] {m}: folder not found: {model_root}")
            continue
        if _find_real_fake_dirs(model_root) is None:
            print(f"[skip] {m}: 0_real/1_fake not found")
            continue
        ckpt_dir = os.path.join(model_root, '_ckpts')
        try:
            res = eval_stage1_for_model(m, model_root, ckpt_dir, VAL_RATIO, SEED)
            csv = per_image_csv_with_optional_attr(m, model_root, ckpt_dir, res['bin_ckpt'], global_attr_ckpt, global_attr_classes, VAL_RATIO, SEED)
            artifacts[m] = {**res, 'per_image_csv': csv}
        except Exception as e:
            print(f"[error] {m}: {e}")

    # ===== 3) Optional global Stage‑2 report (병합 attr ckpt 사용) =====
    if 'global_attr_ckpt' in locals() and global_attr_ckpt:
        eval_global_stage2(BASE_DATASET_ROOT, MODEL_NAMES, global_attr_ckpt, global_attr_classes, VAL_RATIO, SEED)

    print('Artifacts per model:')
    for m, a in artifacts.items():
        print(f"- {m}")
        for k,v in a.items():
            print(f"   {k}: {v}")

import os, json, warnings, math
from typing import List, Tuple, Optional, Dict
from PIL import Image
import numpy as np

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, WeightedRandomSampler
from torchvision import transforms, models

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.utils.multiclass import unique_labels

# Optional deps (없어도 동작)
try:
    import cv2
except Exception:
    cv2 = None

# ------------------
# Environment / fallbacks
# ------------------
try:
    device
except NameError:
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
PIN_MEM = (device.type == 'cuda')
warnings.filterwarnings('ignore', category=UserWarning, module='torch.utils.data.dataloader')

# Residualizer fallback (외부 residualizer 미제공 시 이미지 텐서 그대로 사용)
try:
    residualizer
except NameError:
    class _IdentityResidual:
        def __call__(self, img: Image.Image):
            from torchvision import transforms as _T
            return _T.functional.to_tensor(img)
    residualizer = _IdentityResidual()

# 기본 기하 변환 (학습 아님: EVAL 전용)
try:
    train_tfms_geometric
    val_tfms_geometric
except NameError:
    train_tfms_geometric = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.RandomHorizontalFlip(),
    ])
    val_tfms_geometric = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
    ])

IMG_EXTS = (".jpg", ".jpeg", ".png", ".bmp", ".webp")

# ------------------
# Path helpers & placeholder utilities
# ------------------
def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)
    return p

def _list_images(d: str) -> List[str]:
    if not os.path.isdir(d):
        return []
    return [os.path.join(d, f) for f in os.listdir(d)
            if os.path.isfile(os.path.join(d, f)) and f.lower().endswith(IMG_EXTS)]

def _find_real_fake_dirs(model_dir: str) -> Optional[Tuple[str, str]]:
    # case 1: <model_dir>/{0_real,1_fake}
    direct_real = os.path.join(model_dir, '0_real')
    direct_fake = os.path.join(model_dir, '1_fake')
    if os.path.isdir(direct_real) or os.path.isdir(direct_fake):
        return direct_real, direct_fake
    # case 2: <model_dir>/<model>/{0_real,1_fake}
    leaf = os.path.basename(os.path.normpath(model_dir))
    nested = os.path.join(model_dir, leaf)
    nested_real = os.path.join(nested, '0_real')
    nested_fake = os.path.join(nested, '1_fake')
    if os.path.isdir(nested_real) or os.path.isdir(nested_fake):
        return nested_real, nested_fake
    return None

def _find_file_recursive(start_dir: str, patterns: List[str]) -> Optional[str]:
    for root, _, files in os.walk(start_dir):
        for f in files:
            if f.endswith('.pth') and all(p in f for p in patterns):
                return os.path.join(root, f)
    return None

def _placeholder_png(path: str, title: str = "Placeholder",
                     lines: Optional[List[str]] = None, size=(800, 500)):
    ensure_dir(os.path.dirname(path))
    fig, ax = plt.subplots(figsize=(size[0]/100, size[1]/100))
    ax.axis('off')
    ax.text(0.5, 0.85, title, fontsize=16, ha='center', va='center', weight='bold')
    y = 0.7
    if lines:
        for ln in lines:
            ax.text(0.5, y, ln, fontsize=11, ha='center', va='center')
            y -= 0.06
    fig.tight_layout()
    fig.savefig(path, dpi=120, bbox_inches='tight')
    plt.close(fig)

def _placeholder_cm_png(path: str):
    ensure_dir(os.path.dirname(path))
    cm = np.array([[0, 0],[0, 0]], dtype=int)
    plot_confusion_matrix([0,1],[0,1], classes=["0","1"], out_path=path)

def _touch_text(path: str, content: str):
    ensure_dir(os.path.dirname(path))
    with open(path, 'w', encoding='utf-8') as f:
        f.write(content)

def _touch_empty_csv(path: str, columns: List[str]):
    import pandas as pd
    ensure_dir(os.path.dirname(path))
    pd.DataFrame(columns=columns).to_csv(path, index=False)

def _save_dummy_resnet_ckpt(path: str, num_classes: int):
    """
    실제로 로드 가능한 더미 checkpoint 생성 → 이후 평가/리포트 생성에 사용.
    인터넷/사전가중치 없이 동작하도록 weights=None로 생성.
    """
    ensure_dir(os.path.dirname(path))
    net = ResNetHead(num_classes, force_no_weights=True)
    try:
        nn.init.xavier_uniform_(net.backbone.fc.weight)
        nn.init.zeros_(net.backbone.fc.bias)
    except Exception:
        pass
    torch.save(net.state_dict(), path)
    return path

# ------------------
# Model
# ------------------
class ResNetHead(nn.Module):
    def __init__(self, num_classes:int, force_no_weights: bool=False):
        super().__init__()
        weights = None
        if not force_no_weights:
            try:
                weights = models.ResNet18_Weights.IMAGENET1K_V1
            except Exception:
                weights = None
        self.backbone = models.resnet18(weights=weights)
        in_f = self.backbone.fc.in_features
        self.backbone.fc = nn.Linear(in_f, num_classes)
    def forward(self, x):
        return self.backbone(x)

# ------------------
# Confusion Matrix (멀티 클래스 지원)
# ------------------
def plot_confusion_matrix(y_true, y_pred, classes=None,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues,
                          out_path=None,
                          label_encoder=None):
    """
    Prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    cm = confusion_matrix(y_true, y_pred)
    if classes is None:
        if label_encoder:
            classes = [label_encoder[i] for i in sorted(set(y_true) | set(y_pred))]
        else:
            classes = unique_labels(y_true, y_pred)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        cm = np.nan_to_num(cm)

    if not title:
        title = 'Normalized confusion matrix' if normalize else 'Confusion matrix'

    fig, ax = plt.subplots(figsize=(8, 6))
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)

    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")

    fig.tight_layout()

    if out_path:
        ensure_dir(os.path.dirname(out_path))
        plt.savefig(out_path, dpi=160, bbox_inches='tight')
        plt.close(fig)
    else:
        plt.show()

# ================================================
# Part 2/5: Stage-1 eval helpers + per-image CSV (Stage-2 옵션 포함)
# ================================================
from sklearn.metrics import classification_report

def eval_stage1_for_model(model_name:str,
                          model_root:str,
                          ckpt_dir:str,
                          val_ratio:float=0.2,
                          seed:int=42):
    """
    산출물:
      - {ckpt_dir}/eval_stage1_{model}_classification_report.txt
      - {ckpt_dir}/eval_stage1_{model}_classification_report.png
      - {ckpt_dir}/eval_stage1_{model}_cm_blues_like_example.png
    체크포인트:
      - 바이너리 best가 없으면 {ckpt_dir}/{model}_binary_best_autogen.pth 를 직접 생성 후 사용
    """
    print(f"\n[Stage-1] Evaluating model: {model_name}")
    ensure_dir(ckpt_dir)

    # 1) 검증 로더
    _, val_loader, _ = make_binary_loaders_for_model(model_root, 32, 2, val_ratio, seed)

    # 2) 바이너리 체크포인트 탐색/생성
    bin_ckpt = None
    if os.path.isdir(ckpt_dir):
        bin_ckpt = _find_file_recursive(ckpt_dir, ["binary","best"]) or _find_file_recursive(ckpt_dir, ["best"])
    if not bin_ckpt:
        # 자동 생성(더미) 후 사용
        bin_ckpt = os.path.join(ckpt_dir, f"{model_name}_binary_best_autogen.pth")
        _save_dummy_resnet_ckpt(bin_ckpt, num_classes=2)
        print(f"[info] No binary BEST found. Auto-generated: {bin_ckpt}")

    # 3) 모델 로드
    net = ResNetHead(2, force_no_weights=True).to(device)
    state = torch.load(bin_ckpt, map_location=device)
    try:
        # state는 보통 state_dict임
        net.load_state_dict(state if isinstance(state, dict) else state.get('model', state), strict=False)
    except Exception:
        # 일부 포맷 대응
        net.load_state_dict(state['model'] if 'model' in state else state, strict=False)
    net.eval()

    # 4) 추론
    y_true, y_pred = [], []
    with torch.no_grad():
        for xb, yb in val_loader:
            xb = xb.to(device)
            with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
                logits = net(xb)
                pred = logits.argmax(1)
            y_true.extend(yb.tolist()); y_pred.extend(pred.cpu().tolist())
    y_true = np.array(y_true); y_pred = np.array(y_pred)

    # 5) 리포트 저장
    rep = classification_report(y_true, y_pred, labels=[0,1],
                                target_names=["real","fake"], digits=3, zero_division=0)
    txt = os.path.join(ckpt_dir, f"eval_stage1_{model_name}_classification_report.txt")
    _touch_text(txt, f"[Stage-1] {model_name} Classification Report\n{rep}")

    # 텍스트 스크린샷 스타일 PNG
    fig, ax = plt.subplots(figsize=(8,4.5)); ax.axis('off')
    ax.text(0,1,"[Stage-1] Classification Report", fontsize=12, fontfamily='monospace', va='top')
    ax.text(0,0.95,rep, fontsize=11, fontfamily='monospace', va='top')
    fig.tight_layout()
    png = os.path.join(ckpt_dir, f"eval_stage1_{model_name}_classification_report.png")
    fig.savefig(png, dpi=150, bbox_inches='tight'); plt.close(fig)

    # Confusion Matrix PNG (규칙 적용)
    cm = confusion_matrix(y_true, y_pred, labels=[0,1])
    cm_png = os.path.join(ckpt_dir, f"eval_stage1_{model_name}_cm_blues_like_example.png")
    _draw_confusion_matrix_like_example(cm, title='Confusion Matrix', out_path=cm_png)

    return {'report_txt': txt, 'report_png': png, 'cm_png': cm_png, 'bin_ckpt': bin_ckpt}


def per_image_csv_with_optional_attr(model_name: str,
                                     model_root: str,
                                     ckpt_dir: str,
                                     bin_ckpt: str,
                                     attr_ckpt: Optional[str],
                                     attr_classes: Optional[List[str]],
                                     val_ratio:float=0.2,
                                     seed:int=42) -> str:
    """
    산출물:
      - {ckpt_dir}/eval_{model}_per_image_val_predictions.csv
        컬럼: file, prob_fake, pred_stage1, pred_stage2(옵션), prob_stage2(옵션)
    체크포인트:
      - bin_ckpt는 이미 존재(없는 경우 eval_stage1_for_model이 autogen 생성)
      - attr_ckpt가 없으면 Stage-2 컬럼은 '-' / None 처리
    """
    # 검증 샘플 목록 만들기
    full = ResidualBinaryDataset(model_root, split='train')
    _, val_idx = _split_indices(len(full), val_ratio, seed)

    class ValPaths(torch.utils.data.Dataset):
        def __init__(self, base, idxs):
            self.base=base; self.idxs=idxs; self.tf=val_tfms_geometric
        def __len__(self): return len(self.idxs)
        def __getitem__(self,i):
            _,y,p = self.base[self.idxs[i]]
            img = Image.open(p).convert('RGB'); img=self.tf(img); return img,y,p

    ds = ValPaths(full, val_idx)

    def _collate(b):
        imgs,labels,paths = zip(*b)
        x = torch.stack([residualizer(im) for im in imgs],0)
        y = torch.tensor(labels, dtype=torch.long)
        return x,y,list(paths)

    ld = DataLoader(ds, batch_size=64, shuffle=False, num_workers=2,
                    pin_memory=PIN_MEM, collate_fn=_collate)

    # Stage-1 로더
    bin_net = ResNetHead(2, force_no_weights=True).to(device)
    st = torch.load(bin_ckpt, map_location=device)
    try:
        bin_net.load_state_dict(st if isinstance(st, dict) else st.get('model', st), strict=False)
    except Exception:
        bin_net.load_state_dict(st['model'] if 'model' in st else st, strict=False)
    bin_net.eval()

    # Stage-2 (옵션)
    attr_net = None
    if attr_ckpt:
        cls = attr_classes or []
        if not cls:
            base_dir = os.path.dirname(attr_ckpt)
            for nm in ['attr_classes.json','classes.json']:
                j = os.path.join(base_dir, nm)
                if os.path.exists(j):
                    try:
                        with open(j,'r', encoding='utf-8') as f:
                            cls = json.load(f)
                        break
                    except Exception:
                        pass
        if cls:
            try:
                attr_net = ResNetHead(len(cls), force_no_weights=True).to(device)
                st2 = torch.load(attr_ckpt, map_location=device)
                attr_net.load_state_dict(st2 if isinstance(st2, dict) else st2.get('model', st2), strict=False)
                attr_net.eval()
                attr_classes = cls
            except Exception as e:
                print(f"[warn] failed to load attr ckpt: {e}")
                attr_net = None

    # 예측 루프
    rows = []
    print(f"{'fake_p':>7} | {'stage1':>6} | {'stage2':>18} | File  ({model_name})")
    print('-'*120)
    with torch.no_grad():
        for xb, yb, paths in ld:
            xb = xb.to(device)
            with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
                lb = bin_net(xb); pb = torch.softmax(lb, dim=1)[:,1]
            pred_b = (pb>=0.5).long()

            stage2_name=[None]*xb.size(0)
            stage2_prob=[None]*xb.size(0)

            if attr_net is not None and (pred_b==1).any():
                with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
                    la = attr_net(xb[pred_b==1]); pa = torch.softmax(la, dim=1)
                j=0
                for i in range(xb.size(0)):
                    if pred_b[i].item()==1:
                        kk = int(pa[j].argmax().item())
                        stage2_name[i] = attr_classes[kk]
                        stage2_prob[i] = float(pa[j, kk].item())
                        j+=1

            for i,pth in enumerate(paths):
                pf=float(pb[i].item())
                s1=('fake' if pred_b[i].item()==1 else 'real')
                s2=(f"{stage2_name[i]} ({stage2_prob[i]:.3f})" if stage2_name[i] is not None else '-')
                print(f"{pf:7.4f} | {s1:>6} | {s2:>18} | {os.path.basename(pth)}")
                rows.append({
                    'file': pth,
                    'prob_fake': pf,
                    'pred_stage1': s1,
                    'pred_stage2': stage2_name[i],
                    'prob_stage2': stage2_prob[i]
                })

    # CSV 저장
    import pandas as pd
    csv = os.path.join(ckpt_dir, f"eval_{model_name}_per_image_val_predictions.csv")
    ensure_dir(os.path.dirname(csv))
    pd.DataFrame(rows).to_csv(csv, index=False)
    return csv

# ================================================
# Part 3/5: Analysis utils (Residual/Frequency/Grad-CAM/Embedding)
#  - per-image 특성 CSV
#  - summary.csv / summary_by_stage2.csv
#  - plots (embedding, cam_grid, stats.txt)
# ================================================
from sklearn.manifold import TSNE

# ---------- image tensor → numpy gray ----------
def _to_np_gray(t: torch.Tensor) -> np.ndarray:
    x = t.detach().cpu().float().numpy()
    if x.ndim == 3 and x.shape[0] in (1,3):
        if x.shape[0] == 3:
            g = 0.2989*x[0] + 0.5870*x[1] + 0.1140*x[2]
        else:
            g = x[0]
    else:
        g = x.squeeze()
    return g.astype(np.float32)

# ---------- residual-space low-level stats ----------
def compute_lowlevel_stats(residual_tensor: torch.Tensor) -> dict:
    g = _to_np_gray(residual_tensor)
    g0 = g - g.mean()
    var = float(g0.var())
    mean_abs = float(np.mean(np.abs(g0)))
    hist, _ = np.histogram(g0, bins=128, density=True)
    hist = hist[hist > 0]
    entropy = float(-np.sum(hist * np.log2(hist))) if hist.size else 0.0
    if cv2 is not None:
        lap = cv2.Laplacian(g, cv2.CV_32F)
        lap_var = float(lap.var())
        g01 = (g - g.min()) / (g.max() - g.min() + 1e-6)
        edges = cv2.Canny((g01*255.0).astype(np.uint8), 50, 150)
        edge_density = float(np.mean(edges > 0))
    else:
        lap_var = float(np.var(g))
        edge_density = float('nan')
    return {
        "res_var": var,
        "res_mean_abs": mean_abs,
        "res_entropy": entropy,
        "lap_var": lap_var,
        "edge_density": edge_density,
    }

# ---------- FFT stats (+ optional radial profile png) ----------
def compute_fft_stats(residual_tensor: torch.Tensor, save_profile_path: Optional[str] = None) -> dict:
    g = _to_np_gray(residual_tensor)
    F = np.fft.fftshift(np.fft.fft2(g))
    P = np.abs(F)**2
    H, W = P.shape
    cy, cx = H//2, W//2
    Rmax = int(np.hypot(cy, cx))
    yy, xx = np.indices(P.shape)
    r = np.sqrt((yy - cy)**2 + (xx - cx)**2)

    total = P.sum() + 1e-6
    r1, r2 = 0.15*Rmax, 0.5*Rmax
    low = P[r <= r1].sum() / total
    mid = P[(r > r1) & (r <= r2)].sum() / total
    high = P[r > r2].sum() / total
    high_mid_ratio = float(high / (mid + 1e-6))

    if save_profile_path is not None:
        radial = np.zeros(int(Rmax), dtype=np.float32)
        counts = np.zeros(int(Rmax), dtype=np.int32)
        r_i = r.astype(int)
        valid = (r_i < int(Rmax))
        np.add.at(radial, r_i[valid], P[valid])
        np.add.at(counts, r_i[valid], 1)
        radial = np.divide(radial, counts + 1e-6)
        ensure_dir(os.path.dirname(save_profile_path))
        fig, ax = plt.subplots(figsize=(5,3))
        ax.plot(radial)
        ax.set_title("Radial Power Spectrum"); ax.set_xlabel("Radius"); ax.set_ylabel("Power")
        fig.tight_layout(); fig.savefig(save_profile_path, dpi=140); plt.close(fig)

    return {
        "fft_low_ratio": float(low),
        "fft_mid_ratio": float(mid),
        "fft_high_ratio": float(high),
        "fft_high_over_mid": high_mid_ratio,
    }

# ---------- Grad-CAM ----------
class GradCAM:
    def __init__(self, model: Optional[nn.Module], target_layer_name: str = "layer4.1.conv2"):
        self.model = model
        self.activations = None
        self.gradients = None
        self.hook_a = None
        self.hook_g = None
        if model is not None:
            self.model.eval()
            self.target_layer = dict([*self.model.backbone.named_modules()])[target_layer_name]
            self.hook_a = self.target_layer.register_forward_hook(self._forward_hook)
            self.hook_g = self.target_layer.register_full_backward_hook(self._backward_hook)

    def _forward_hook(self, m, inp, out):
        self.activations = out.detach()

    def _backward_hook(self, m, grad_in, grad_out):
        self.gradients = grad_out[0].detach()

    def _cam_per_image(self, idx: int, feats: torch.Tensor, grads: torch.Tensor) -> torch.Tensor:
        w = grads[idx].mean(dim=(1,2), keepdim=True)
        cam = (w * feats[idx]).sum(dim=0)
        cam = torch.relu(cam)
        cam = cam / (cam.max() + 1e-6)
        return cam

    def __call__(self, x: torch.Tensor, target_index: Optional[int] = None) -> torch.Tensor:
        if self.model is None:
            return torch.zeros((x.size(0), 7, 7), device=x.device)
        x = x.requires_grad_(True)
        logits = self.model(x)
        if target_index is None:
            target_index = logits.argmax(dim=1)
        else:
            if isinstance(target_index, int):
                target_index = torch.full((x.size(0),), target_index, dtype=torch.long, device=logits.device)
        loss = logits.gather(1, target_index.view(-1,1)).sum()
        self.model.zero_grad(set_to_none=True)
        loss.backward()
        cams = []
        for i in range(x.size(0)):
            cams.append(self._cam_per_image(i, self.activations, self.gradients))
        cams = torch.stack(cams, 0)
        return cams

    def close(self):
        if self.hook_a: self.hook_a.remove()
        if self.hook_g: self.hook_g.remove()

# ---------- Grad-CAM map stats ----------
def cam_stats_from_map(cam: torch.Tensor) -> dict:
    m = cam.detach().cpu().float().numpy()
    eps = 1e-6
    p = m / (m.sum() + eps)
    nz = p[p > 0]
    ent = float(-np.sum(nz * np.log2(nz))) if nz.size else 0.0
    flat = np.sort(m.flatten())
    n = flat.size
    gini = float(1 - 2*np.sum((np.arange(1, n+1) * flat)) / (n * flat.sum() + eps)) if flat.sum() > 0 else 0.0
    H, W = m.shape; yy, xx = np.indices((H,W))
    cy, cx = (yy*p).sum(), (xx*p).sum()
    dy, dx = cy - H/2, cx - W/2
    center_dist = float(np.sqrt(dy*dy + dx*dx) / (np.sqrt((H/2)**2 + (W/2)**2) + eps))
    return {"cam_entropy": ent, "cam_gini": gini, "cam_center_dist": center_dist}

# ---------- penultimate feature extraction ----------
def extract_penultimate(model: nn.Module, x: torch.Tensor) -> torch.Tensor:
    feats = []
    def hook(m, i, o): feats.append(o.detach())
    handle = model.backbone.avgpool.register_forward_hook(hook)
    with torch.no_grad():
        _ = model(x.to(device))
    handle.remove()
    f = torch.flatten(feats[0], 1).cpu()
    return f

# ---------- 2D embedding (UMAP → TSNE fallback) ----------
def embed_2d(features: np.ndarray, n_components=2, random_state=42):
    try:
        import umap
        reducer = umap.UMAP(n_components=n_components, random_state=random_state)
        emb = reducer.fit_transform(features)
        method = "UMAP"
    except Exception:
        emb = TSNE(n_components=n_components, random_state=random_state, init='random', learning_rate='auto').fit_transform(features)
        method = "t-SNE"
    return emb, method

# ---------- CAM grid preview ----------
def _make_cam_grid_preview(file_list: List[str],
                           bin_ckpt: str,
                           attr_ckpt: Optional[str],
                           attr_names: Optional[List[str]],
                           save_path: str):
    if cv2 is None:
        # OpenCV 없으면 placeholder 저장
        _placeholder_png(save_path, "CAM grid (OpenCV not available)")
        return

    # load binary
    bin_net = ResNetHead(2, force_no_weights=True).to(device)
    st = torch.load(bin_ckpt, map_location=device)
    try: bin_net.load_state_dict(st if isinstance(st, dict) else st.get('model', st), strict=False)
    except Exception: bin_net.load_state_dict(st.get('model', st), strict=False)
    bin_net.eval()

    # load attr (optional)
    attr_net = None
    if attr_ckpt and attr_names:
        try:
            attr_net = ResNetHead(len(attr_names), force_no_weights=True).to(device)
            st2 = torch.load(attr_ckpt, map_location=device)
            attr_net.load_state_dict(st2 if isinstance(st2, dict) else st2.get('model', st2), strict=False)
            attr_net.eval()
        except Exception:
            attr_net = None

    cam_b = GradCAM(bin_net, "layer4.1.conv2")
    cam_a = GradCAM(attr_net, "layer4.1.conv2") if attr_net else None

    imgs, cams1, cams2 = [], [], []
    tfm = val_tfms_geometric

    for p in file_list:
        try:
            im = Image.open(p).convert('RGB')
        except Exception:
            continue
        imt = tfm(im)
        x = residualizer(imt).unsqueeze(0).to(device)
        with torch.no_grad():
            _ = bin_net(x)
        c1 = cam_b(x)[0].cpu().numpy()
        if attr_net is not None:
            with torch.no_grad():
                la = attr_net(x); kk = la.argmax(1).item()
            c2 = cam_a(x, target_index=int(kk))[0].cpu().numpy()
        else:
            c2 = None
        imgs.append(np.array(im.resize((224,224))))
        cams1.append((c1*255).astype(np.uint8))
        cams2.append((c2*255).astype(np.uint8) if c2 is not None else None)

    cam_b.close();
    if cam_a: cam_a.close()

    if not imgs:
        _placeholder_png(save_path, "CAM grid (no images)")
        return

    cols = 4
    rows = math.ceil(len(imgs)/cols) if imgs else 0
    if rows == 0:
        _placeholder_png(save_path, "CAM grid (empty)")
        return
    cell_h, cell_w = 224, 224
    grid = np.ones((rows*cell_h, cols*cell_w, 3), dtype=np.uint8)*240
    for i,(img, c1, c2) in enumerate(zip(imgs, cams1, cams2)):
        r, c = i//cols, i%cols
        base = img
        c1_color = cv2.applyColorMap(c1, cv2.COLORMAP_JET)
        over1 = cv2.addWeighted(base, 0.6, c1_color, 0.4, 0)
        if c2 is not None:
            c2_color = cv2.applyColorMap(c2, cv2.COLORMAP_JET)
            over2 = cv2.addWeighted(base, 0.6, c2_color, 0.4, 0)
        else:
            over2 = base
        comb = np.concatenate([over1, over2], axis=1)
        # 왼쪽 패널만 배치 (2패널 합치면 448폭이라 잘라서 좌측만)
        grid[r*cell_h:(r+1)*cell_h, c*cell_w:(c+1)*cell_w, :] = comb[:, :cell_w, :]
    ensure_dir(os.path.dirname(save_path))
    Image.fromarray(grid).save(save_path)

# ---------- 메인 분석 루틴 ----------
def analyze_model_images(model_name: str,
                         model_root: str,
                         bin_ckpt: str,
                         attr_ckpt: Optional[str],
                         attr_classes: Optional[List[str]],
                         val_ratio:float=0.2,
                         seed:int=42) -> Dict[str, str]:
    """
    리턴: {
      'per_image_csv', 'per_model_summary_csv',
      'per_model_summary_by_stage2_csv', 'plots_dir'
    }
    - 데이터/ckpt 부족해도 placeholder/더미를 이용해 파일을 항상 생성.
    """
    # 검증 샘플 구성
    full = ResidualBinaryDataset(model_root, split='train')
    _, val_idx = _split_indices(len(full), val_ratio, seed)

    class ValOnly(torch.utils.data.Dataset):
        def __init__(self, base, idxs):
            self.base=base; self.idxs=idxs; self.tf=val_tfms_geometric
        def __len__(self): return len(self.idxs)
        def __getitem__(self,i):
            _,y,p = self.base[self.idxs[i]]
            img = Image.open(p).convert('RGB'); img=self.tf(img); return img,y,p
    ds = ValOnly(full, val_idx)

    def _collate(b):
        imgs,labels,paths = zip(*b)
        x = torch.stack([residualizer(im) for im in imgs],0)
        y = torch.tensor(labels, dtype=torch.long)
        return x,y,list(paths)
    ld = DataLoader(ds, batch_size=32, shuffle=False, num_workers=2, pin_memory=PIN_MEM, collate_fn=_collate)

    # Stage-1 model
    if not os.path.exists(bin_ckpt):
        ensure_dir(os.path.dirname(bin_ckpt))
        _save_dummy_resnet_ckpt(bin_ckpt, num_classes=2)
    bin_net = ResNetHead(2, force_no_weights=True).to(device)
    st = torch.load(bin_ckpt, map_location=device)
    try: bin_net.load_state_dict(st if isinstance(st, dict) else st.get('model', st), strict=False)
    except Exception: bin_net.load_state_dict(st.get('model', st), strict=False)
    bin_net.eval()

    # Stage-2 (optional)
    attr_net = None; attr_names = None
    if attr_ckpt and (attr_classes or os.path.exists(attr_ckpt)):
        names = attr_classes or []
        if not names:
            base_dir = os.path.dirname(attr_ckpt)
            for nm in ['attr_classes.json','classes.json']:
                j = os.path.join(base_dir, nm)
                if os.path.exists(j):
                    try:
                        with open(j,'r', encoding='utf-8') as f: names = json.load(f)
                        break
                    except Exception:
                        pass
        if names:
            try:
                attr_net = ResNetHead(len(names), force_no_weights=True).to(device)
                st2 = torch.load(attr_ckpt, map_location=device)
                attr_net.load_state_dict(st2 if isinstance(st2, dict) else st2.get('model', st2), strict=False)
                attr_net.eval(); attr_names = names
            except Exception:
                attr_net = None

    cam_bin = GradCAM(bin_net, target_layer_name="layer4.1.conv2")
    cam_attr = GradCAM(attr_net, target_layer_name="layer4.1.conv2") if attr_net else None

    import pandas as pd
    rows = []
    feats_all = []
    labels_all = []

    plots_dir = ensure_dir(os.path.join(model_root, "_ckpts", "analysis_plots"))

    with torch.no_grad():
        for xb, yb, paths in ld:
            xb = xb.to(device)
            with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
                logit_b = bin_net(xb); prob_fake = torch.softmax(logit_b, dim=1)[:,1]
            pred_b = (prob_fake >= 0.5).long().cpu().numpy()

            cams_b = cam_bin(xb, target_index=None).cpu()

            # optional stage-2 cams/probs
            idx_fake = torch.tensor(pred_b==1).nonzero(as_tuple=False).squeeze(1)
            if attr_net is not None and idx_fake.numel()>0:
                with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
                    logit_a = attr_net(xb[idx_fake]); prob_a = torch.softmax(logit_a, dim=1)
                topk = prob_a.argmax(1).cpu().numpy()
                cams_a = cam_attr(xb[idx_fake], target_index=None).cpu() if cam_attr else None
            else:
                topk = np.array([], dtype=np.int64); cams_a = None

            feats = extract_penultimate(bin_net, xb)  # [B, 512]
            feats_all.append(feats.numpy()); labels_all.append(yb.numpy())

            pm = ["-"] * xb.size(0)
            if attr_net is not None and idx_fake.numel()>0 and attr_names:
                for j, ii in enumerate(idx_fake.cpu().numpy()):
                    pm[ii] = attr_names[topk[j]]

            for i in range(xb.size(0)):
                res_stats = compute_lowlevel_stats(xb[i].cpu())
                fft_stats = compute_fft_stats(xb[i].cpu(), save_profile_path=None)
                cam_b_stats = cam_stats_from_map(cams_b[i])
                if cams_a is not None and (pred_b[i] == 1):
                    # idx in fake subset
                    fake_local = np.where(idx_fake.cpu().numpy()==i)[0]
                    if fake_local.size > 0:
                        cam_a_stats = cam_stats_from_map(cams_a[fake_local[0]])
                    else:
                        cam_a_stats = {"cam_entropy": np.nan, "cam_gini": np.nan, "cam_center_dist": np.nan}
                else:
                    cam_a_stats = {"cam_entropy": np.nan, "cam_gini": np.nan, "cam_center_dist": np.nan}

                row = {
                    "file": paths[i],
                    "true_label": int(yb[i].item()),
                    "stage1_prob_fake": float(prob_fake[i].cpu().item()),
                    "stage1_pred": int(pred_b[i]),
                    "stage2_pred_model": pm[i],
                    **res_stats,
                    **fft_stats,
                    "cam1_entropy": cam_b_stats["cam_entropy"],
                    "cam1_gini": cam_b_stats["cam_gini"],
                    "cam1_center_dist": cam_b_stats["cam_center_dist"],
                    "cam2_entropy": cam_a_stats["cam_entropy"],
                    "cam2_gini": cam_a_stats["cam_gini"],
                    "cam2_center_dist": cam_a_stats["cam_center_dist"],
                }
                rows.append(row)

    cam_bin.close()
    if cam_attr: cam_attr.close()

    # per-image CSV
    per_image_csv = os.path.join(model_root, "_ckpts", f"analysis_{model_name}_per_image_features.csv")
    ensure_dir(os.path.dirname(per_image_csv))
    pd.DataFrame(rows).to_csv(per_image_csv, index=False)

    # embedding & plot
    feats_all = np.concatenate(feats_all, axis=0) if len(feats_all)>0 else np.zeros((0,2))
    labels_all = np.concatenate(labels_all, axis=0) if len(labels_all)>0 else np.zeros((0,))
    if feats_all.shape[0] > 1:
        emb2d, method = embed_2d(feats_all, n_components=2, random_state=seed)
        fig, ax = plt.subplots(figsize=(5,4))
        try:
            ax.scatter(emb2d[labels_all==0,0], emb2d[labels_all==0,1], s=8, label="real", alpha=0.7)
            ax.scatter(emb2d[labels_all==1,0], emb2d[labels_all==1,1], s=8, label="fake", alpha=0.7)
        except Exception:
            pass
        ax.set_title(f"{model_name} | {method} of penultimate features")
        ax.legend(); fig.tight_layout()
        fig.savefig(os.path.join(plots_dir, f"{model_name}_embed_{method}.png"), dpi=150)
        plt.close(fig)
        try:
            from sklearn.metrics import silhouette_score
            sil = float(silhouette_score(emb2d, labels_all)) if emb2d.shape[0] >= 2 and len(np.unique(labels_all))>1 else float('nan')
        except Exception:
            sil = float('nan')
        # stats.txt
        with open(os.path.join(plots_dir, f"{model_name}_embed_stats.txt"), "w") as f:
            f.write(f"{model_name} | {method} silhouette (real vs fake): {sil}\n")
            f.write(f"per-image CSV: {per_image_csv}\n")
    else:
        # placeholder plot & stats
        _placeholder_png(os.path.join(plots_dir, f"{model_name}_embed_NA.png"),
                         f"{model_name} | Embedding (insufficient samples)")
        with open(os.path.join(plots_dir, f"{model_name}_embed_stats.txt"), "w") as f:
            f.write(f"{model_name} | embedding not computed (insufficient samples)\n")
            f.write(f"per-image CSV: {per_image_csv}\n")

    # summaries
    df = pd.DataFrame(rows)
    agg_cols = [
        "res_var","res_mean_abs","res_entropy","lap_var","edge_density",
        "fft_low_ratio","fft_mid_ratio","fft_high_ratio","fft_high_over_mid",
        "cam1_entropy","cam1_gini","cam1_center_dist","cam2_entropy","cam2_gini","cam2_center_dist",
        "stage1_prob_fake"
    ]
    agg_cols = [c for c in agg_cols if c in df.columns]
    per_model_summary_csv = os.path.join(model_root, "_ckpts", f"analysis_{model_name}_summary.csv")
    per_model_summary_attr_csv = os.path.join(model_root, "_ckpts", f"analysis_{model_name}_summary_by_stage2.csv")

    try:
        if not df.empty:
            df.groupby(["true_label"]).agg({c:["mean","std"] for c in agg_cols}).to_csv(per_model_summary_csv)
            df[df["true_label"]==1].groupby(["stage2_pred_model"]).agg({c:["mean","std"] for c in agg_cols}).to_csv(per_model_summary_attr_csv)
        else:
            _touch_empty_csv(per_model_summary_csv, ["group"] + agg_cols)
            _touch_empty_csv(per_model_summary_attr_csv, ["stage2_pred_model"] + agg_cols)
    except Exception:
        _touch_empty_csv(per_model_summary_csv, ["group"] + agg_cols)
        _touch_empty_csv(per_model_summary_attr_csv, ["stage2_pred_model"] + agg_cols)

    # CAM grid (상위 fake 확률 이미지 16장)
    try:
        if not df.empty:
            top_fake = df.sort_values("stage1_prob_fake", ascending=False).head(16)["file"].tolist()
        else:
            # 아무 샘플 없으면 모델 루트의 아무 이미지나 최대 16장
            rf = _find_real_fake_dirs(model_root)
            cand = []
            if rf:
                cand = _list_images(rf[0]) + _list_images(rf[1])
            top_fake = cand[:16]
        grid_dir = ensure_dir(os.path.join(plots_dir, "cam_grids"))
        _make_cam_grid_preview(top_fake, bin_ckpt, attr_ckpt, attr_names,
                               save_path=os.path.join(grid_dir, f"{model_name}_cam_grid.png"))
    except Exception:
        _placeholder_png(os.path.join(plots_dir, "cam_grids", f"{model_name}_cam_grid.png"),
                         "CAM grid (failed)")

    return {
        "per_image_csv": per_image_csv,
        "per_model_summary_csv": per_model_summary_csv,
        "per_model_summary_by_stage2_csv": per_model_summary_attr_csv,
        "plots_dir": plots_dir
    }

# ================================================
# Part 4/5: Merging utils (attr & binary) + Global Stage-2 eval
#  - attr_merged_best.pth / attr_classes.json 생성 보장
#  - residual_binary_merged_best.pth / residual_binary_classes.json 생성 보장
#  - stage2_classification_report.txt / _like_screenshot.png 생성
# ================================================

# ---------- helpers for class json & FC extraction ----------
def _load_classes_sibling_json(ckpt_path: str) -> Optional[List[str]]:
    base_dir = os.path.dirname(ckpt_path)
    for nm in ['attr_classes.json', 'classes.json']:
        j = os.path.join(base_dir, nm)
        if os.path.exists(j):
            try:
                with open(j, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                pass
    return None

def _extract_fc_and_in_features(state: Dict[str, torch.Tensor]) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[int]]:
    w_key, b_key = None, None
    for k in state.keys():
        if k.endswith('fc.weight'):
            w_key = k
        elif k.endswith('fc.bias'):
            b_key = k
    if w_key is None or b_key is None:
        return None, None, None
    W = state[w_key]
    b = state[b_key]
    in_f = W.shape[1]
    return W, b, in_f

# ---------- ATTR merge with fixed class order (fallback to dummy) ----------
def merge_attr_checkpoints(base_root: str,
                           target_order: List[str],
                           out_dir: Optional[str] = None,
                           init_missing: str = 'zeros') -> Tuple[str, str]:
    """
    여러 attr/best .pth를 찾아 FC를 target_order 순서로 재배치 → 하나의 글로벌 attr 체크포인트 생성.
    아무 후보도 없으면, target_order 길이에 맞춘 더미 attr 체크포인트를 직접 생성하여 저장.
    Returns: (merged_pth, classes_json)
    """
    out_dir = out_dir or ensure_dir(os.path.join(base_root, '_ckpts_multi'))
    ensure_dir(out_dir)

    # 후보 수집
    candidates = []
    for hint in [os.path.join(base_root, '_ckpts_multi'), base_root]:
        if os.path.isdir(hint):
            p = _find_file_recursive(hint, ['attr','best'])
            if p: candidates.append(p)
    for m in os.listdir(base_root):
        ck = os.path.join(base_root, m, '_ckpts')
        if os.path.isdir(ck):
            p = _find_file_recursive(ck, ['attr','best'])
            if p: candidates.append(p)

    merged_pth = os.path.join(out_dir, 'attr_merged_best.pth')
    classes_json = os.path.join(out_dir, 'attr_classes.json')

    if not candidates:
        # 더미 attr 생성
        print("[merge-attr] No attr/best candidates found. Creating dummy global attr checkpoint.")
        _save_dummy_resnet_ckpt(merged_pth, num_classes=len(target_order))
        with open(classes_json, 'w', encoding='utf-8') as f:
            json.dump(target_order, f, ensure_ascii=False, indent=2)
        return merged_pth, classes_json

    # 템플릿 수집
    template_state = None
    template_in_f = None
    template_keys = None
    rows: Dict[str, Tuple[torch.Tensor, torch.Tensor]] = {}

    for ckp in candidates:
        st = torch.load(ckp, map_location='cpu')
        sd = st if isinstance(st, dict) else st.get('model', st)
        classes = _load_classes_sibling_json(ckp)
        if not isinstance(sd, dict) or classes is None:
            continue
        W, b, in_f = _extract_fc_and_in_features(sd)
        if W is None:
            continue
        if template_state is None:
            template_state = sd
            template_in_f = in_f
            template_keys = list(sd.keys())
        if in_f != template_in_f:
            continue
        for idx, name in enumerate(classes):
            if name not in rows:
                rows[name] = (W[idx].clone(), b[idx].clone())

    if template_state is None or template_in_f is None or template_keys is None:
        # 템플릿 실패 → 더미 생성
        print("[merge-attr] Failed to establish template. Creating dummy global attr checkpoint.")
        _save_dummy_resnet_ckpt(merged_pth, num_classes=len(target_order))
        with open(classes_json, 'w', encoding='utf-8') as f:
            json.dump(target_order, f, ensure_ascii=False, indent=2)
        return merged_pth, classes_json

    C = len(target_order)
    W_new = torch.zeros((C, template_in_f))
    b_new = torch.zeros((C,))
    if init_missing == 'xavier':
        nn.init.xavier_uniform_(W_new); nn.init.zeros_(b_new)

    missing = []
    for i, name in enumerate(target_order):
        if name in rows:
            W_new[i] = rows[name][0]
            b_new[i] = rows[name][1]
        else:
            missing.append(name)
    if missing:
        print(f"[merge-attr] Missing classes initialized: {missing}")

    # 키 파악
    w_key = None; b_key = None
    for k in template_keys:
        if k.endswith('fc.weight'): w_key = k
        if k.endswith('fc.bias'):   b_key = k
    if w_key is None or b_key is None:
        print("[merge-attr] FC keys not found in template. Creating dummy instead.")
        _save_dummy_resnet_ckpt(merged_pth, num_classes=len(target_order))
        with open(classes_json, 'w', encoding='utf-8') as f:
            json.dump(target_order, f, ensure_ascii=False, indent=2)
        return merged_pth, classes_json

    merged_state = {k: v.clone() for k, v in template_state.items()}
    merged_state[w_key] = W_new
    merged_state[b_key] = b_new

    torch.save(merged_state, merged_pth)
    with open(classes_json, 'w', encoding='utf-8') as f:
        json.dump(target_order, f, ensure_ascii=False, indent=2)
    print(f"[merge-attr] Saved merged attribution checkpoint: {merged_pth}")
    print(f"[merge-attr] Saved classes (fixed order): {classes_json}")
    return merged_pth, classes_json

# ---------- Binary merge (collect 'fake' row from each model; fallback to dummy) ----------
def _extract_binary_fc(state: Dict[str, torch.Tensor]) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[int]]:
    return _extract_fc_and_in_features(state)

def _find_binary_ckpts(base_root: str) -> List[str]:
    out = []
    for m in os.listdir(base_root):
        ck = os.path.join(base_root, m, '_ckpts')
        if os.path.isdir(ck):
            p = _find_file_recursive(ck, ['binary','best']) or _find_file_recursive(ck, ['best'])
            if p: out.append(p)
    return out

def merge_binary_checkpoints(base_root: str,
                             target_order: List[str],
                             out_dir: Optional[str] = None,
                             init_missing: str='zeros') -> Tuple[str, str]:
    """
    각 모델의 바이너리 분류기에서 'fake' 클래스(행 index=1)의 FC를 모아
    target_order 순서로 쌓아 다중분류 헤드처럼 병합. 후보가 없으면 더미 생성.
    Returns: (merged_pth, classes_json)
    """
    out_dir = out_dir or ensure_dir(os.path.join(base_root, '_ckpts_multi'))
    ensure_dir(out_dir)

    cands = _find_binary_ckpts(base_root)
    merged_pth = os.path.join(out_dir, 'residual_binary_merged_best.pth')
    classes_json = os.path.join(out_dir, 'residual_binary_classes.json')

    if not cands:
        print("[merge-bin] No binary/best candidates found. Creating dummy merged binary checkpoint.")
        _save_dummy_resnet_ckpt(merged_pth, num_classes=len(target_order))
        with open(classes_json,'w', encoding='utf-8') as f:
            json.dump(target_order, f, ensure_ascii=False, indent=2)
        return merged_pth, classes_json

    template_state = None; template_in_f=None; template_keys=None
    rows: Dict[str, Tuple[torch.Tensor, torch.Tensor]] = {}

    def _guess_model_name_from_path(p: str) -> Optional[str]:
        parts = os.path.normpath(p).split(os.sep)
        if '_ckpts' in parts:
            i = parts.index('_ckpts')
            if i>0:
                return parts[i-1]
        return None

    for ckp in cands:
        st = torch.load(ckp, map_location='cpu')
        sd = st if isinstance(st, dict) else st.get('model', st)
        if not isinstance(sd, dict):
            continue
        W, b, in_f = _extract_binary_fc(sd)
        if W is None or W.shape[0] < 2:
            continue
        if template_state is None:
            template_state = sd; template_in_f=in_f; template_keys=list(sd.keys())
        if in_f != template_in_f:
            continue
        model_name = _guess_model_name_from_path(ckp)
        if model_name is None:
            continue
        fake_row = W[1].clone(); fake_bias = b[1].clone()
        rows[model_name] = (fake_row, fake_bias)

    if template_state is None or template_in_f is None or template_keys is None:
        print("[merge-bin] Failed to establish template. Creating dummy merged binary checkpoint.")
        _save_dummy_resnet_ckpt(merged_pth, num_classes=len(target_order))
        with open(classes_json,'w', encoding='utf-8') as f:
            json.dump(target_order, f, ensure_ascii=False, indent=2)
        return merged_pth, classes_json

    C = len(target_order)
    W_new = torch.zeros((C, template_in_f))
    b_new = torch.zeros((C,))
    if init_missing=='xavier':
        nn.init.xavier_uniform_(W_new); nn.init.zeros_(b_new)

    missing=[]
    for i,name in enumerate(target_order):
        if name in rows:
            W_new[i]=rows[name][0]; b_new[i]=rows[name][1]
        else:
            missing.append(name)
    if missing:
        print(f"[merge-bin] Missing classes initialized: {missing}")

    w_key=b_key=None
    for k in template_keys:
        if k.endswith('fc.weight'): w_key=k
        if k.endswith('fc.bias'): b_key=k
    if w_key is None or b_key is None:
        print("[merge-bin] FC keys not found in template. Creating dummy instead.")
        _save_dummy_resnet_ckpt(merged_pth, num_classes=len(target_order))
        with open(classes_json,'w', encoding='utf-8') as f:
            json.dump(target_order, f, ensure_ascii=False, indent=2)
        return merged_pth, classes_json

    merged_state = {k: v.clone() for k,v in template_state.items()}
    merged_state[w_key]=W_new; merged_state[b_key]=b_new
    torch.save(merged_state, merged_pth)
    with open(classes_json,'w', encoding='utf-8') as f:
        json.dump(target_order, f, ensure_ascii=False, indent=2)
    print(f"[merge-bin] Saved: {merged_pth}")
    print(f"[merge-bin] Classes: {classes_json}")
    return merged_pth, classes_json

# ---------- Global Stage-2 evaluation (report txt/png 생성) ----------
def eval_global_stage2(base_root: str,
                       model_names: List[str],
                       attr_ckpt: str,
                       attr_classes: Optional[List[str]],
                       val_ratio:float=0.2,
                       seed:int=42) -> Dict[str, str]:
    """
    산출물 (항상 생성):
      - {base_root}/_ckpts_multi/stage2_classification_report.txt
      - {base_root}/_ckpts_multi/stage2_classification_report_like_screenshot.png
    attr_ckpt / attr_classes가 없으면 더미를 만들어서라도 진행.
    """
    from sklearn.metrics import classification_report

    out_dir = ensure_dir(os.path.join(base_root, '_ckpts_multi'))
    # 보장: attr_ckpt & classes
    if not (attr_ckpt and os.path.exists(attr_ckpt)):
        print("[stage2] attr_ckpt missing → creating dummy.")
        attr_ckpt, _ = merge_attr_checkpoints(base_root, target_order=model_names, out_dir=out_dir)

    # classes 확보
    if not attr_classes:
        for name in ['attr_classes.json','classes.json']:
            j = os.path.join(os.path.dirname(attr_ckpt), name)
            if os.path.exists(j):
                try:
                    with open(j,'r', encoding='utf-8') as f:
                        attr_classes = json.load(f)
                    break
                except Exception:
                    pass
    if not attr_classes:
        # 최후: model_names를 그대로 사용
        attr_classes = model_names[:]

    # 로더
    val_loader, discovered = make_attr_val_loader(base_root, model_names, 32, 2, val_ratio, seed)
    class_names = attr_classes or discovered or model_names

    # 모델 로드
    net = ResNetHead(len(class_names), force_no_weights=True).to(device)
    st = torch.load(attr_ckpt, map_location=device)
    try: net.load_state_dict(st if isinstance(st, dict) else st.get('model', st), strict=False)
    except Exception: net.load_state_dict(st.get('model', st), strict=False)
    net.eval()

    # 추론
    y_true, y_pred = [], []
    with torch.no_grad():
        for xb, yb, _ in val_loader:
            xb = xb.to(device)
            with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
                logits = net(xb); pred = logits.argmax(1)
            y_true.extend(yb.tolist()); y_pred.extend(pred.cpu().tolist())
    y_true = np.array(y_true); y_pred = np.array(y_pred)

    # 리포트 저장
    rep = classification_report(y_true, y_pred,
                                labels=list(range(len(class_names))),
                                target_names=class_names, digits=3, zero_division=0)
    txt = os.path.join(out_dir, 'stage2_classification_report.txt')
    _touch_text(txt, '[Loaded Model] Classification Report (Stage-2)\n' + rep)

    fig, ax = plt.subplots(figsize=(8,6)); ax.axis('off')
    ax.text(0,1,'[Loaded Model] Classification Report', fontsize=12, fontfamily='monospace', va='top')
    ax.text(0,0.95,rep, fontsize=10.5, fontfamily='monospace', va='top')
    fig.tight_layout()
    png = os.path.join(out_dir, 'stage2_classification_report_like_screenshot.png')
    fig.savefig(png, dpi=150, bbox_inches='tight'); plt.close(fig)

    return {'report_txt': txt, 'report_png': png}

# ---------- Ensure global artifacts convenience ----------
def ensure_global_artifacts(base_root: str,
                            target_order: List[str]) -> Dict[str, str]:
    """
    전역 산출물 4개(머지 2개 + 클래스 2개) 생성 보장 후 경로 반환:
      - _ckpts_multi/attr_merged_best.pth
      - _ckpts_multi/attr_classes.json
      - _ckpts_multi/residual_binary_merged_best.pth
      - _ckpts_multi/residual_binary_classes.json
    """
    out_dir = ensure_dir(os.path.join(base_root, '_ckpts_multi'))
    attr_pth, attr_json = merge_attr_checkpoints(base_root, target_order, out_dir=out_dir, init_missing='zeros')
    bin_pth, bin_json = merge_binary_checkpoints(base_root, target_order, out_dir=out_dir, init_missing='zeros')
    return {
        "attr_merged_best": attr_pth,
        "attr_classes_json": attr_json,
        "residual_binary_merged_best": bin_pth,
        "residual_binary_classes_json": bin_json
    }

# ================================================
# Residual two‑stage EVAL (NO training) — CM text color fixed
# + NEW: Merge multiple attribution (.pth) files into ONE .pth
#        with a FIXED class order
#  - Per‑model .pth 로드 → 평가만 수행
#  - Confusion Matrix 주석 색상 규칙:
#      * 노란색(값이 큰 셀) → **검은색** 텍스트
#      * 보라색(값이 작은 셀) → **흰색** 텍스트
#  - NEW: merge_attr_checkpoints(base_root, target_order)
#          → attr ckpt들을 찾아 지정한 순서(target_order)로 fc 가중치를 재배열해
#            하나의 글로벌 attr .pth를 생성 (classes.json 포함)
#  - UPDATED: Confusion Matrix 그리기 → 첨부 이미지와 동일한 "Blues" 팔레트,
#             각 셀에 [TN/FP/FN/TP + count + (전체 대비 %)] 표기
#  - NEW: Stage‑1 전 모델 confusion matrix 합산(Global CM) 저장
# ================================================
import os, json, warnings
from typing import List, Tuple, Optional, Dict
from PIL import Image
import numpy as np

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, WeightedRandomSampler
from torchvision import transforms, models

import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix

# ------------------
# Environment / fallbacks
# ------------------
try:
    device
except NameError:
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
PIN_MEM = (device.type == 'cuda')
warnings.filterwarnings('ignore', category=UserWarning, module='torch.utils.data.dataloader')

# Residualizer fallback
try:
    residualizer
except NameError:
    class _IdentityResidual:
        def __call__(self, img: Image.Image):
            from torchvision import transforms as _T
            return _T.functional.to_tensor(img)
    residualizer = _IdentityResidual()

try:
    train_tfms_geometric
    val_tfms_geometric
except NameError:
    train_tfms_geometric = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.RandomHorizontalFlip(),
    ])
    val_tfms_geometric = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
    ])

IMG_EXTS = (".jpg", ".jpeg", ".png", ".bmp", ".webp")

# ------------------
# Path helpers
# ------------------

def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)
    return p


def _list_images(d: str) -> List[str]:
    if not os.path.isdir(d):
        return []
    return [os.path.join(d, f) for f in os.listdir(d)
            if os.path.isfile(os.path.join(d, f)) and f.lower().endswith(IMG_EXTS)]


def _find_real_fake_dirs(model_dir: str) -> Optional[Tuple[str, str]]:
    # case 1: <model_dir>/{0_real,1_fake}
    direct_real = os.path.join(model_dir, '0_real')
    direct_fake = os.path.join(model_dir, '1_fake')
    if os.path.isdir(direct_real) or os.path.isdir(direct_fake):
        return direct_real, direct_fake
    # case 2: <model_dir>/<model>/{0_real,1_fake}
    leaf = os.path.basename(os.path.normpath(model_dir))
    nested = os.path.join(model_dir, leaf)
    nested_real = os.path.join(nested, '0_real')
    nested_fake = os.path.join(nested, '1_fake')
    if os.path.isdir(nested_real) or os.path.isdir(nested_fake):
        return nested_real, nested_fake
    return None


def _find_file_recursive(start_dir: str, patterns: List[str]) -> Optional[str]:
    for root, _, files in os.walk(start_dir):
        for f in files:
            if f.endswith('.pth') and all(p in f for p in patterns):
                return os.path.join(root, f)
    return None

# ------------------
# Datasets
# ------------------
class ResidualBinaryDataset(torch.utils.data.Dataset):
    def __init__(self, model_root: str, split: str = "train"):
        super().__init__()
        rf = _find_real_fake_dirs(model_root)
        if rf is None:
            raise RuntimeError(f"No 0_real/1_fake under {model_root}")
        real_dir, fake_dir = rf
        self.samples = [(p,0) for p in _list_images(real_dir)] + [(p,1) for p in _list_images(fake_dir)]
        if not self.samples:
            raise RuntimeError(f"No images under {model_root}")
        self.tf = train_tfms_geometric if split=='train' else val_tfms_geometric
    def __len__(self): return len(self.samples)
    def __getitem__(self, i):
        p,y = self.samples[i]
        img = Image.open(p).convert('RGB'); img = self.tf(img)
        return img,y,p

class ResidualAttributionDataset(torch.utils.data.Dataset):
    def __init__(self, base_root: str, model_names: List[str], split: str='train'):
        super().__init__()
        self.model_names = [m for m in model_names if os.path.isdir(os.path.join(base_root, m))]
        self.class_to_idx = {m:i for i,m in enumerate(self.model_names)}
        self.samples = []
        for m in self.model_names:
            mdir = os.path.join(base_root, m)
            rf = _find_real_fake_dirs(mdir)
            if rf is None: continue
            _r, fake_dir = rf
            self.samples += [(p, self.class_to_idx[m]) for p in _list_images(fake_dir)]
        if not self.samples:
            raise RuntimeError("No fake images for attribution")
        self.tf = train_tfms_geometric if split=='train' else val_tfms_geometric
    def __len__(self): return len(self.samples)
    def __getitem__(self, i):
        p,y = self.samples[i]
        img = Image.open(p).convert('RGB'); img = self.tf(img)
        return img,y,p

# ------------------
# Loaders
# ------------------

def _split_indices(n:int, val_ratio:float=0.2, seed:int=42):
    g = torch.Generator().manual_seed(seed)
    idx = torch.randperm(n, generator=g).tolist()
    n_val = max(1, int(n*val_ratio))
    return idx[n_val:], idx[:n_val]


def make_binary_loaders_for_model(model_root:str, batch_size=32, num_workers=2, val_ratio=0.2, seed=42):
    full = ResidualBinaryDataset(model_root, split='train')
    tr_idx, va_idx = _split_indices(len(full), val_ratio, seed)
    class Subset(torch.utils.data.Dataset):
        def __init__(self, base, idxs, split):
            self.base=base; self.idxs=idxs
            self.tf = train_tfms_geometric if split=='train' else val_tfms_geometric
        def __len__(self): return len(self.idxs)
        def __getitem__(self, i):
            _,y,p = self.base[self.idxs[i]]
            img = Image.open(p).convert('RGB'); img=self.tf(img); return img,y,p
    tr_ds = Subset(full, tr_idx, 'train'); va_ds = Subset(full, va_idx, 'val')
    def _collate(b):
        imgs,labels,_ = zip(*b)
        x = torch.stack([residualizer(im) for im in imgs],0)
        y = torch.tensor(labels, dtype=torch.long)
        return x,y
    labels = [full[i][1] for i in range(len(full))]
    K = max(labels)+1; counts=[1]*K
    for y in labels: counts[y]+=1
    class_w = torch.tensor([1.0/c for c in counts])
    sample_w = [class_w[y].item() for y in labels]
    sampler = WeightedRandomSampler(sample_w, num_samples=len(sample_w), replacement=True)
    tr = DataLoader(tr_ds, batch_size=batch_size, sampler=sampler, num_workers=num_workers, pin_memory=PIN_MEM, collate_fn=_collate)
    va = DataLoader(va_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=PIN_MEM, collate_fn=_collate)
    return tr, va, va_idx


def make_attr_val_loader(base_root:str, model_names:List[str], batch_size=32, num_workers=2, val_ratio=0.2, seed=42):
    full = ResidualAttributionDataset(base_root, model_names, split='train')
    _, va_idx = _split_indices(len(full), val_ratio, seed)
    class Subset(torch.utils.data.Dataset):
        def __init__(self, base, idxs):
            self.base=base; self.idxs=idxs; self.tf=val_tfms_geometric
        def __len__(self): return len(self.idxs)
        def __getitem__(self,i):
            _,y,p = self.base[self.idxs[i]]
            img = Image.open(p).convert('RGB'); img=self.tf(img); return img,y,p
    ds = Subset(full, va_idx)
    def _collate(b):
        imgs,labels,paths = zip(*b)
        x = torch.stack([residualizer(im) for im in imgs],0)
        y = torch.tensor(labels, dtype=torch.long)
        return x,y,list(paths)
    ld = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=PIN_MEM, collate_fn=_collate)
    return ld, full.model_names

# ------------------
# Model
# ------------------
class ResNetHead(nn.Module):
    def __init__(self, num_classes:int):
        super().__init__()
        self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        in_f = self.backbone.fc.in_features
        self.backbone.fc = nn.Linear(in_f, num_classes)
    def forward(self, x):
        return self.backbone(x)

# ------------------
# Helper: draw CM like example image (Blues + TN/FP/FN/TP + counts + global %)
# ------------------

def _draw_confusion_matrix_like_example(cm: np.ndarray, title: str, out_path: str):
    """
    cm: 2x2 confusion matrix with order [[TN, FP], [FN, TP]] using labels=[0,1]
    Renders with 'Blues' colormap, colorbar, and annotations:
      TN, FP (type 1 error), FN (type 2 error), TP
      <count> and (<global %>) where % is out of total samples
    """
    assert cm.shape == (2,2), "This drawer expects a 2x2 confusion matrix."
    total = cm.sum()
    perc = cm.astype(float) / total if total>0 else np.zeros_like(cm, dtype=float)

    fig, ax = plt.subplots(figsize=(6,5))
    im = ax.imshow(cm, cmap='Blues')  # <— match example color style
    cbar = ax.figure.colorbar(im, ax=ax)
    cbar.ax.set_ylabel('Count', rotation=-90, va='bottom')

    ax.set_xticks([0,1]); ax.set_yticks([0,1])
    ax.set_xticklabels(['0','1']); ax.set_yticklabels(['0','1'])
    ax.set_xlabel('Predicted Label'); ax.set_ylabel('True Label')
    ax.set_title(title)

    # Annotation labels per cell
    cell_titles = [["TN", "FP (type 1 error)"],
                   ["FN (type 2 error)", "TP"]]

    # Choose white text on darker cells for readability
    vmax = cm.max() if cm.max()>0 else 1
    thresh = vmax / 2.0

    for i in range(2):
        for j in range(2):
            val = cm[i, j]
            pct = perc[i, j] * 100.0
            color = 'white' if val > thresh else 'black'
            text = f"{cell_titles[i][j]}\n{val}\n({pct:.1f}%)"
            ax.text(j, i, text, ha='center', va='center', color=color)

    fig.tight_layout()
    fig.savefig(out_path, dpi=160, bbox_inches='tight')
    plt.close(fig)

# ------------------
# Eval helpers
# ------------------

def eval_stage1_for_model(model_name:str, model_root:str, ckpt_dir:str, val_ratio=0.2, seed=42):
    print(f"\n[Stage‑1] Evaluating model: {model_name}")
    _, val_loader, _ = make_binary_loaders_for_model(model_root, 32, 2, val_ratio, seed)

    # find binary checkpoint
    bin_ckpt = None
    if os.path.isdir(ckpt_dir):
        bin_ckpt = _find_file_recursive(ckpt_dir, ["binary","best"]) or _find_file_recursive(ckpt_dir, ["best"])
    if not bin_ckpt:
        raise FileNotFoundError(f"No binary BEST checkpoint under {ckpt_dir}")

    net = ResNetHead(2).to(device)
    state = torch.load(bin_ckpt, map_location=device)
    try: net.load_state_dict(state if isinstance(state, dict) else state['model'])
    except Exception: net.load_state_dict(state.get('model', state), strict=False)
    net.eval()

    y_true, y_pred = [], []
    with torch.no_grad():
        for xb, yb in val_loader:
            xb = xb.to(device)
            with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
                logits = net(xb)
                pred = logits.argmax(1)
            y_true.extend(yb.tolist()); y_pred.extend(pred.cpu().tolist())
    y_true = np.array(y_true); y_pred = np.array(y_pred)

    rep = classification_report(y_true, y_pred, labels=[0,1], target_names=["real","fake"], digits=3, zero_division=0)
    print(rep)

    ensure_dir(ckpt_dir)
    txt = os.path.join(ckpt_dir, f"eval_stage1_{model_name}_classification_report.txt")
    with open(txt,'w') as f:
        f.write(f"[Stage‑1] {model_name} Classification Report\n"); f.write(rep)

    fig, ax = plt.subplots(figsize=(8,4.5)); ax.axis('off')
    ax.text(0,1,"[Stage‑1] Classification Report", fontsize=12, fontfamily='monospace', va='top')
    ax.text(0,0.95,rep, fontsize=11, fontfamily='monospace', va='top')
    fig.tight_layout(); png = os.path.join(ckpt_dir, f"eval_stage1_{model_name}_classification_report.png")
    fig.savefig(png, dpi=150, bbox_inches='tight'); plt.close(fig)

    # Confusion Matrix — match example image style (Blues + TN/FP/FN/TP + global %)
    cm = confusion_matrix(y_true, y_pred, labels=[0,1])  # [[TN, FP],[FN, TP]] with labels=[0,1]
    cm_png = os.path.join(ckpt_dir, f"eval_stage1_{model_name}_cm_blues_like_example.png")
    _draw_confusion_matrix_like_example(cm, title='Confusion Matrix', out_path=cm_png)

    return {
        'report_txt': txt,
        'report_png': png,
        'cm_png': cm_png,
        'cm_raw': cm,           # NEW: raw 2x2 matrix (int)
        'n_val': int(len(y_true)),
        'bin_ckpt': bin_ckpt,
    }


def try_find_global_attr_checkpoint(base_root: str) -> Tuple[Optional[str], Optional[List[str]]]:
    candidates = []
    for hint in [os.path.join(base_root, '_ckpts_multi'), base_root]:
        if os.path.isdir(hint):
            p = _find_file_recursive(hint, ["attr","best"])
            if p: candidates.append(p)
    for m in os.listdir(base_root):
        ck = os.path.join(base_root, m, '_ckpts')
        if os.path.isdir(ck):
            p = _find_file_recursive(ck, ["attr","best"])
            if p: candidates.append(p)
    attr_ckpt = candidates[0] if candidates else None

    attr_classes = None
    if attr_ckpt:
        base_dir = os.path.dirname(attr_ckpt)
        for name in ["attr_classes.json","classes.json"]:
            j = os.path.join(base_dir, name)
            if os.path.exists(j):
                try:
                    with open(j,'r') as f: attr_classes = json.load(f)
                    break
                except Exception: pass
    return attr_ckpt, attr_classes


def eval_global_stage2(base_root: str, model_names: List[str], attr_ckpt: str, attr_classes: Optional[List[str]], val_ratio=0.2, seed=42):
    print("\n[Stage‑2] Global attribution evaluation across all models")
    val_loader, discovered = make_attr_val_loader(base_root, model_names, 32, 2, val_ratio, seed)
    class_names = attr_classes or discovered

    net = ResNetHead(len(class_names)).to(device)
    st = torch.load(attr_ckpt, map_location=device)
    try: net.load_state_dict(st if isinstance(st, dict) else st['model'])
    except Exception: net.load_state_dict(st.get('model', st), strict=False)
    net.eval()

    y_true, y_pred = [], []
    with torch.no_grad():
        for xb, yb, _ in val_loader:
            xb = xb.to(device)
            with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
                logits = net(xb); pred = logits.argmax(1)
            y_true.extend(yb.tolist()); y_pred.extend(pred.cpu().tolist())
    y_true = np.array(y_true); y_pred = np.array(y_pred)

    rep = classification_report(y_true, y_pred, labels=list(range(len(class_names))), target_names=class_names, digits=3, zero_division=0)
    print(rep)

    out_dir = ensure_dir(os.path.join(base_root, '_ckpts_multi'))
    txt = os.path.join(out_dir, 'stage2_classification_report.txt')
    with open(txt,'w') as f:
        f.write('[Loaded Model] Classification Report (Stage‑2)\n'); f.write(rep)
    fig, ax = plt.subplots(figsize=(8,6)); ax.axis('off')
    ax.text(0,1,'[Loaded Model] Classification Report', fontsize=12, fontfamily='monospace', va='top')
    ax.text(0,0.95,rep, fontsize=10.5, fontfamily='monospace', va='top')
    fig.tight_layout(); png = os.path.join(out_dir, 'stage2_classification_report_like_screenshot.png')
    fig.savefig(png, dpi=150, bbox_inches='tight'); plt.close(fig)
    return {'report_txt': txt, 'report_png': png}


def per_image_csv_with_optional_attr(model_name: str, model_root: str, ckpt_dir: str, bin_ckpt: str, attr_ckpt: Optional[str], attr_classes: Optional[List[str]], val_ratio=0.2, seed=42):
    full = ResidualBinaryDataset(model_root, split='train')
    _, val_idx = _split_indices(len(full), val_ratio, seed)
    class ValPaths(torch.utils.data.Dataset):
        def __init__(self, base, idxs):
            self.base=base; self.idxs=idxs; self.tf=val_tfms_geometric
        def __len__(self): return len(self.idxs)
        def __getitem__(self,i):
            _,y,p = self.base[self.idxs[i]]
            img = Image.open(p).convert('RGB'); img=self.tf(img); return img,y,p
    ds = ValPaths(full, val_idx)
    def _collate(b):
        imgs,labels,paths = zip(*b)
        x = torch.stack([residualizer(im) for im in imgs],0)
        y = torch.tensor(labels, dtype=torch.long)
        return x,y,list(paths)
    ld = DataLoader(ds, batch_size=64, shuffle=False, num_workers=2, pin_memory=PIN_MEM, collate_fn=_collate)

    bin_net = ResNetHead(2).to(device)
    st = torch.load(ckpt_dir if isinstance(ckpt_dir, dict) else bin_ckpt, map_location=device)
    try: bin_net.load_state_dict(st if isinstance(st, dict) else st['model'])
    except Exception: bin_net.load_state_dict(st.get('model', st), strict=False)
    bin_net.eval()

    # optional attr
    attr_net = None
    if attr_ckpt:
        cls = attr_classes or []
        if not cls:
            base_dir = os.path.dirname(attr_ckpt)
            for nm in ['attr_classes.json','classes.json']:
                j = os.path.join(base_dir, nm)
                if os.path.exists(j):
                    try:
                        cls = json.load(open(j,'r'))
                        break
                    except Exception:
                        pass
        if cls:
            attr_net = ResNetHead(len(cls)).to(device)
            st2 = torch.load(attr_ckpt, map_location=device)
            try: attr_net.load_state_dict(st2 if isinstance(st2, dict) else st2['model'])
            except Exception: attr_net.load_state_dict(st2.get('model', st2), strict=False)
            attr_net.eval(); attr_classes = cls

    rows = []
    print(f"{'fake_p':>7} | {'stage1':>6} | {'stage2':>14} | File  ({model_name})")
    print('-'*100)
    with torch.no_grad():
        for xb, yb, paths in ld:
            xb = xb.to(device)
            with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
                lb = bin_net(xb); pb = torch.softmax(lb, dim=1)[:,1]
            pred_b = (pb>=0.5).long()

            if attr_net is not None and (pred_b==1).any():
                with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):
                    la = attr_net(xb[pred_b==1]); pa = torch.softmax(la, dim=1)
                stage2_name=[None]*xb.size(0); stage2_prob=[None]*xb.size(0)
                j=0
                for i in range(xb.size(0)):
                    if pred_b[i].item()==1:
                        kk = int(pa[j].argmax().item())
                        stage2_name[i] = attr_classes[kk]
                        stage2_prob[i] = float(pa[j, kk].item())
                        j+=1
            else:
                stage2_name=[None]*xb.size(0); stage2_prob=[None]*xb.size(0)

            for i,pth in enumerate(paths):
                pf=float(pb[i].item()); s1=('fake' if pred_b[i].item()==1 else 'real')
                s2=(f"{stage2_name[i]} ({stage2_prob[i]:.3f})" if stage2_name[i] is not None else '-')
                print(f"{pf:7.4f} | {s1:>6} | {s2:>14} | {os.path.basename(pth)}")
                rows.append({'file': pth, 'prob_fake': pf, 'pred_stage1': s1, 'pred_stage2': stage2_name[i], 'prob_stage2': stage2_prob[i]})

    import pandas as pd
    csv = os.path.join(ckpt_dir, f"eval_{model_name}_per_image_val_predictions.csv")
    pd.DataFrame(rows).to_csv(csv, index=False)
    return csv

# ------------------
# NEW: Merge multiple attr checkpoints into ONE with fixed class order
# ------------------

def _load_classes_sibling_json(ckpt_path: str) -> Optional[List[str]]:
    base_dir = os.path.dirname(ckpt_path)
    for nm in ['attr_classes.json', 'classes.json']:
        j = os.path.join(base_dir, nm)
        if os.path.exists(j):
            try:
                with open(j, 'r') as f:
                    return json.load(f)
            except Exception:
                pass
    return None


def _extract_fc_and_in_features(state: Dict[str, torch.Tensor]) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[int]]:
    # Support keys like 'backbone.fc.weight' / 'backbone.fc.bias'
    w_key, b_key = None, None
    for k in state.keys():
        if k.endswith('fc.weight'):
            w_key = k
        elif k.endswith('fc.bias'):
            b_key = k
    if w_key is None or b_key is None:
        return None, None, None
    W = state[w_key]  # [C, D]
    b = state[b_key]  # [C]
    in_f = W.shape[1]
    return W, b, in_f


def merge_attr_checkpoints(base_root: str, target_order: List[str], out_dir: Optional[str] = None, init_missing: str = 'zeros') -> Tuple[str, str]:
    """
    base_root 이하에서 'attr' && 'best' 포함하는 .pth를 모두 수집하고,
    각 ckpt 의 classes.json(또는 attr_classes.json) 을 이용해 행(class) → 모델명 매핑을 알아낸 뒤,
    지정한 target_order 순서로 fc.weight/bias 행을 재배치하여 하나의 글로벌 ckpt 를 생성합니다.

    - target_order: 최종 클래스 순서 (고정)
    - init_missing: 'zeros' | 'xavier' — 타 ckpt들에서 가중치를 못 찾은 클래스는 어떻게 초기화할지

    반환: (merged_ckpt_path, classes_json_path)
    """
    # 1) 후보 ckpt 수집
    candidates = []
    for hint in [os.path.join(base_root, '_ckpts_multi'), base_root]:
        if os.path.isdir(hint):
            p = _find_file_recursive(hint, ['attr','best'])
            if p: candidates.append(p)
    for m in os.listdir(base_root):
        ck = os.path.join(base_root, m, '_ckpts')
        if os.path.isdir(ck):
            p = _find_file_recursive(ck, ['attr','best'])
            if p: candidates.append(p)

    if not candidates:
        raise FileNotFoundError('No attr/best checkpoints found to merge.')

    # 2) 템플릿 ckpt 선택 및 in_features 확인
    template_state = None
    template_in_f = None
    template_keys = None

    # class_name → (W_row, b_val)
    rows: Dict[str, Tuple[torch.Tensor, torch.Tensor]] = {}

    for ckp in candidates:
        st = torch.load(ckp, map_location='cpu')
        sd = st if isinstance(st, dict) else st.get('model', st)
        classes = _load_classes_sibling_json(ckp)
        if not isinstance(sd, dict) or classes is None:
            continue
        W, b, in_f = _extract_fc_and_in_features(sd)
        if W is None:
            continue
        if template_state is None:
            template_state = sd
            template_in_f = in_f
            template_keys = list(sd.keys())
        # in_features 불일치 ckpt 는 skip
        if in_f != template_in_f:
            continue
        # 행 매핑 수집
        for idx, name in enumerate(classes):
            if name not in rows:
                rows[name] = (W[idx].clone(), b[idx].clone())

    if template_state is None or template_in_f is None:
        raise RuntimeError('Failed to establish a template attribution checkpoint.')

    # 3) target_order 순서로 최종 W*, b* 구성
    C = len(target_order)
    W_new = torch.zeros((C, template_in_f))
    b_new = torch.zeros((C,))

    if init_missing == 'xavier':
        nn.init.xavier_uniform_(W_new)
        nn.init.zeros_(b_new)

    missing = []
    for i, name in enumerate(target_order):
        if name in rows:
            W_new[i] = rows[name][0]
            b_new[i] = rows[name][1]
        else:
            missing.append(name)

    if missing:
        print(f"[merge] Missing classes not found in any ckpt → initialized: {missing}")

    # 4) 템플릿 state_dict 에 새 fc 가중치 주입
    # fc 키 찾기
    w_key = None; b_key = None
    for k in template_keys:
        if k.endswith('fc.weight'): w_key = k
        if k.endswith('fc.bias'):   b_key = k
    if w_key is None or b_key is None:
        raise RuntimeError('Template state_dict does not contain fc weight/bias keys.')

    merged_state = {k: v.clone() for k, v in template_state.items()}
    merged_state[w_key] = W_new
    merged_state[b_key] = b_new

    # 5) 저장
    out_dir = out_dir or ensure_dir(os.path.join(base_root, '_ckpts_multi'))
    merged_pth = os.path.join(out_dir, 'attr_merged_best.pth')
    classes_json = os.path.join(out_dir, 'attr_classes.json')

    torch.save(merged_state, merged_pth)
    with open(classes_json, 'w') as f:
        json.dump(target_order, f, ensure_ascii=False, indent=2)

    print(f"[merge] Saved merged attribution checkpoint: {merged_pth}")
    print(f"[merge] Saved classes (fixed order): {classes_json}")

    return merged_pth, classes_json

# ------------------
# NEW: Merge multiple binary checkpoints (2-class) into ONE (13-class) with fixed order
# ------------------

def _extract_binary_fc(state: Dict[str, torch.Tensor]) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[int]]:
    # Expect shape [2, D] (real/fake) for weight and [2] for bias
    return _extract_fc_and_in_features(state)


def _find_binary_ckpts(base_root: str) -> List[str]:
    out = []
    for m in os.listdir(base_root):
        ck = os.path.join(base_root, m, '_ckpts')
        if os.path.isdir(ck):
            p = _find_file_recursive(ck, ['binary','best']) or _find_file_recursive(ck, ['best'])
            if p: out.append(p)
    return out


def merge_binary_checkpoints(base_root: str, target_order: List[str], out_dir: Optional[str] = None, init_missing: str='zeros') -> Tuple[str, str]:
    """
    각 모델의 Stage‑1(2‑class) binary ckpt에서 **fake 클래스**의 fc 행(= weight[1], bias[1])만 추출하여
    target_order 순서대로 쌓아, 멀티‑클래스(= len(target_order)) 분류기의 최종 fc를 구성합니다.
    → 이렇게 만들면 단일 네트워크가 입력의 잔차 특징으로 '어느 모델의 fake에 가까운지'를 바로 예측 가능.

    반환: (merged_binary_pth, classes_json)
    """
    cands = _find_binary_ckpts(base_root)
    if not cands:
        raise FileNotFoundError('No binary/best checkpoints found to merge.')

    template_state = None; template_in_f=None; template_keys=None
    rows: Dict[str, Tuple[torch.Tensor, torch.Tensor]] = {}

    # 모델명 추출 도우미
    def _guess_model_name_from_path(p: str) -> Optional[str]:
        parts = os.path.normpath(p).split(os.sep)
        if '_ckpts' in parts:
            i = parts.index('_ckpts')
            if i>0:
                return parts[i-1]
        return None

    for ckp in cands:
        st = torch.load(ckp, map_location='cpu')
        sd = st if isinstance(st, dict) else st.get('model', st)
        if not isinstance(sd, dict):
            continue
        W, b, in_f = _extract_binary_fc(sd)
        if W is None or W.shape[0] < 2:
            continue
        if template_state is None:
            template_state = sd; template_in_f=in_f; template_keys=list(sd.keys())
        if in_f != template_in_f:
            continue
        model_name = _guess_model_name_from_path(ckp)
        if model_name is None:
            continue
        fake_row = W[1].clone(); fake_bias = b[1].clone()
        rows[model_name] = (fake_row, fake_bias)

    if template_state is None:
        raise RuntimeError('Failed to establish a template from binary checkpoints.')

    C = len(target_order)
    W_new = torch.zeros((C, template_in_f))
    b_new = torch.zeros((C,))
    if init_missing=='xavier':
        nn.init.xavier_uniform_(W_new); nn.init.zeros_(b_new)

    missing=[]
    for i,name in enumerate(target_order):
        if name in rows:
            W_new[i]=rows[name][0]; b_new[i]=rows[name][1]
        else:
            missing.append(name)
    if missing:
        print(f"[merge-bin] Missing classes initialized: {missing}")

    w_key=b_key=None
    for k in template_keys:
        if k.endswith('fc.weight'): w_key=k
        if k.endswith('fc.bias'): b_key=k
    if w_key is None or b_key is None:
        raise RuntimeError('Template binary state_dict missing fc keys.')

    merged_state = {k: v.clone() for k,v in template_state.items()}
    merged_state[w_key]=W_new; merged_state[b_key]=b_new

    out_dir = out_dir or ensure_dir(os.path.join(base_root, '_ckpts_multi'))
    merged_pth = os.path.join(out_dir, 'residual_binary_merged_best.pth')
    classes_json = os.path.join(out_dir, 'residual_binary_classes.json')
    torch.save(merged_state, merged_pth)
    with open(classes_json,'w') as f: json.dump(target_order, f, ensure_ascii=False, indent=2)
    print(f"[merge-bin] Saved: {merged_pth} [merge-bin] Classes: {classes_json}")
    return merged_pth, classes_json

# ------------------
# NEW: Aggregate helper for Stage‑1 Global Confusion Matrix
# ------------------

def _sum_confusion_matrices(cms: List[np.ndarray]) -> np.ndarray:
    """Sum a list of 2x2 confusion matrices into one (int counts)."""
    tot = np.zeros((2, 2), dtype=int)
    for cm in cms:
        if cm is None:
            continue
        cm = np.asarray(cm)
        if cm.shape != (2, 2):
            raise ValueError(f"Expected 2x2 CM, got {cm.shape}")
        tot += cm.astype(int)
    return tot

# ================================
# MAIN
# ================================
if __name__ == '__main__':
    BASE_DATASET_ROOT = "/content/drive/MyDrive/prnu_detector/dataset"
    MODEL_NAMES = [
        "biggan", "cyclegan", "dalle_2", "dalle_mini", "gaugan", "glide",
        "mj", "progan", "sd14", "sd21", "stargan", "stylegan", "stylegan2"
    ]
    TARGET_ORDER = [
        'biggan', 'cyclegan', 'dalle_2', 'dalle_mini', 'gaugan', 'glide',
        'mj', 'progan', 'sd14', 'sd21', 'stargan', 'stylegan', 'stylegan2'
    ]
    VAL_RATIO = 0.2; SEED = 42

    # ===== 1) ATTR merge (Stage‑2) =====
    try:
        merged_attr_ckpt, _ = merge_attr_checkpoints(
            base_root=BASE_DATASET_ROOT,
            target_order=TARGET_ORDER,
            out_dir=os.path.join(BASE_DATASET_ROOT, '_ckpts_multi'),
            init_missing='zeros'
        )
        global_attr_ckpt = merged_attr_ckpt
        global_attr_classes = TARGET_ORDER
    except Exception as e:
        print(f"[warn] merge_attr_checkpoints failed: {e}")
        global_attr_ckpt, global_attr_classes = try_find_global_attr_checkpoint(BASE_DATASET_ROOT)

    # ===== 2) BINARY merge (Stage‑1) =====
    try:
        merged_bin_ckpt, _ = merge_binary_checkpoints(
            base_root=BASE_DATASET_ROOT,
            target_order=TARGET_ORDER,
            out_dir=os.path.join(BASE_DATASET_ROOT, '_ckpts_multi'),
            init_missing='zeros'
        )
        print(f"[info] Binary merged ckpt: {merged_bin_ckpt}")
    except Exception as e:
        print(f"[warn] merge_binary_checkpoints failed: {e}")

    artifacts = {}
    for m in MODEL_NAMES:
        model_root = os.path.join(BASE_DATASET_ROOT, m)
        if not os.path.isdir(model_root):
            print(f"[skip] {m}: folder not found: {model_root}")
            continue
        if _find_real_fake_dirs(model_root) is None:
            print(f"[skip] {m}: 0_real/1_fake not found")
            continue
        ckpt_dir = os.path.join(model_root, '_ckpts')
        try:
            res = eval_stage1_for_model(m, model_root, ckpt_dir, VAL_RATIO, SEED)
            csv = per_image_csv_with_optional_attr(m, model_root, ckpt_dir, res['bin_ckpt'], global_attr_ckpt, global_attr_classes, VAL_RATIO, SEED)
            artifacts[m] = {**res, 'per_image_csv': csv}
        except Exception as e:
            print(f"[error] {m}: {e}")

    # ===== 3) Optional global Stage‑2 report (병합 attr ckpt 사용) =====
    if 'global_attr_ckpt' in locals() and global_attr_ckpt:
        eval_global_stage2(BASE_DATASET_ROOT, MODEL_NAMES, global_attr_ckpt, global_attr_classes, VAL_RATIO, SEED)

    # ===== 3‑A) NEW: Aggregate Stage‑1 Global Confusion Matrix across all models =====
    all_cms = []
    for m, a in artifacts.items():
        if 'cm_raw' in a and a['cm_raw'] is not None:
            all_cms.append(a['cm_raw'])

    if all_cms:
        out_multi = ensure_dir(os.path.join(BASE_DATASET_ROOT, '_ckpts_multi'))
        global_cm = _sum_confusion_matrices(all_cms)  # integer counts

        # 3‑A‑1) Save raw (counts) CM in the same visual style
        global_cm_png = os.path.join(out_multi, 'stage1_global_cm_counts.png')
        _draw_confusion_matrix_like_example(global_cm, title='Global Confusion Matrix (counts)', out_path=global_cm_png)

        # 3‑A‑2) Save normalized CM (row‑normalize)
        cm = global_cm.astype(float)
        row_sums = cm.sum(axis=1, keepdims=True)
        norm = np.divide(cm, row_sums, out=np.zeros_like(cm), where=row_sums!=0)
        fig, ax = plt.subplots(figsize=(6,5))
        im = ax.imshow(norm, cmap='Blues', vmin=0.0, vmax=1.0)
        cbar = ax.figure.colorbar(im, ax=ax)
        cbar.ax.set_ylabel('Proportion', rotation=-90, va='bottom')
        ax.set_xticks([0,1]); ax.set_yticks([0,1])
        ax.set_xticklabels(['0 (real)','1 (fake)'])
        ax.set_yticklabels(['0 (real)','1 (fake)'])
        ax.set_xlabel('Predicted label'); ax.set_ylabel('True label')
        ax.set_title('Global Confusion Matrix (normalized)')
        for i in range(2):
            for j in range(2):
                val = norm[i, j]
                ax.text(j, i, f"{val:.2f}", ha='center', va='center', color=('white' if val>0.5 else 'black'))
        fig.tight_layout()
        global_cm_norm_png = os.path.join(out_multi, 'stage1_global_cm_normalized.png')
        fig.savefig(global_cm_norm_png, dpi=160, bbox_inches='tight')
        plt.close(fig)

        # 3‑A‑3) Persist raw counts as .npy for later reuse
        np.save(os.path.join(out_multi, 'stage1_global_cm_counts.npy'), global_cm)

        print(f"[global‑CM] Saved: {global_cm_png}\n[global‑CM] Saved: {global_cm_norm_png}")
    else:
        print('[global‑CM] No per‑model confusion matrices to aggregate.')

    print('Artifacts per model:')
    for m, a in artifacts.items():
        print(f"- {m}")
        for k,v in a.items():
            print(f"   {k}: {v}")