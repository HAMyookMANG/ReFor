{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1FFj0C6pLXxV8s9cBs01HTgyCuGKS6yeG","authorship_tag":"ABX9TyPOeYjiRkZT1glsDxI6wmg1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# ==========================================\n","# Noise Residual 기반 \"2단계\" 분류 파이프라인\n","#   1) 1단계: real vs fake (binary)\n","#   2) 2단계: fake로 판정된 경우에만 모델 attribution (multi-class)\n","#\n","# 기대 폴더 구조 (동일):\n","#   /content/Drive/MyDrive/prnu_detector/dataset/\n","#       biggan/{0_real,1_fake}/**.png|jpg|jpeg|bmp|webp\n","#       cyclegan/{0_real,1_fake}/**.*\n","#       ...\n","# ==========================================\n","# 주석 추가용\n","!pip -q install --upgrade torch torchvision scikit-learn\n","\n","import os, math, random, io, itertools, json, warnings\n","from typing import List, Tuple, Optional, Dict\n","from PIL import Image\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, WeightedRandomSampler\n","from torchvision import transforms, models\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","import hashlib\n","import json\n","from pathlib import Path\n","\n","import shutil\n","import pandas as pd"],"metadata":{"id":"pTdNy8UQmEWb","executionInfo":{"status":"ok","timestamp":1758855913603,"user_tz":-540,"elapsed":12284,"user":{"displayName":"yena","userId":"07153292099691088597"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Colab 기본 gdown이 있긴 하지만, 없을 경우 대비\n","!pip -q install gdown==4.6.0\n","\n","# 다운로드\n","!gdown https://drive.google.com/uc?id=1ruo_EdKl61uacWKD2sBknbqMQ6QrFKyU -O /content/prnu_dataset.zip\n","\n","# 압축 해제\n","!unzip -q -o /content/prnu_dataset.zip -d /content/prnu_dataset\n","\n","# (선택) 구조 확인\n","!find /content/prnu_dataset -maxdepth 2 -type d | sort | head -n 50"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y6TO0mWH6Bv2","executionInfo":{"status":"ok","timestamp":1758856260946,"user_tz":-540,"elapsed":347327,"user":{"displayName":"yena","userId":"07153292099691088597"}},"outputId":"49d63f99-24ff-4e0f-81b0-bcca64d064a8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1ruo_EdKl61uacWKD2sBknbqMQ6QrFKyU\n","To: /content/prnu_dataset.zip\n","100% 12.4G/12.4G [03:05<00:00, 66.9MB/s]\n","/content/prnu_dataset\n","/content/prnu_dataset/data\n","/content/prnu_dataset/data/biggan\n","/content/prnu_dataset/data/cyclegan\n","/content/prnu_dataset/data/dalle_2\n","/content/prnu_dataset/data/dalle_mini\n","/content/prnu_dataset/data/gaugan\n","/content/prnu_dataset/data/glide\n","/content/prnu_dataset/data/mj\n","/content/prnu_dataset/data/progan\n","/content/prnu_dataset/data/sd14\n","/content/prnu_dataset/data/sd21\n","/content/prnu_dataset/data/stargan\n","/content/prnu_dataset/data/stylegan\n","/content/prnu_dataset/data/stylegan2\n"]}]},{"cell_type":"code","source":["# --------------------------\n","# 0) 환경/시드/GPU\n","# --------------------------\n","def seed_all(seed=42):\n","    random.seed(seed); np.random.seed(seed)\n","    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n","seed_all(42)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Device:\", device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pO-KwkA2mLkn","executionInfo":{"status":"ok","timestamp":1758856261100,"user_tz":-540,"elapsed":150,"user":{"displayName":"yena","userId":"07153292099691088597"}},"outputId":"ee2eb7ad-20ec-47a4-8753-af001be8b2a0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]}]},{"cell_type":"code","source":["# --------------------------\n","# 1) Noise Residual 변환\n","# --------------------------\n","class ToResidualTensor:\n","    def __init__(self, ksize=5, sigma=1.2, eps=1e-6):\n","        self.ksize = ksize\n","        self.sigma = sigma\n","        self.eps = eps\n","        # 가우시안 커널\n","        ax = torch.arange(ksize) - (ksize-1)/2\n","        xx, yy = torch.meshgrid(ax, ax, indexing='ij')\n","        kernel = torch.exp(-(xx**2 + yy**2) / (2*sigma*sigma))\n","        kernel = kernel / kernel.sum()\n","        self.registered = False\n","        self.kernel = kernel\n","\n","    def _register(self, device):\n","        k = self.kernel.to(device=device, dtype=torch.float32)\n","        self.weight = k.view(1,1,self.ksize,self.ksize)\n","        self.registered = True\n","\n","    def __call__(self, img: Image.Image):\n","        # 1) PIL -> Tensor [0,1] (C,H,W)\n","        x = transforms.functional.to_tensor(img)\n","        # 2) Grayscale(Y)\n","        if x.shape[0] == 3:\n","            y = 0.299*x[0] + 0.587*x[1] + 0.114*x[2]\n","        else:\n","            y = x[0]\n","        y = y.unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n","        if not self.registered:\n","            self._register(y.device)\n","        # 3) Blur\n","        y_blur = F.conv2d(y, self.weight, padding=self.ksize//2)\n","        # 4) Residual\n","        r = y - y_blur\n","        # 5) Normalize\n","        mean = r.mean()\n","        std  = r.std()\n","        r = (r - mean) / (std + self.eps)\n","        # 6) 3채널 복제\n","        r3 = r.repeat(1,3,1,1).squeeze(0)  # (3,H,W)\n","        return r3"],"metadata":{"id":"QyGLSfsvmNhO","executionInfo":{"status":"ok","timestamp":1758856261142,"user_tz":-540,"elapsed":40,"user":{"displayName":"yena","userId":"07153292099691088597"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# --------------------------\n","# 2) 데이터 구성 (2단계 전용)\n","# --------------------------\n","IMG_SIZE = 224\n","IMG_EXTS = ('.jpg', '.jpeg', '.png', '.bmp', '.webp')\n","\n","train_tfms_geometric = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(IMG_SIZE),\n","    transforms.RandomHorizontalFlip(),\n","])\n","\n","val_tfms_geometric = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(IMG_SIZE),\n","])\n","\n","residualizer = ToResidualTensor(ksize=5, sigma=1.2)\n","\n","def _list_images(d):\n","    return [os.path.join(d, f) for f in os.listdir(d)\n","            if os.path.isfile(os.path.join(d,f)) and f.lower().endswith(IMG_EXTS)]\n","\n","# ---- (A) 1단계: Binary (real=0, fake=1) ----\n","class ResidualBinaryDataset(torch.utils.data.Dataset):\n","    def __init__(self, root: str, split: str = \"train\"):\n","        super().__init__()\n","        self.samples = []\n","        model_dirs = [d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))]\n","        for m in model_dirs:\n","            real_dir = os.path.join(root, m, \"0_real\")\n","            fake_dir = os.path.join(root, m, \"1_fake\")\n","            if os.path.isdir(real_dir):\n","                self.samples += [(p, 0) for p in _list_images(real_dir)]\n","            if os.path.isdir(fake_dir):\n","                self.samples += [(p, 1) for p in _list_images(fake_dir)]\n","        if not self.samples:\n","            raise RuntimeError(f\"No images found under {root}\")\n","        self.tf = train_tfms_geometric if split == \"train\" else val_tfms_geometric\n","\n","    def __len__(self): return len(self.samples)\n","    def __getitem__(self, idx):\n","        p,y = self.samples[idx]\n","        img = Image.open(p).convert(\"RGB\"); img = self.tf(img)\n","        return img, y, p\n","\n","# ---- (B) 2단계: Fake Attribution (multi-class: 각 모델 이름) ----\n","class ResidualAttributionDataset(torch.utils.data.Dataset):\n","    \"\"\"\n","    root/\n","      modelA/{0_real,1_fake}/...\n","      modelB/{0_real,1_fake}/...\n","      ...\n","    - 1_fake → 해당 모델 라벨\n","    - 0_real은 사용하지 않음 (2단계는 fake만)\n","    \"\"\"\n","    def __init__(self, root: str, split: str = \"train\"):\n","        super().__init__()\n","        self.root = root\n","        self.split = split\n","\n","        # 1) fake 데이터가 '실제로 존재'하는 모델만 클래스에 포함\n","        all_model_dirs = [d for d in os.listdir(root) if os.path.isdir(os.path.join(root,d))]\n","        model_names = []\n","        for m in sorted(all_model_dirs):\n","            fake_dir = os.path.join(root, m, \"1_fake\")\n","            if os.path.isdir(fake_dir):\n","                imgs = _list_images(fake_dir)\n","                if len(imgs) > 0:\n","                    model_names.append(m)\n","\n","        if len(model_names) == 0:\n","            raise RuntimeError(f\"No valid model folders with 1_fake images under: {root}\")\n","\n","        self.model_names = model_names\n","        self.class_to_idx = {m:i for i,m in enumerate(self.model_names)}\n","\n","        # 2) 샘플 구성\n","        self.samples: List[Tuple[str,int]] = []\n","        for m in self.model_names:\n","            fake_dir = os.path.join(root, m, \"1_fake\")\n","            for p in _list_images(fake_dir):\n","                self.samples.append((p, self.class_to_idx[m]))\n","\n","        if len(self.samples) == 0:\n","            raise RuntimeError(f\"No fake images found under: {root}\")\n","\n","        self.transform = train_tfms_geometric if split == \"train\" else val_tfms_geometric\n","\n","    def __len__(self): return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        path, label = self.samples[idx]\n","        img = Image.open(path).convert('RGB')\n","        img = self.transform(img)\n","        return img, label, path\n","\n","# ---- 공통: 로더/샘플러 ----\n","def _class_counts_from_indices(dataset, indices: List[int], num_classes: int):\n","    counts = [0]*num_classes\n","    for i in indices:\n","        _, y, _ = dataset[i]\n","        counts[y] += 1\n","    return counts\n","\n","def _split_indices(n: int, val_ratio: float = 0.2, seed: int = 42):\n","    g = torch.Generator().manual_seed(seed)\n","    indices = torch.randperm(n, generator=g).tolist()\n","    n_val = max(1, int(len(indices)*val_ratio))\n","    return indices[n_val:], indices[:n_val]  # train, val\n","\n","def _make_loader_with_residual(dataset, batch_size=32, num_workers=2, use_sampler=True):\n","    # collate: residualizer 적용 + 텐서 스택\n","    def _collate_with_residual(batch):\n","        imgs, labels, _paths = zip(*batch)\n","        tensors = [residualizer(img) for img in imgs]\n","        x = torch.stack(tensors, dim=0)\n","        y = torch.tensor(labels, dtype=torch.long)\n","        return x, y\n","    if use_sampler:\n","        # 클래스 불균형 보정\n","        labels = [dataset[i][1] for i in range(len(dataset))]\n","        K = max(labels)+1\n","        counts = [1]*K\n","        for y in labels: counts[y] += 1\n","        class_w = torch.tensor([1.0/c for c in counts], dtype=torch.float32)\n","        sample_w = [class_w[y].item() for y in labels]\n","        sampler = WeightedRandomSampler(sample_w, num_samples=len(sample_w), replacement=True)\n","        return DataLoader(dataset, batch_size=batch_size, sampler=sampler,\n","                          num_workers=num_workers, pin_memory=True,\n","                          collate_fn=_collate_with_residual)\n","    else:\n","        return DataLoader(dataset, batch_size=batch_size, shuffle=False,\n","                          num_workers=num_workers, pin_memory=True,\n","                          collate_fn=_collate_with_residual)\n","\n","def make_binary_loaders(dataset_root: str, batch_size=32, num_workers=2, val_ratio=0.2, seed=42):\n","    # 데이터 수집\n","    all_samples = []\n","    model_dirs = [d for d in os.listdir(dataset_root) if os.path.isdir(os.path.join(dataset_root, d))]\n","\n","    for m in model_dirs:\n","        real_dir = os.path.join(dataset_root, m, \"0_real\")\n","        fake_dir = os.path.join(dataset_root, m, \"1_fake\")\n","        if os.path.isdir(real_dir):\n","            all_samples += [(p, 0, p) for p in _list_images(real_dir)]\n","        if os.path.isdir(fake_dir):\n","            all_samples += [(p, 1, p) for p in _list_images(fake_dir)]\n","\n","    # 분할\n","    tr_idx, va_idx = _split_indices(len(all_samples), val_ratio, seed)\n","    tr_samples = [all_samples[i] for i in tr_idx]\n","    va_samples = [all_samples[i] for i in va_idx]\n","\n","    # 데이터셋 생성 (Subset 방식 적용)\n","    class BinarySubset(torch.utils.data.Dataset):\n","        def __init__(self, samples, split):\n","            self.samples = samples\n","            self.tf = train_tfms_geometric if split == \"train\" else val_tfms_geometric\n","\n","        def __len__(self):\n","            return len(self.samples)\n","\n","        def __getitem__(self, i):\n","            path, label, _ = self.samples[i]\n","            img = Image.open(path).convert(\"RGB\")\n","            img = self.tf(img)\n","            return img, label, path\n","\n","    tr_ds = BinarySubset(tr_samples, \"train\")\n","    va_ds = BinarySubset(va_samples, \"val\")\n","\n","    def _collate(batch):\n","        imgs, labels, _ = zip(*batch)\n","        x = torch.stack([residualizer(im) for im in imgs], 0)\n","        y = torch.tensor(labels, dtype=torch.long)\n","        return x, y\n","\n","    # 샘플러 설정 (훈련 데이터만 사용)\n","    labels = [y for (_, y, _) in tr_samples]\n","    K = max(labels) + 1\n","    counts = [1] * K\n","    for y in labels:\n","        counts[y] += 1\n","    class_w = torch.tensor([1.0/c for c in counts])\n","    sample_w = [class_w[y].item() for y in labels]\n","    sampler = WeightedRandomSampler(sample_w, num_samples=len(sample_w), replacement=True)\n","\n","    tr_loader = DataLoader(tr_ds, batch_size=batch_size, sampler=sampler,\n","                          num_workers=num_workers, pin_memory=True, collate_fn=_collate)\n","    va_loader = DataLoader(va_ds, batch_size=batch_size, shuffle=False,\n","                          num_workers=num_workers, pin_memory=True, collate_fn=_collate)\n","\n","    print(f\"[Binary Data] train={len(tr_samples)} val={len(va_samples)}\")\n","    return tr_loader, va_loader\n","\n","def make_attr_loaders(dataset_root: str, batch_size=32, num_workers=2, val_ratio=0.2, seed=42):\n","    full = ResidualAttributionDataset(dataset_root, split=\"train\")\n","    class_names = full.model_names\n","    train_idx, val_idx = _split_indices(len(full), val_ratio, seed)\n","\n","    class Subset(torch.utils.data.Dataset):\n","        def __init__(self, base, idxs, split):\n","            self.base = base; self.idxs = idxs\n","            self.tf = train_tfms_geometric if split==\"train\" else val_tfms_geometric\n","        def __len__(self): return len(self.idxs)\n","        def __getitem__(self, i):\n","            _, y, p = self.base[self.idxs[i]]\n","            img = Image.open(p).convert('RGB')\n","            img = self.tf(img)\n","            return img, y, p\n","\n","    train_ds = Subset(full, train_idx, \"train\")\n","    val_ds   = Subset(full, val_idx,   \"val\")\n","    train_loader = _make_loader_with_residual(train_ds, batch_size, num_workers, use_sampler=True)\n","    val_loader   = _make_loader_with_residual(val_ds,   batch_size, num_workers, use_sampler=False)\n","\n","    print(f\"[Attribution Data] classes={class_names}\")\n","    print(f\"[Attribution Data] train={len(train_idx)} val={len(val_idx)}\")\n","    return train_loader, val_loader, class_names"],"metadata":{"id":"N4jyrGYSmRna","executionInfo":{"status":"ok","timestamp":1758856261146,"user_tz":-540,"elapsed":2,"user":{"displayName":"yena","userId":"07153292099691088597"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# --------------------------\n","# 3) 모델 정의\n","# --------------------------\n","class ResNetHead(nn.Module):\n","    def __init__(self, num_classes: int):\n","        super().__init__()\n","        self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","        in_f = self.backbone.fc.in_features\n","        self.backbone.fc = nn.Linear(in_f, num_classes)\n","\n","    def forward(self, x):\n","        return self.backbone(x)"],"metadata":{"id":"uuQ2FjyLmT97","executionInfo":{"status":"ok","timestamp":1758856261149,"user_tz":-540,"elapsed":1,"user":{"displayName":"yena","userId":"07153292099691088597"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# --------------------------\n","# 4) 학습/평가 루프 (체크포인트 로드/저장 + 혼동행렬/리포트 포함)\n","# --------------------------\n","def accuracy_from_logits(logits, y):\n","    pred = logits.argmax(dim=1)\n","    return (pred == y).float().mean().item()\n","\n","def _plot_cm(cm, class_names, title=\"Confusion Matrix\", normalize=True):\n","    if normalize:\n","        cm = cm.astype('float') / (cm.sum(axis=1, keepdims=True) + 1e-12)\n","    fig, ax = plt.subplots(figsize=(6, 5))\n","    im = ax.imshow(cm, interpolation='nearest')\n","    ax.figure.colorbar(im, ax=ax)\n","    ax.set(xticks=np.arange(len(class_names)), yticks=np.arange(len(class_names)),\n","           xticklabels=class_names, yticklabels=class_names,\n","           ylabel='True', xlabel='Predicted', title=title)\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n","    thresh = cm.max() / 2.0\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, f\"{cm[i, j]:.2f}\" if normalize else int(cm[i, j]),\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n","    plt.tight_layout()\n","    plt.show()\n","\n","def _print_report_and_cm(y_true, y_pred, class_names=None, title_prefix=\"\"):\n","    if class_names is None:\n","        # 기본: 이진 분류 가정 (0=real, 1=fake)\n","        class_names = [\"real\", \"fake\"] if len(set(y_true)) <= 2 else [str(i) for i in sorted(set(y_true))]\n","    print(f\"\\n{title_prefix}Classification Report\")\n","    print(classification_report(y_true, y_pred, target_names=class_names, digits=3))\n","    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n","    print(f\"{title_prefix}Confusion Matrix (counts)\")\n","    print(cm)\n","    _plot_cm(cm, class_names, title=f\"{title_prefix}Confusion Matrix (normalized)\")\n","\n","def train_one_epoch(model, loader, optimizer, scaler, criterion):\n","    model.train()\n","    loss_m = acc_m = 0.0\n","    for x, y in loader:\n","        x, y = x.to(device), y.to(device)\n","        optimizer.zero_grad(set_to_none=True)\n","        with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):\n","            logits = model(x)\n","            loss = criterion(logits, y)\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        acc = accuracy_from_logits(logits, y)\n","        loss_m += loss.item()\n","        acc_m += acc\n","    n = len(loader)\n","    return loss_m/n, acc_m/n\n","\n","@torch.no_grad()\n","def evaluate(model, loader, criterion):\n","    model.eval()\n","    loss_m = acc_m = 0.0\n","    all_true, all_pred = [], []\n","    for x, y in loader:\n","        x, y = x.to(device), y.to(device)\n","        with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):\n","            logits = model(x)\n","            loss = criterion(logits, y)\n","        acc = accuracy_from_logits(logits, y)\n","        loss_m += loss.item()\n","        acc_m += acc\n","        all_true.extend(y.detach().cpu().tolist())\n","        all_pred.extend(logits.argmax(dim=1).detach().cpu().tolist())\n","    n = len(loader)\n","    return (loss_m/n, acc_m/n, np.array(all_true), np.array(all_pred))\n","\n","#----\n","#바이너리코드\n","#----\n","\n","def fit_model(model, train_loader, val_loader, epochs=8, lr=1e-3, wd=1e-4,\n","              head_freeze=True, save_path=\"/content/best.pth\", label_smoothing=0.1,\n","              class_names=None, print_metrics=True):\n","    \"\"\"\n","    캐시 없는 훈련 함수\n","    \"\"\"\n","    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n","\n","    # 1) 헤드-프리즈 설정\n","    params_all = list(model.parameters())\n","    if head_freeze:\n","        for p in model.backbone.layer1.parameters(): p.requires_grad = False\n","        for p in model.backbone.layer2.parameters(): p.requires_grad = False\n","        for p in model.backbone.layer3.parameters(): p.requires_grad = False\n","        for p in model.backbone.layer4.parameters(): p.requires_grad = False\n","        for p in model.backbone.fc.parameters():     p.requires_grad = True\n","\n","    opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n","                            lr=lr, weight_decay=wd)\n","    scaler = torch.amp.GradScaler('cuda', enabled=(device.type=='cuda'))\n","    scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=3, gamma=0.5)\n","\n","    tr_losses=[]; va_losses=[]; tr_accs=[]; va_accs=[]\n","    best_acc=-1.0\n","\n","    head_epochs = max(3, min(5, epochs//2))\n","    print(f\"[Training] Starting training with {epochs} epochs (head: {head_epochs}, finetune: {epochs-head_epochs})\")\n","\n","    # 2) 헤드 학습\n","    for ep in range(1, head_epochs+1):\n","        tl, ta = train_one_epoch(model, train_loader, opt, scaler, criterion)\n","        vl, va, y_true, y_pred = evaluate(model, val_loader, criterion)\n","        scheduler.step()\n","        tr_losses.append(tl); tr_accs.append(ta); va_losses.append(vl); va_accs.append(va)\n","        print(f\"[Head {ep}/{head_epochs}] train {tl:.4f}/{ta:.3f} | val {vl:.4f}/{va:.3f}\")\n","        if va > best_acc:\n","            best_acc = va\n","            torch.save(model.state_dict(), save_path)\n","            print(\"  -> saved best (head stage)\")\n","\n","    # 3) 전체 미세조정\n","    for p in model.parameters(): p.requires_grad = True\n","    opt = torch.optim.AdamW(model.parameters(), lr=max(lr*0.03, 3e-5), weight_decay=wd)\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max(1, epochs-head_epochs))\n","\n","    for ep in range(head_epochs+1, epochs+1):\n","        tl, ta = train_one_epoch(model, train_loader, opt, scaler, criterion)\n","        vl, va, y_true, y_pred = evaluate(model, val_loader, criterion)\n","        scheduler.step()\n","        tr_losses.append(tl); tr_accs.append(ta); va_losses.append(vl); va_accs.append(va)\n","        print(f\"[Finetune {ep}/{epochs}] train {tl:.4f}/{ta:.3f} | val {vl:.4f}/{va:.3f}\")\n","        if va > best_acc:\n","            best_acc = va\n","            torch.save(model.state_dict(), save_path)\n","            print(\"  -> saved best\")\n","\n","    # 4) 학습 곡선\n","    epochs_range = range(1, len(tr_losses)+1)\n","    plt.figure(figsize=(7,5))\n","    plt.plot(epochs_range, tr_losses, marker='o', label='Train Loss')\n","    plt.plot(epochs_range, va_losses, marker='s', label='Val Loss')\n","    plt.title(\"Loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n","    plt.grid(True); plt.legend(); plt.show()\n","\n","    plt.figure(figsize=(7,5))\n","    plt.plot(epochs_range, tr_accs, marker='o', label='Train Acc')\n","    plt.plot(epochs_range, va_accs, marker='s', label='Val Acc')\n","    plt.title(\"Accuracy\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\")\n","    plt.grid(True); plt.legend(); plt.show()\n","\n","    # 5) 베스트 체크포인트 로드 후 최종 검증/지표\n","    if os.path.exists(save_path):\n","        state = torch.load(save_path, map_location=device)\n","        model.load_state_dict(state)\n","        model.to(device).eval()\n","\n","    print(f\"[Best Val Acc] {best_acc:.3f} (saved to {save_path})\")\n","\n","    if val_loader is not None and print_metrics:\n","        vl, va, y_true, y_pred = evaluate(model, val_loader, criterion)\n","        print(f\"[Final Eval @ Best CKPT] val loss/acc: {vl:.4f}/{va:.3f}\")\n","        _print_report_and_cm(y_true, y_pred, class_names=class_names, title_prefix=\"[Best CKPT] \")\n","\n","    return best_acc"],"metadata":{"id":"n0iZ8UVFmXTx","executionInfo":{"status":"ok","timestamp":1758856261301,"user_tz":-540,"elapsed":151,"user":{"displayName":"yena","userId":"07153292099691088597"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# --------------------------\n","# 5) 추론 유틸 (2단계 연결)\n","# --------------------------\n","@torch.no_grad()\n","def _softmax_np(logits: torch.Tensor):\n","    p = torch.softmax(logits, dim=1)[0].detach().cpu().numpy()\n","    return p\n","\n","@torch.no_grad()\n","def predict_two_stage(\n","    img_path: str,\n","    binary_model: nn.Module,\n","    attr_model: nn.Module,\n","    attr_class_names: List[str],\n","    tta: bool = True,\n","    fake_threshold: float = 0.5\n","):\n","    \"\"\"\n","    1단계에서 fake 확률 >= fake_threshold 이면 2단계 attribution.\n","    반환:\n","      {\n","        'stage1': {'pred': 'real'|'fake', 'prob_fake': float},\n","        'stage2': {'pred_model': str|None, 'prob': float|None, 'probs': list|None}\n","      }\n","    \"\"\"\n","    # 공통 변환\n","    img = Image.open(img_path).convert('RGB')\n","    xg = val_tfms_geometric(img)\n","    xr = residualizer(xg).unsqueeze(0).to(device)\n","\n","    # ---- Stage 1: real/fake\n","    binary_model.eval()\n","    with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):\n","        logit_b = binary_model(xr)\n","        if tta: logit_b = logit_b + binary_model(torch.flip(xr, dims=[3]))\n","    prob_fake = float(_softmax_np(logit_b)[1])  # [real, fake] 순으로 가정\n","    stage1_pred = 'fake' if prob_fake >= fake_threshold else 'real'\n","\n","    # ---- Stage 2: only if fake\n","    stage2 = {'pred_model': None, 'prob': None, 'probs': None}\n","    if stage1_pred == 'fake':\n","        attr_model.eval()\n","        with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):\n","            logit_a = attr_model(xr)\n","            if tta: logit_a = logit_a + attr_model(torch.flip(xr, dims=[3]))\n","        probs = _softmax_np(logit_a)\n","        idx = int(np.argmax(probs))\n","        stage2 = {'pred_model': attr_class_names[idx], 'prob': float(probs[idx]), 'probs': probs.tolist()}\n","\n","    return {\n","        'stage1': {'pred': stage1_pred, 'prob_fake': prob_fake},\n","        'stage2': stage2\n","    }"],"metadata":{"id":"Tf_5CE-smZOm","executionInfo":{"status":"ok","timestamp":1758856261339,"user_tz":-540,"elapsed":34,"user":{"displayName":"yena","userId":"07153292099691088597"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# --------------------------\n","# 6) 평가(예시): 균형 샘플 평가 (혼동행렬/리포트 강화)\n","# --------------------------\n","def _plot_cm(cm, class_names, title=\"Confusion Matrix\", normalize=True):\n","    if normalize:\n","        cm = cm.astype('float') / (cm.sum(axis=1, keepdims=True) + 1e-12)\n","    fig, ax = plt.subplots(figsize=(6, 5))\n","    im = ax.imshow(cm, interpolation='nearest')\n","    ax.figure.colorbar(im, ax=ax)\n","    ax.set(xticks=np.arange(len(class_names)), yticks=np.arange(len(class_names)),\n","           xticklabels=class_names, yticklabels=class_names,\n","           ylabel='True', xlabel='Predicted', title=title)\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n","    thresh = cm.max() / 2.0\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, f\"{cm[i, j]:.2f}\" if normalize else int(cm[i, j]),\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"-LUOEPuDmaTw","executionInfo":{"status":"ok","timestamp":1758856261341,"user_tz":-540,"elapsed":33,"user":{"displayName":"yena","userId":"07153292099691088597"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# --------------------------\n","# 6.5) 전체 검증셋 End-to-End 평가 (1단계→2단계 체인)\n","# --------------------------\n","if \"_plot_cm\" not in globals():\n","    import matplotlib.pyplot as plt\n","    def _plot_cm(cm, class_names, title=\"Confusion Matrix\", normalize=True):\n","        if normalize:\n","            cm = cm.astype('float') / (cm.sum(axis=1, keepdims=True) + 1e-12)\n","        fig, ax = plt.subplots(figsize=(6, 5))\n","        im = ax.imshow(cm, interpolation='nearest')\n","        ax.figure.colorbar(im, ax=ax)\n","        ax.set(xticks=np.arange(len(class_names)), yticks=np.arange(len(class_names)),\n","               xticklabels=class_names, yticklabels=class_names,\n","               ylabel='True', xlabel='Predicted', title=title)\n","        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n","        thresh = cm.max() / 2.0\n","        for i in range(cm.shape[0]):\n","            for j in range(cm.shape[1]):\n","                ax.text(j, i, f\"{cm[i, j]:.2f}\" if normalize else int(cm[i, j]),\n","                        ha=\"center\", va=\"center\",\n","                        color=\"white\" if cm[i, j] > thresh else \"black\")\n","        plt.tight_layout()\n","        plt.show()"],"metadata":{"id":"YMzIz0ZjmcAx","executionInfo":{"status":"ok","timestamp":1758856261342,"user_tz":-540,"elapsed":4,"user":{"displayName":"yena","userId":"07153292099691088597"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# ================================================\n","# Residual two‑stage EVAL (NO training) — CM text color fixed\n","# + NEW: Merge multiple attribution (.pth) files into ONE .pth\n","#        with a FIXED class order\n","# + NEW: \"unknown\" 전용 단일 클래스 학습 유틸 추가\n","#        - 학습 데이터: /content/drive/MyDrive/prnu_detector/dataset/unknown/0_real\n","#        - 산출물(pth): /content/drive/MyDrive/prnu_detector/dataset/unknown/_ckpts/unknown_binary_best.pth\n","#        - 평가/리포트 파일명/출력에서도 unknown~ 접두어 사용\n","#  - Per‑model .pth 로드 → 평가만 수행\n","#  - Confusion Matrix 주석 색상 규칙:\n","#      * 노란색(값이 큰 셀) → **검은색** 텍스트\n","#      * 보라색(값이 작은 셀) → **흰색** 텍스트\n","#  - NEW: merge_attr_checkpoints(base_root, target_order)\n","#          → attr ckpt들을 찾아 지정한 순서(target_order)로 fc 가중치를 재배열해\n","#            하나의 글로벌 attr .pth를 생성 (classes.json 포함)\n","#  - UPDATED: Confusion Matrix 그리기 → 첨부 이미지와 동일한 \"Blues\" 팔레트,\n","#             각 셀에 [TN/FP/FN/TP + count + (전체 대비 %)] 표기\n","# ================================================\n","\n","\n","# ------------------\n","# Environment / fallbacks\n","# ------------------\n","try:\n","    device\n","except NameError:\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","PIN_MEM = (device.type == 'cuda')\n","warnings.filterwarnings('ignore', category=UserWarning, module='torch.utils.data.dataloader')\n","\n","# Residualizer fallback — accepts either PIL.Image or Tensor\n","try:\n","    residualizer\n","except NameError:\n","    class _IdentityResidual:\n","        def __call__(self, img):\n","            if isinstance(img, torch.Tensor):\n","                return img\n","            from torchvision.transforms.functional import to_tensor\n","            return to_tensor(img)\n","    residualizer = _IdentityResidual()\n","\n","try:\n","    train_tfms_geometric\n","    val_tfms_geometric\n","except NameError:\n","    train_tfms_geometric = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","    ])\n","    val_tfms_geometric = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","    ])\n","\n","IMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\")\n","\n","# ------------------\n","# Path helpers\n","# ------------------\n","\n","def ensure_dir(p: str):\n","    os.makedirs(p, exist_ok=True)\n","    return p\n","\n","\n","def _list_images(d: str) -> List[str]:\n","    if not os.path.isdir(d):\n","        return []\n","    return [os.path.join(d, f) for f in os.listdir(d)\n","            if os.path.isfile(os.path.join(d, f)) and f.lower().endswith(IMG_EXTS)]\n","\n","\n","def _find_real_fake_dirs(model_dir: str) -> Optional[Tuple[str, str]]:\n","    # case 1: <model_dir>/{0_real,1_fake}\n","    direct_real = os.path.join(model_dir, '0_real')\n","    direct_fake = os.path.join(model_dir, '1_fake')\n","    if os.path.isdir(direct_real) or os.path.isdir(direct_fake):\n","        return direct_real, direct_fake\n","    # case 2: <model_dir>/<model>/{0_real,1_fake}\n","    leaf = os.path.basename(os.path.normpath(model_dir))\n","    nested = os.path.join(model_dir, leaf)\n","    nested_real = os.path.join(nested, '0_real')\n","    nested_fake = os.path.join(nested, '1_fake')\n","    if os.path.isdir(nested_real) or os.path.isdir(nested_fake):\n","        return nested_real, nested_fake\n","    return None\n","\n","\n","def _find_file_recursive(start_dir: str, patterns: List[str]) -> Optional[str]:\n","    for root, _, files in os.walk(start_dir):\n","        for f in files:\n","            if f.endswith('.pth') and all(p in f for p in patterns):\n","                return os.path.join(root, f)\n","    return None\n","\n","# ------------------\n","# Datasets\n","# ------------------\n","\n","# ------------------\n","# Loaders\n","# ------------------\n","\n","def _split_indices(n:int, val_ratio:float=0.2, seed:int=42):\n","    g = torch.Generator().manual_seed(seed)\n","    idx = torch.randperm(n, generator=g).tolist()\n","    n_val = max(1, int(n*val_ratio))\n","    return idx[n_val:], idx[:n_val]\n","\n","\n","\n","# ------------------\n","# Model\n","# ------------------\n","#중복삭제\n","\n","# ------------------\n","# Helper: draw CM like example image (Blues + TN/FP/FN/TP + counts + global %)\n","# ------------------\n","\n","#confusion matrix 코드 삭제\n","\n"],"metadata":{"id":"NfOGQBIFmdqf","executionInfo":{"status":"ok","timestamp":1758856261343,"user_tz":-540,"elapsed":4,"user":{"displayName":"yena","userId":"07153292099691088597"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def evaluate_two_stage_full_val(\n","    dataset_root: str,\n","    binary_model: nn.Module,\n","    attr_model: nn.Module,\n","    attr_class_names: List[str],\n","    val_ratio: float = 0.2,\n","    seed: int = 42,\n","    batch_size: int = 64,\n","    tta: bool = True,\n","    fake_threshold: float = 0.5,\n","):\n","    \"\"\"전체 검증셋에 대한 2단계 평가\"\"\"\n","\n","    # Binary 검증 분할 재현\n","    full = ResidualBinaryDataset(dataset_root, split=\"train\")\n","    train_idx, val_idx = _split_indices(len(full), val_ratio, seed)\n","\n","    # 검증 데이터셋 준비\n","    class ValDataset(torch.utils.data.Dataset):\n","        def __init__(self, base, idxs):\n","            self.base = base\n","            self.idxs = idxs\n","            self.tf = val_tfms_geometric\n","\n","        def __len__(self):\n","            return len(self.idxs)\n","\n","        def __getitem__(self, i):\n","            _, y_bin, p = self.base[self.idxs[i]]\n","            img = Image.open(p).convert('RGB')\n","            x = residualizer(self.tf(img))\n","            # E2E 정답 라벨 계산\n","            if y_bin == 0:  # real\n","                y_e2e = 0\n","            else:  # fake -> 모델명 추출\n","                rel = os.path.relpath(p, dataset_root)\n","                model_name = rel.split(os.sep)[0]\n","                if model_name in attr_class_names:\n","                    y_e2e = 1 + attr_class_names.index(model_name)\n","                else:\n","                    y_e2e = 0  # unknown으로 처리\n","            return x, y_bin, y_e2e\n","\n","    val_ds = ValDataset(full, val_idx)\n","    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False,\n","                           num_workers=2, pin_memory=True)\n","\n","    # 평가 수행\n","    y_true_bin, y_pred_bin = [], []\n","    y_true_attr, y_pred_attr = [], []\n","\n","    binary_model.eval()\n","    attr_model.eval()\n","\n","    with torch.no_grad():\n","        for x_batch, y_bin_batch, y_e2e_batch in val_loader:\n","            x_batch = x_batch.to(device)\n","\n","            # Stage 1: Binary\n","            logits_bin = binary_model(x_batch)\n","            if tta:\n","                logits_bin = logits_bin + binary_model(torch.flip(x_batch, dims=[3]))\n","\n","            probs_bin = torch.softmax(logits_bin, dim=1)\n","            pred_bin = (probs_bin[:, 1] >= fake_threshold).long()\n","\n","            # Stage 2: Attribution (fake로 예측된 것만)\n","            logits_attr = attr_model(x_batch)\n","            if tta:\n","                logits_attr = logits_attr + attr_model(torch.flip(x_batch, dims=[3]))\n","            pred_attr = logits_attr.argmax(dim=1)\n","\n","            # 결과 저장\n","            y_true_bin.extend(y_bin_batch.tolist())\n","            y_pred_bin.extend(pred_bin.cpu().tolist())\n","\n","            # Attribution 평가 (fake 샘플만)\n","            fake_mask = (y_bin_batch == 1) & (pred_bin.cpu() == 1)\n","            if fake_mask.any():\n","                # 실제 모델 인덱스 추출\n","                true_attr = (y_e2e_batch[fake_mask] - 1).clamp(min=0)\n","                y_true_attr.extend(true_attr.tolist())\n","                y_pred_attr.extend(pred_attr[fake_mask].cpu().tolist())\n","\n","    # Confusion Matrix 1: Binary\n","    print(\"\\n========== Stage 1: Binary Classification ==========\")\n","    cm1 = confusion_matrix(y_true_bin, y_pred_bin, labels=[0, 1])\n","    print(\"Confusion Matrix:\")\n","    print(cm1)\n","    _plot_cm(cm1, [\"real\", \"fake\"], title=\"Stage 1: Real vs Fake\")\n","\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_true_bin, y_pred_bin,\n","                              target_names=[\"real\", \"fake\"], digits=3))\n","\n","    # Confusion Matrix 2: Attribution (fake로 올바르게 분류된 것만)\n","    if len(y_true_attr) > 0:\n","        print(\"\\n========== Stage 2: Attribution (13 classes) ==========\")\n","        cm2 = confusion_matrix(y_true_attr, y_pred_attr,\n","                              labels=list(range(len(attr_class_names))))\n","        print(\"Confusion Matrix:\")\n","        print(cm2)\n","        _plot_cm(cm2, attr_class_names, title=\"Stage 2: Model Attribution\")\n","\n","        print(\"\\nClassification Report:\")\n","        print(classification_report(y_true_attr, y_pred_attr,\n","                                  target_names=attr_class_names, digits=3))\n","    else:\n","        print(\"\\n[Stage 2] No fake samples correctly identified by Stage 1\")\n","\n","    return {\"cm_binary\": cm1, \"cm_attr\": cm2 if len(y_true_attr) > 0 else None}"],"metadata":{"id":"VMKncs1lym6D","executionInfo":{"status":"ok","timestamp":1758856261345,"user_tz":-540,"elapsed":1,"user":{"displayName":"yena","userId":"07153292099691088597"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# ================================================\n","# Residual two‑stage EVAL (NO training) — CM text color fixed\n","# + NEW: Merge multiple attribution (.pth) files into ONE .pth\n","#        with a FIXED class order\n","#  - Per‑model .pth 로드 → 평가만 수행\n","#  - Confusion Matrix 주석 색상 규칙:\n","#      * 노란색(값이 큰 셀) → **검은색** 텍스트\n","#      * 보라색(값이 작은 셀) → **흰색** 텍스트\n","#  - NEW: merge_attr_checkpoints(base_root, target_order)\n","#          → attr ckpt들을 찾아 지정한 순서(target_order)로 fc 가중치를 재배열해\n","#            하나의 글로벌 attr .pth를 생성 (classes.json 포함)\n","#  - UPDATED: Confusion Matrix 그리기 → 첨부 이미지와 동일한 \"Blues\" 팔레트,\n","#             각 셀에 [TN/FP/FN/TP + count + (전체 대비 %)] 표기\n","# ================================================\n","\n","\n","# ------------------\n","# Environment / fallbacks\n","# ------------------\n","try:\n","    device\n","except NameError:\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","PIN_MEM = (device.type == 'cuda')\n","warnings.filterwarnings('ignore', category=UserWarning, module='torch.utils.data.dataloader')\n","\n","# Residualizer fallback\n","try:\n","    residualizer\n","except NameError:\n","    class _IdentityResidual:\n","        def __call__(self, img: Image.Image):\n","            from torchvision import transforms as _T\n","            return _T.functional.to_tensor(img)\n","    residualizer = _IdentityResidual()\n","\n","try:\n","    train_tfms_geometric\n","    val_tfms_geometric\n","except NameError:\n","    train_tfms_geometric = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","    ])\n","    val_tfms_geometric = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","    ])\n","\n","IMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\")\n","\n","# ------------------\n","# Path helpers\n","# ------------------\n","\n","def ensure_dir(p: str):\n","    os.makedirs(p, exist_ok=True)\n","    return p\n","\n","\n","def _list_images(d: str) -> List[str]:\n","    if not os.path.isdir(d):\n","        return []\n","    return [os.path.join(d, f) for f in os.listdir(d)\n","            if os.path.isfile(os.path.join(d, f)) and f.lower().endswith(IMG_EXTS)]\n","\n","\n","def _find_real_fake_dirs(model_dir: str) -> Optional[Tuple[str, str]]:\n","    # case 1: <model_dir>/{0_real,1_fake}\n","    direct_real = os.path.join(model_dir, '0_real')\n","    direct_fake = os.path.join(model_dir, '1_fake')\n","    if os.path.isdir(direct_real) or os.path.isdir(direct_fake):\n","        return direct_real, direct_fake\n","    # case 2: <model_dir>/<model>/{0_real,1_fake}\n","    leaf = os.path.basename(os.path.normpath(model_dir))\n","    nested = os.path.join(model_dir, leaf)\n","    nested_real = os.path.join(nested, '0_real')\n","    nested_fake = os.path.join(nested, '1_fake')\n","    if os.path.isdir(nested_real) or os.path.isdir(nested_fake):\n","        return nested_real, nested_fake\n","    return None\n","\n","\n","def _find_file_recursive(start_dir: str, patterns: List[str]) -> Optional[str]:\n","    for root, _, files in os.walk(start_dir):\n","        for f in files:\n","            if f.endswith('.pth') and all(p in f for p in patterns):\n","                return os.path.join(root, f)\n","    return None\n","\n","# ------------------\n","# Datasets\n","# ------------------\n","\n","\n","# ------------------\n","# Loaders\n","# ------------------\n","\n","def _split_indices(n:int, val_ratio:float=0.2, seed:int=42):\n","    g = torch.Generator().manual_seed(seed)\n","    idx = torch.randperm(n, generator=g).tolist()\n","    n_val = max(1, int(n*val_ratio))\n","    return idx[n_val:], idx[:n_val]\n","\n","\n","\n","# ------------------\n","# Model\n","# ------------------\n","#중복삭제\n","\n","# ------------------\n","# Helper: draw CM like example image (Blues + TN/FP/FN/TP + counts + global %)\n","# ------------------\n","\n","#confusion matrix 코드 삭제\n","\n","# ------------------\n","# Eval helpers\n","# ------------------\n","# def per_image_csv_with_optional_attr 삭제\n","\n","\n","\n","#def _extract_fc_and_in_features 삭제\n","# ------------------\n","# NEW: Merge multiple binary checkpoints (2-class) into ONE (13-class) with fixed order\n","# ------------------\n","\n","# def _extract_binary_fc 삭제\n","\n","\n","#def _find_binary_ckpts 삭제\n","\n","\n","# ================================\n","# MAIN\n","# ================================\n","import shutil\n","\n","if __name__ == \"__main__\":\n","    BASE_DATASET_ROOT = \"/content/prnu_dataset/data\"\n","    VAL_RATIO = 0.2\n","    SEED = 42\n","\n","    # 체크포인트 경로 설정\n","    ensure_dir(os.path.join(BASE_DATASET_ROOT, \"_ckpts_multi\"))\n","    binary_ckpt_path = os.path.join(BASE_DATASET_ROOT, \"_ckpts_multi\", \"binary_full_best.pth\")\n","    attr_ckpt_path = os.path.join(BASE_DATASET_ROOT, \"_ckpts_multi\", \"attr_full_best.pth\")\n","\n","    # ---- 1단계: Binary 분류 ----\n","    print(\"=\"*60)\n","    print(\"Training Stage 1: Binary Classification\")\n","    print(\"=\"*60)\n","\n","    train_loader, val_loader = make_binary_loaders(\n","        BASE_DATASET_ROOT,\n","        batch_size=32,\n","        num_workers=2,\n","        val_ratio=VAL_RATIO,\n","        seed=SEED\n","    )\n","\n","    binary_model = ResNetHead(num_classes=2).to(device)\n","\n","    # 체크포인트가 있으면 로드, 없으면 학습\n","    if os.path.exists(binary_ckpt_path):\n","        print(f\"Loading existing binary checkpoint from {binary_ckpt_path}\")\n","        binary_model.load_state_dict(torch.load(binary_ckpt_path, map_location=device))\n","        binary_model.eval()\n","    else:\n","        best_acc_binary = fit_model(\n","            binary_model,\n","            train_loader,\n","            val_loader,\n","            epochs=20,\n","            lr=1e-3,\n","            wd=1e-4,\n","            save_path=binary_ckpt_path,\n","            class_names=[\"real\", \"fake\"]\n","        )\n","        print(f\"[Binary] Training completed with accuracy: {best_acc_binary:.4f}\")\n","\n","    # ---- 2단계: Attribution 분류 ----\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"Training Stage 2: Attribution Classification\")\n","    print(\"=\"*60)\n","\n","    attr_train_loader, attr_val_loader, attr_classes = make_attr_loaders(\n","        BASE_DATASET_ROOT,\n","        batch_size=32,\n","        num_workers=2,\n","        val_ratio=VAL_RATIO,\n","        seed=SEED\n","    )\n","\n","    print(f\"Attribution classes: {attr_classes}\")\n","    attr_model = ResNetHead(num_classes=len(attr_classes)).to(device)\n","\n","    # 체크포인트가 있으면 로드, 없으면 학습\n","    if os.path.exists(attr_ckpt_path):\n","        print(f\"Loading existing attribution checkpoint from {attr_ckpt_path}\")\n","        attr_model.load_state_dict(torch.load(attr_ckpt_path, map_location=device))\n","        attr_model.eval()\n","    else:\n","        best_acc_attr = fit_model(\n","            attr_model,\n","            attr_train_loader,\n","            attr_val_loader,\n","            epochs=20,\n","            lr=1e-3,\n","            wd=1e-4,\n","            save_path=attr_ckpt_path,\n","            class_names=attr_classes\n","        )\n","        print(f\"[Attribution] Training completed with accuracy: {best_acc_attr:.4f}\")\n","\n","    # ---- 전체 테스트셋 평가 ----\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"Evaluating Two-Stage Pipeline on Test Set\")\n","    print(\"=\"*60)\n","\n","    results = evaluate_two_stage_full_val(\n","        dataset_root=BASE_DATASET_ROOT,\n","        binary_model=binary_model,\n","        attr_model=attr_model,\n","        attr_class_names=attr_classes,\n","        val_ratio=VAL_RATIO,\n","        seed=SEED,\n","        batch_size=64,\n","        tta=True,\n","        fake_threshold=0.5\n","    )\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"Evaluation Complete!\")\n","    print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZSj2DDNpmfJx","outputId":"75555cfe-68de-46db-848f-712f9c64b09a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Training Stage 1: Binary Classification\n","============================================================\n","[Binary Data] train=60276 val=15068\n","Loading existing binary checkpoint from /content/prnu_dataset/data/_ckpts_multi/binary_full_best.pth\n","\n","============================================================\n","Training Stage 2: Attribution Classification\n","============================================================\n"]}]},{"cell_type":"code","source":["# ------------------\n","# Environment / fallbacks\n","# ------------------\n","try:\n","    device\n","except NameError:\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","PIN_MEM = (device.type == 'cuda')\n","warnings.filterwarnings('ignore', category=UserWarning, module='torch.utils.data.dataloader')\n","\n","# Residualizer fallback (사용자가 별도 residualizer 주입 안 했을 때 안전 동작)\n","try:\n","    residualizer\n","except NameError:\n","    class _IdentityResidual:\n","        def __call__(self, img: Image.Image):\n","            from torchvision import transforms as _T\n","            return _T.functional.to_tensor(img)\n","    residualizer = _IdentityResidual()\n","\n","try:\n","    train_tfms_geometric\n","    val_tfms_geometric\n","except NameError:\n","    train_tfms_geometric = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","    ])\n","    val_tfms_geometric = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","    ])\n","\n","IMG_EXTS = ('.jpg', '.jpeg', '.png', '.bmp', '.webp')\n","\n","# ------------------\n","# Path helpers\n","# ------------------\n","\n","def ensure_dir(p: str):\n","    os.makedirs(p, exist_ok=True)\n","    return p\n","\n","\n","def _list_images(d: str) -> List[str]:\n","    if not os.path.isdir(d):\n","        return []\n","    return [os.path.join(d, f) for f in os.listdir(d)\n","            if os.path.isfile(os.path.join(d, f)) and f.lower().endswith(IMG_EXTS)]\n","\n","\n","def _find_real_fake_dirs(model_dir: str) -> Optional[Tuple[str, str]]:\n","    # case 1: <model_dir>/{0_real,1_fake}\n","    direct_real = os.path.join(model_dir, '0_real')\n","    direct_fake = os.path.join(model_dir, '1_fake')\n","    if os.path.isdir(direct_real) or os.path.isdir(direct_fake):\n","        return direct_real, direct_fake\n","    # case 2: <model_dir>/<model>/{0_real,1_fake}\n","    leaf = os.path.basename(os.path.normpath(model_dir))\n","    nested = os.path.join(model_dir, leaf)\n","    nested_real = os.path.join(nested, '0_real')\n","    nested_fake = os.path.join(nested, '1_fake')\n","    if os.path.isdir(nested_real) or os.path.isdir(nested_fake):\n","        return nested_real, nested_fake\n","    return None\n","\n","\n","def _find_file_recursive(start_dir: str, patterns: List[str]) -> Optional[str]:\n","    for root, _, files in os.walk(start_dir):\n","        for f in files:\n","            if f.endswith('.pth') and all(p in f for p in patterns):\n","                return os.path.join(root, f)\n","    return None\n","\n","# ------------------\n","# Datasets\n","# ------------------\n","\n","# ------------------\n","# Loaders\n","# ------------------\n","\n","def _split_indices(n: int, val_ratio: float = 0.2, seed: int = 42):\n","    g = torch.Generator().manual_seed(seed)\n","    idx = torch.randperm(n, generator=g).tolist()\n","    n_val = max(1, int(n * val_ratio))\n","    return idx[n_val:], idx[:n_val]\n","\n","\n","\n","# ------------------\n","# Model\n","# ------------------\n","#중복삭제\n","\n","# ------------------\n","# Helper: draw 2x2 CM (Blues + TN/FP/FN/TP + counts + global %)\n","# ------------------\n","\n","#confusion matrix 코드 삭제\n","\n","# ------------------\n","# Helper: draw Global Multiclass CM (Blues, normalized)\n","# ------------------\n","\n","#def _draw_global_multiclass_cm 삭제\n","\n","# ------------------\n","# Eval helpers (Stage-1)\n","# ------------------\n","\n","# ------------------\n","# Unknown-threshold: compute from Stage-2 attribution validation\n","# ------------------\n","\n","# ------------------\n","# Stage-2 (attr) eval that applies unknown-threshold\n","# ------------------\n","\n","\n","# ------------------\n","# per-image CSV (+unknown label 적용해 저장)\n","# ------------------\n","\n","# ------------------\n","# Merge attr checkpoints into ONE with fixed order (+UNKNOWN CLASS APPEND)\n","# ------------------\n","\n","# def _load_classes_sibling_json 삭제\n","\n","\n","#def _extract_fc_and_in_features 삭제\n","\n","\n","# ------------------\n","# Binary merge (unchanged here; still for per-model eval usage)\n","# ------------------\n","# def _extract_binary_fc 삭제\n","\n","\n","# def _find_binary_ckpts 삭제\n","\n","# ------------------\n","# Aggregate helper for Stage‑1 Global Confusion Matrix (2x2 sum)\n","# ------------------\n","\n","# def _sum_confusion_matrices 삭제\n","\n","# ================================\n","# MAIN\n","# ================================\n"],"metadata":{"id":"GPpGRfPmmhuB","executionInfo":{"status":"aborted","timestamp":1758856869877,"user_tz":-540,"elapsed":23,"user":{"displayName":"yena","userId":"07153292099691088597"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shutil.copy(\"/content/prnu_dataset/data/_ckpts_multi/binary_full_best.pth\", \"/content/drive/MyDrive/prnu_detector/binary_full_best.pth\")"],"metadata":{"id":"8xdAQomZmi6G","executionInfo":{"status":"aborted","timestamp":1758856869881,"user_tz":-540,"elapsed":13,"user":{"displayName":"yena","userId":"07153292099691088597"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shutil.copy(\"/content/prnu_dataset/data/_ckpts_multi/attr_full_best.pth\", \"/content/drive/MyDrive/prnu_detector/attr_full_best.pth\")"],"metadata":{"id":"1kM5Qnvh0cbZ","executionInfo":{"status":"aborted","timestamp":1758856869883,"user_tz":-540,"elapsed":14,"user":{"displayName":"yena","userId":"07153292099691088597"}}},"execution_count":null,"outputs":[]}]}